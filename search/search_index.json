{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"CodeOptiX <p>Agentic Code Optimization &amp; Deep Evaluation for Superior Coding Agent Experience</p> <p> Built by Superagentic AI, Powered by GEPA and Bloom </p> \ud83d\ude80 Get Started  \u26a1 Quick Start  \ud83d\udce6 View on GitHub"},{"location":"#what-is-codeoptix","title":"\ud83d\udd0d What is CodeOptiX?","text":"<p>CodeOptiX is the universal code optimization engine that improves coding agent experience with deep evaluations and optimization.</p> <p>When AI coding agents dazzle with impressive code but leave you wondering about quality, maintainability, security, and reliability, CodeOptiX ensures proper behavior through evaluations, reflection, and self-improvement. Powered by GEPA optimization and Bloom scenario generation.</p>"},{"location":"#key-capabilities","title":"\ud83d\ude80 Key Capabilities","text":"<ul> <li>\ud83d\udd0d Deep Behavioral Evaluation - Comprehensive testing against security, reliability, and quality behaviors</li> <li>\ud83e\uddec GEPA Optimization Engine - Genetic-Pareto Evolution for automatic agent improvement</li> <li>\ud83c\udf38 Bloom-Style Scenario Generation - Intelligent test case creation for thorough evaluation</li> <li>\ud83c\udfaf Multi-Agent Support - Works with Claude Code, Codex, Gemini CLI, and custom agents</li> <li>\ud83d\udd27 Multi-Provider LLM Support - OpenAI, Anthropic, Google, and Ollama (local models included!)</li> <li>\u26a1 CI/CD Integration - Automated quality gates and GitHub Actions support</li> </ul> <p>Ollama Support - No API Key Required!</p> <p>CodeOptiX supports Ollama for evaluations - use local models without API keys:</p> <ul> <li>\u2705 Ollama integration - Run evaluations with local models</li> <li>\u2705 No API key needed - Perfect for open-source users</li> <li>\u2705 Privacy-friendly - All processing happens locally</li> <li>\u2705 Free to use - No cloud costs</li> <li>\u26a0\ufe0f Limited evolution support - Use cloud providers for <code>codeoptix evolve</code></li> </ul> <p>See Ollama Integration for setup and limitations.</p> <p>Cloud providers (OpenAI, Anthropic, Google) still require API keys. See Installation for setup.</p>"},{"location":"#open-source-limitations","title":"\ud83d\udccb Open Source Limitations","text":"<p>The open source version provides core evaluation capabilities. Advanced features like agent evolution and optimization have limited support. For full optimization capabilities tailored to your needs, please get in touch.</p>"},{"location":"#quick-start-30-seconds","title":"\ud83d\ude80 Quick Start (30 Seconds)","text":"<pre><code># 1. Install\npip install codeoptix\n\n# 2. Option A: Use Ollama (No API key!)\nollama serve\ncodeoptix eval \\\n  --agent basic \\\n  --behaviors insecure-code \\\n  --llm-provider ollama\n\n# 2. Option B: Use cloud provider (requires API key)\nexport OPENAI_API_KEY=\"your-key-here\"\ncodeoptix eval \\\n  --agent basic \\\n  --behaviors insecure-code \\\n  --llm-provider openai\n</code></pre> <p>That's it! You've just run your first quality check.</p>"},{"location":"#key-features","title":"\u2728 Key Features","text":"\ud83e\udd16 Agent-Agnostic <p>Works with any coding agent:</p> <ul> <li>Claude Code</li> <li>Codex (GPT-5.2)</li> <li>Gemini CLI</li> <li>ACP-compatible agents</li> <li>Custom agents</li> </ul> \ud83c\udfaf Behavior-Agnostic <p>Modular behavior specifications:</p> <ul> <li>insecure-code - Security vulnerabilities</li> <li>vacuous-tests - Test quality</li> <li>plan-drift - Requirements alignment</li> <li>Custom behaviors</li> </ul> \ud83d\udd04 Self-Improving <p>Evolves agent prompts using GEPA (Genetic-Pareto):</p> <ul> <li>Automatic optimization</li> <li>Iterative improvement</li> <li>Performance tracking</li> <li>Reflective mutation</li> </ul> \ud83d\udcca Deep Evaluations <p>Comprehensive behavioral analysis:</p> <ul> <li>Multi-modal evaluation (static analysis, LLM, tests)</li> <li>Bloom-style scenario generation</li> <li>Root cause analysis</li> <li>Actionable insights</li> </ul> \ud83d\udd0c Multiple Usage Modes <p>Use CodeOptiX how you want:</p> <ul> <li>Local Check - Run when ready to test</li> <li>CI/CD Mode - Automated quality gates</li> <li>ACP Integration - Editor quality bridge</li> <li>Standalone API - Programmatic access</li> </ul> \ud83d\udcc8 Reproducible <p>All evaluations are versioned:</p> <ul> <li>Results tracking</li> <li>Reflection reports</li> <li>Evolution history</li> <li>Artifact management</li> </ul>"},{"location":"#who-uses-codeoptix","title":"\ud83d\udc65 Who Uses CodeOptiX?","text":"\ud83d\udc68\u200d\ud83d\udcbb Solo Developers <p>Pain Point: \"I generated code with Claude Code/Codex/Gemini. Should I ship it?\"</p> <p>Solution:</p> <ul> <li>Quick quality checks after feature completion</li> <li>Security scanning</li> <li>Test quality validation</li> <li>Multi-LLM critique</li> </ul> <p>Usage:</p> <pre><code>codeoptix eval --agent claude-code --behaviors insecure-code\n</code></pre> \ud83d\udc65 Engineering Teams <p>Pain Point: \"We need consistent quality gates for AI-generated code\"</p> <p>Solution:</p> <ul> <li>Automated CI/CD quality gates</li> <li>PR-level quality enforcement</li> <li>Team-wide standards</li> <li>Behavioral optimization at scale</li> </ul> <p>Usage:</p> <pre><code>codeoptix ci --agent codex --behaviors insecure-code --fail-on-failure\n</code></pre> \ud83d\udd12 Security &amp; Testing Teams <p>Pain Point: \"We need automated security and behavior validation\"</p> <p>Solution:</p> <ul> <li>Automated security scanning</li> <li>Behavior testing and validation</li> <li>Agent optimization</li> <li>Evaluation metrics and reporting</li> </ul> <p>Usage:</p> <pre><code>codeoptix ci \\\n  --agent claude-code \\\n  --behaviors insecure-code,vacuous-tests \\\n  --config .codeoptix/config.yaml\n</code></pre>"},{"location":"#how-it-works","title":"\ud83c\udfd7\ufe0f How It Works","text":"<p>CodeOptiX follows a simple Observe \u2192 Evaluate \u2192 Reflect \u2192 Evolve workflow:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Observe   \u2502 --&gt; \u2502  Evaluate   \u2502 --&gt; \u2502  Reflect    \u2502 --&gt; \u2502   Evolve    \u2502\n\u2502             \u2502     \u2502             \u2502     \u2502             \u2502     \u2502             \u2502\n\u2502 Capture     \u2502     \u2502 Measure     \u2502     \u2502 Generate    \u2502     \u2502 Optimize    \u2502\n\u2502 agent       \u2502     \u2502 against     \u2502     \u2502 insights    \u2502     \u2502 prompts     \u2502\n\u2502 behavior    \u2502     \u2502 behaviors   \u2502     \u2502 on failures \u2502     \u2502 with GEPA   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"#1-observe","title":"1. Observe","text":"<p>Capture agent behavior (code, tests, traces) from any coding agent.</p>"},{"location":"#2-evaluate","title":"2. Evaluate","text":"<p>Measure behavior against configurable specifications (security, test quality, plan alignment).</p>"},{"location":"#3-reflect","title":"3. Reflect","text":"<p>Generate actionable insights on failures with root cause analysis.</p>"},{"location":"#4-evolve","title":"4. Evolve","text":"<p>Optimize agent prompts using GEPA (Genetic-Pareto) for continuous improvement.</p>"},{"location":"#usage-modes","title":"\ud83d\ude80 Usage Modes","text":""},{"location":"#mode-1-local-check-primary","title":"Mode 1: Local Check (Primary)","text":"<p>When: You're ready to test your code after completing a feature/task</p> <pre><code># Single behavior (recommended for getting started)\ncodeoptix eval \\\n  --agent claude-code \\\n  --behaviors insecure-code \\\n  --llm-provider openai\n\n# Multiple behaviors\ncodeoptix eval \\\n  --agent codex \\\n  --behaviors insecure-code,vacuous-tests,plan-drift \\\n  --config examples/configs/basic.yaml\n</code></pre> <p>Features:</p> <ul> <li>\u2705 Regression testing</li> <li>\u2705 Code quality evaluation</li> <li>\u2705 Multi-LLM critique (optional different model judge)</li> <li>\u2705 Security scanning</li> <li>\u2705 Optimization suggestions</li> </ul>"},{"location":"#mode-2-cicd-integration-primary","title":"Mode 2: CI/CD Integration (Primary)","text":"<p>When: Teams want automated quality gates</p> <pre><code># CI/CD optimized command\ncodeoptix ci \\\n  --agent codex \\\n  --behaviors insecure-code \\\n  --fail-on-failure \\\n  --output-format summary\n</code></pre> <p>Features:</p> <ul> <li>\u2705 GitHub Actions integration</li> <li>\u2705 Proper exit codes for automation</li> <li>\u2705 Summary and JSON output formats</li> <li>\u2705 Fail-fast behavior</li> <li>\u2705 PR comments (coming soon)</li> </ul>"},{"location":"#mode-3-acp-integration-quality-bridge","title":"Mode 3: ACP Integration (Quality Bridge)","text":"<p>When: Users want quality checks integrated into editor workflow</p> <pre><code># Register CodeOptiX as ACP agent\ncodeoptix acp register\n\n# Use as quality bridge\ncodeoptix acp bridge --agent-name claude-code --auto-eval\n\n# Multi-agent judge\ncodeoptix acp judge \\\n  --generate-agent claude-code \\\n  --judge-agent grok \\\n  --prompt \"Write secure code\"\n</code></pre> <p>Features:</p> <ul> <li>\u2705 Quality bridge for editors (Zed, JetBrains, Neovim, VS Code)</li> <li>\u2705 Multi-agent judge (generate with one, judge with another)</li> <li>\u2705 Intelligent agent orchestration</li> <li>\u2705 Real-time quality feedback</li> <li>\u2705 Automatic code extraction</li> </ul>"},{"location":"#built-in-behaviors","title":"\ud83d\udccb Built-in Behaviors","text":"<p>CodeOptiX includes three built-in behavior specifications:</p> \ud83d\udd12 insecure-code <p>Detects security vulnerabilities:</p> <ul> <li>Hardcoded secrets</li> <li>SQL injection risks</li> <li>XSS vulnerabilities</li> <li>Insecure authentication</li> <li>And more...</li> </ul> <p>Usage:</p> <pre><code>codeoptix eval --behaviors insecure-code\n</code></pre> \ud83e\uddea vacuous-tests <p>Identifies low-quality tests:</p> <ul> <li>Missing assertions</li> <li>Trivial tests</li> <li>Test coverage issues</li> <li>Meaningless test cases</li> </ul> <p>Usage:</p> <pre><code>codeoptix eval --behaviors vacuous-tests\n</code></pre> \ud83d\udcd0 plan-drift <p>Detects requirements misalignment:</p> <ul> <li>Plan deviations</li> <li>Missing features</li> <li>Extra features</li> <li>Requirements drift</li> </ul> <p>Usage:</p> <pre><code>codeoptix eval --behaviors plan-drift --context plan.json\n</code></pre>"},{"location":"#example-use-cases","title":"\ud83d\udca1 Example Use Cases","text":""},{"location":"#security-auditing","title":"Security Auditing","text":"<p>Ensure your agent never writes insecure code:</p> <pre><code>codeoptix eval \\\n  --agent claude-code \\\n  --behaviors insecure-code \\\n  --fail-on-failure\n</code></pre>"},{"location":"#test-quality-validation","title":"Test Quality Validation","text":"<p>Verify your agent generates meaningful tests:</p> <pre><code>codeoptix eval \\\n  --agent codex \\\n  --behaviors vacuous-tests \\\n  --config examples/configs/single-behavior-vacuous-tests.yaml\n</code></pre>"},{"location":"#requirements-alignment","title":"Requirements Alignment","text":"<p>Check that your agent follows requirements:</p> <pre><code>codeoptix eval \\\n  --agent gemini-cli \\\n  --behaviors plan-drift \\\n  --context requirements.json\n</code></pre>"},{"location":"#cicd-quality-gates","title":"CI/CD Quality Gates","text":"<p>Automated quality checks in your pipeline:</p> <pre><code>codeoptix ci \\\n  --agent codex \\\n  --behaviors insecure-code,vacuous-tests \\\n  --fail-on-failure \\\n  --output-format summary\n</code></pre>"},{"location":"#installation","title":"\ud83d\udce6 Installation","text":""},{"location":"#using-pip","title":"Using pip","text":"<pre><code>pip install codeoptix\n</code></pre>"},{"location":"#using-uv-recommended","title":"Using uv (Recommended)","text":"<pre><code>uv pip install codeoptix\n</code></pre>"},{"location":"#from-source","title":"From Source","text":"<pre><code>git clone https://github.com/SuperagenticAI/codeoptix.git\ncd codeoptix\npip install -e .\n</code></pre>"},{"location":"#documentation","title":"\ud83d\udcda Documentation","text":""},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li>Installation - Set up CodeOptiX</li> <li>Quick Start - Your first evaluation (free with Ollama!)</li> <li>Single Behavior Quickstart - Simple API key-based testing</li> <li>Your First Evaluation - Step-by-step guide</li> </ul>"},{"location":"#core-concepts","title":"Core Concepts","text":"<ul> <li>Overview - Understanding CodeOptiX</li> <li>Agent Adapters - Connecting to agents</li> <li>Behavior Specifications - Defining behaviors</li> <li>Evaluation Engine - Running evaluations</li> <li>Reflection Engine - Understanding results</li> <li>Evolution Engine - Improving agents</li> <li>ACP Integration - Editor integration</li> </ul>"},{"location":"#guides","title":"Guides","text":"<ul> <li>CLI Usage - Command-line interface (including <code>ci</code> command)</li> <li>Python API - Use CodeOptiX in Python</li> <li>Configuration - Advanced configuration</li> <li>GitHub Actions - CI/CD integration</li> <li>ACP Integration - ACP protocol integration</li> <li>Custom Behaviors - Create custom behaviors</li> </ul>"},{"location":"#advanced","title":"Advanced","text":"<ul> <li>GEPA Integration - Genetic-Pareto optimization</li> <li>Bloom Integration - Scenario generation</li> <li>Error Handling - Handle errors gracefully</li> </ul>"},{"location":"#example-configurations","title":"\ud83c\udf93 Example Configurations","text":"<p>We provide ready-to-use configuration files:</p> <ul> <li><code>examples/configs/single-behavior-insecure-code.yaml</code> - Security checks only</li> <li><code>examples/configs/single-behavior-vacuous-tests.yaml</code> - Test quality only</li> <li><code>examples/configs/single-behavior-plan-drift.yaml</code> - Requirements alignment only</li> <li><code>examples/configs/ci-cd.yaml</code> - Optimized for CI/CD pipelines</li> <li><code>examples/configs/basic.yaml</code> - Minimal configuration</li> </ul> <p>See <code>examples/configs/</code> directory in the repository for configuration examples.</p>"},{"location":"#why-codeoptix","title":"\ud83c\udfc6 Why CodeOptiX?","text":"\ud83c\udfaf Agentic Code Optimizer <p>CodeOptiX acts as your Agentic Code Optimizer - automatically evaluating, testing, and optimizing code generated by any coding agent using GEPA and Bloom.</p> \ud83d\udd04 When You're Ready <p>Run CodeOptiX when you're ready to test - after completing a feature or task. No need to integrate into every step.</p> \ud83e\udd1d Works with Any Agent <p>Don't lock yourself into one agent. CodeOptiX works with Claude Code, Codex, Gemini CLI, and any ACP-compatible agent.</p> \ud83d\udcc8 Continuous Improvement <p>Evolve your agent prompts automatically using GEPA (Genetic-Pareto) for better results over time.</p>"},{"location":"#get-started-now","title":"\ud83d\ude80 Get Started NowReady to elevate your agent experience?","text":"\ud83d\ude80 Get Started  \u26a1 Quick Start  \ud83d\udce6 View Examples"},{"location":"#support-community","title":"\ud83d\udcde Support &amp; Community","text":"<ul> <li>\ud83d\udcd6 Documentation</li> <li>\ud83d\udcac GitHub Issues</li> <li>\ud83d\udcac GitHub Discussions</li> </ul>"},{"location":"#license","title":"\ud83d\udcc4 License","text":"<p>Apache License 2.0. See LICENSE for details.</p>"},{"location":"#about-superagentic-ai","title":"\ud83e\udd16 About Superagentic AI","text":"<p>CodeOptiX is proudly built by Superagentic AI - Advancing the future of AI agent optimization and autonomous systems.</p>"},{"location":"#our-mission","title":"Our Mission","text":"<p>We're pioneering intelligent agent optimization technologies to enhance developer productivity and code quality. CodeOptiX represents our commitment to building tools that make AI coding assistants more reliable, efficient, and trustworthy.</p>"},{"location":"#explore-more","title":"Explore More","text":"<ul> <li>Visit Superagentic AI - Learn about our mission and vision</li> <li>Our GitHub - Discover our other AI agent projects</li> </ul> <p>Brought to you by Superagentic AI</p> <p>\u2b50 Star us on GitHub</p>"},{"location":"api-reference/","title":"API Reference","text":"<p>Complete API reference for CodeOptiX.</p>"},{"location":"api-reference/#agent-adapters","title":"Agent Adapters","text":""},{"location":"api-reference/#create_adapteradapter_type-str-config-dictstr-any-agentadapter","title":"<code>create_adapter(adapter_type: str, config: Dict[str, Any]) -&gt; AgentAdapter</code>","text":"<p>Create an agent adapter.</p> <p>Parameters: - <code>adapter_type</code>: One of <code>\"claude-code\"</code>, <code>\"codex\"</code>, <code>\"gemini-cli\"</code> - <code>config</code>: Configuration dictionary</p> <p>Returns: - <code>AgentAdapter</code> instance</p> <p>Example: <pre><code>from codeoptix.adapters.factory import create_adapter\n\nadapter = create_adapter(\"codex\", {\n    \"llm_config\": {\n        \"provider\": \"openai\",\n        \"api_key\": \"sk-...\",\n    }\n})\n</code></pre></p>"},{"location":"api-reference/#evaluation-engine","title":"Evaluation Engine","text":""},{"location":"api-reference/#evaluationengine","title":"<code>EvaluationEngine</code>","text":"<p>Main orchestrator for behavioral evaluation.</p>"},{"location":"api-reference/#__init__adapter-llm_client-confignone","title":"<code>__init__(adapter, llm_client, config=None)</code>","text":"<p>Initialize evaluation engine.</p> <p>Parameters: - <code>adapter</code>: Agent adapter instance - <code>llm_client</code>: LLM client instance - <code>config</code>: Optional configuration dictionary</p>"},{"location":"api-reference/#evaluate_behaviorsbehavior_names-scenariosnone-contextnone-dict","title":"<code>evaluate_behaviors(behavior_names, scenarios=None, context=None) -&gt; Dict</code>","text":"<p>Evaluate agent against behaviors.</p> <p>Parameters: - <code>behavior_names</code>: List of behavior names - <code>scenarios</code>: Optional pre-generated scenarios - <code>context</code>: Optional context dictionary</p> <p>Returns: - Results dictionary</p> <p>Example: <pre><code>from codeoptix.evaluation import EvaluationEngine\n\nengine = EvaluationEngine(adapter, llm_client)\nresults = engine.evaluate_behaviors(\n    behavior_names=[\"insecure-code\"],\n    context={\"plan\": \"Create secure API\"}\n)\n</code></pre></p>"},{"location":"api-reference/#behavior-specifications","title":"Behavior Specifications","text":""},{"location":"api-reference/#create_behaviorname-str-config-optionaldict-none-behaviorspec","title":"<code>create_behavior(name: str, config: Optional[Dict] = None) -&gt; BehaviorSpec</code>","text":"<p>Create a behavior specification.</p> <p>Parameters: - <code>name</code>: Behavior name - one of: <code>\"insecure-code\"</code>, <code>\"vacuous-tests\"</code>, or <code>\"plan-drift\"</code> - <code>config</code>: Optional configuration</p> <p>Returns: - <code>BehaviorSpec</code> instance</p> <p>Example: <pre><code>from codeoptix.behaviors import create_behavior\n\nbehavior = create_behavior(\"insecure-code\")\nresult = behavior.evaluate(agent_output)\n</code></pre></p>"},{"location":"api-reference/#reflection-engine","title":"Reflection Engine","text":""},{"location":"api-reference/#reflectionengine","title":"<code>ReflectionEngine</code>","text":"<p>Generates reflection reports.</p>"},{"location":"api-reference/#__init__artifact_managernone-confignone","title":"<code>__init__(artifact_manager=None, config=None)</code>","text":"<p>Initialize reflection engine.</p>"},{"location":"api-reference/#reflectresults-agent_namenone-savetrue-str","title":"<code>reflect(results, agent_name=None, save=True) -&gt; str</code>","text":"<p>Generate reflection from results.</p> <p>Parameters: - <code>results</code>: Evaluation results dictionary - <code>agent_name</code>: Optional agent name for report - <code>save</code>: Whether to save reflection to file (default: True)</p> <p>Returns: - Reflection markdown string</p> <p>Example: <pre><code>from codeoptix.reflection import ReflectionEngine\n\nengine = ReflectionEngine(artifact_manager)\nreflection = engine.reflect(results, agent_name=\"codex\")\n</code></pre></p>"},{"location":"api-reference/#reflect_from_run_idrun_id-agent_namenone-str","title":"<code>reflect_from_run_id(run_id, agent_name=None) -&gt; str</code>","text":"<p>Generate reflection from a saved run ID.</p> <p>Parameters: - <code>run_id</code>: Run ID string - <code>agent_name</code>: Optional agent name for report</p> <p>Returns: - Reflection markdown string</p> <p>Example: <pre><code>reflection = engine.reflect_from_run_id(\"run-001\", agent_name=\"codex\")\n</code></pre></p>"},{"location":"api-reference/#evolution-engine","title":"Evolution Engine","text":""},{"location":"api-reference/#evolutionengine","title":"<code>EvolutionEngine</code>","text":"<p>Optimizes agent prompts.</p>"},{"location":"api-reference/#__init__adapter-evaluation_engine-llm_client-artifact_managernone-confignone","title":"<code>__init__(adapter, evaluation_engine, llm_client, artifact_manager=None, config=None)</code>","text":"<p>Initialize evolution engine.</p>"},{"location":"api-reference/#evolveevaluation_results-reflection-behavior_namesnone-contextnone-dict","title":"<code>evolve(evaluation_results, reflection, behavior_names=None, context=None) -&gt; Dict</code>","text":"<p>Evolve agent prompts using GEPA optimization.</p> <p>Parameters: - <code>evaluation_results</code>: Results from evaluation - <code>reflection</code>: Reflection string from reflection engine - <code>behavior_names</code>: Optional list of behaviors to optimize for - <code>context</code>: Optional context dictionary</p> <p>Returns: - Dictionary containing evolved prompts and metadata</p> <p>Example: <pre><code>from codeoptix.evolution import EvolutionEngine\n\nengine = EvolutionEngine(adapter, eval_engine, llm_client)\nevolved = engine.evolve(results, reflection, behavior_names=[\"insecure-code\"])\n</code></pre></p>"},{"location":"api-reference/#artifact-manager","title":"Artifact Manager","text":""},{"location":"api-reference/#artifactmanager","title":"<code>ArtifactManager</code>","text":"<p>Manages evaluation artifacts.</p>"},{"location":"api-reference/#save_resultsresults-run_idnone-path","title":"<code>save_results(results, run_id=None) -&gt; Path</code>","text":"<p>Save evaluation results to JSON file.</p> <p>Parameters: - <code>results</code>: Results dictionary to save - <code>run_id</code>: Optional run ID (generated if not provided)</p> <p>Returns: - Path to saved results file</p>"},{"location":"api-reference/#load_resultsrun_id-dict","title":"<code>load_results(run_id) -&gt; Dict</code>","text":"<p>Load evaluation results from file.</p> <p>Parameters: - <code>run_id</code>: Run ID to load</p> <p>Returns: - Results dictionary</p>"},{"location":"api-reference/#save_reflectionreflection_content-run_idnone-path","title":"<code>save_reflection(reflection_content, run_id=None) -&gt; Path</code>","text":"<p>Save reflection report to markdown file.</p> <p>Parameters: - <code>reflection_content</code>: Reflection markdown content - <code>run_id</code>: Optional run ID (generated if not provided)</p> <p>Returns: - Path to saved reflection file</p>"},{"location":"api-reference/#save_evolved_promptsevolved_prompts-run_idnone-path","title":"<code>save_evolved_prompts(evolved_prompts, run_id=None) -&gt; Path</code>","text":"<p>Save evolved prompts to YAML file.</p> <p>Parameters: - <code>evolved_prompts</code>: Evolved prompts dictionary - <code>run_id</code>: Optional run ID (generated if not provided)</p> <p>Returns: - Path to saved prompts file</p>"},{"location":"api-reference/#list_runs-listdict","title":"<code>list_runs() -&gt; List[Dict]</code>","text":"<p>List all evaluation runs with metadata.</p> <p>Returns: - List of dictionaries with run information (run_id, timestamp, score, behaviors)</p>"},{"location":"api-reference/#llm-client","title":"LLM Client","text":""},{"location":"api-reference/#create_llm_clientprovider-api_keynone-confignone-llmclient","title":"<code>create_llm_client(provider, api_key=None, config=None) -&gt; LLMClient</code>","text":"<p>Create an LLM client for the specified provider.</p> <p>Parameters: - <code>provider</code>: <code>LLMProvider.OPENAI</code>, <code>LLMProvider.ANTHROPIC</code>, <code>LLMProvider.GOOGLE</code>, or <code>LLMProvider.OLLAMA</code> - <code>api_key</code>: Optional API key (required for cloud providers, not needed for Ollama) - <code>config</code>: Optional configuration dictionary</p> <p>Returns: - Configured LLMClient instance</p> <p>Example: <pre><code>from codeoptix.utils.llm import create_llm_client, LLMProvider\n\n# For OpenAI\nclient = create_llm_client(LLMProvider.OPENAI, api_key=\"sk-...\")\nresponse = client.chat_completion(\n    messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n    model=\"gpt-5.2\"\n)\n\n# For Ollama (no API key needed)\nollama_client = create_llm_client(LLMProvider.OLLAMA)\n</code></pre></p>"},{"location":"api-reference/#llmprovider","title":"<code>LLMProvider</code>","text":"<p>Enum for supported LLM providers.</p> <p>Values: - <code>OPENAI</code>: OpenAI models (GPT-4, GPT-3.5) - <code>ANTHROPIC</code>: Anthropic models (Claude) - <code>GOOGLE</code>: Google models (Gemini) - <code>OLLAMA</code>: Local Ollama models</p>"},{"location":"api-reference/#error-handling","title":"Error Handling","text":""},{"location":"api-reference/#retry_llm_callmax_attempts3-initial_wait10-max_wait600","title":"<code>retry_llm_call(max_attempts=3, initial_wait=1.0, max_wait=60.0)</code>","text":"<p>Retry decorator for LLM calls.</p> <p>Example: <pre><code>from codeoptix.utils.retry import retry_llm_call\n\n@retry_llm_call(max_attempts=3)\ndef call_api():\n    # Your API call\n    pass\n</code></pre></p>"},{"location":"api-reference/#acp-agent-client-protocol-integration","title":"ACP (Agent Client Protocol) Integration","text":"<p>CodeOptiX supports ACP for editor integration and multi-agent workflows.</p>"},{"location":"api-reference/#acpagentregistry","title":"<code>ACPAgentRegistry</code>","text":"<p>Registry for managing ACP-compatible agents.</p>"},{"location":"api-reference/#registername-command-cwdnone-descriptionnone","title":"<code>register(name, command, cwd=None, description=None)</code>","text":"<p>Register a new ACP agent.</p>"},{"location":"api-reference/#unregistername","title":"<code>unregister(name)</code>","text":"<p>Remove an ACP agent from registry.</p>"},{"location":"api-reference/#list_agents-dict","title":"<code>list_agents() -&gt; Dict</code>","text":"<p>List all registered agents.</p>"},{"location":"api-reference/#get_agentname-dict","title":"<code>get_agent(name) -&gt; Dict</code>","text":"<p>Get agent configuration by name.</p>"},{"location":"api-reference/#acpqualitybridge","title":"<code>ACPQualityBridge</code>","text":"<p>Quality bridge for ACP workflows.</p>"},{"location":"api-reference/#__init__agent_command-behaviorsnone-auto_evaltrue","title":"<code>__init__(agent_command, behaviors=None, auto_eval=True)</code>","text":"<p>Initialize quality bridge.</p> <p>Parameters: - <code>agent_command</code>: Command to spawn ACP agent - <code>behaviors</code>: Comma-separated behavior names - <code>auto_eval</code>: Whether to auto-evaluate code quality</p>"},{"location":"api-reference/#codeoptixagent","title":"<code>CodeOptiXAgent</code>","text":"<p>ACP agent implementation for CodeOptiX.</p>"},{"location":"api-reference/#generate_codeprompt-kwargs-agentoutput","title":"<code>generate_code(prompt, **kwargs) -&gt; AgentOutput</code>","text":"<p>Generate code using registered adapters.</p> <p>For usage examples, see the Python API Guide.</p>"},{"location":"advanced/bloom/","title":"Bloom Integration","text":"<p>CodeOptiX integrates the Bloom framework for advanced scenario generation in behavioral evaluations.</p>"},{"location":"advanced/bloom/#what-is-bloom","title":"What is Bloom?","text":"<p>Bloom is an open-source framework originally developed by Anthropic for automated behavior evaluation of large language models. It provides sophisticated patterns for ideation, scenario generation, and behavioral testing.</p> <p>Learn more: Bloom Repository</p>"},{"location":"advanced/bloom/#why-bloom","title":"Why Bloom?","text":"<p>Bloom excels at: - Ideation: Generating diverse test scenarios for behaviors - Variation: Creating variations of base scenarios for comprehensive coverage - Behavioral Testing: Systematic evaluation of LLM behaviors</p>"},{"location":"advanced/bloom/#vendored-bloom","title":"Vendored Bloom","text":"<p>CodeOptiX includes a vendored version of Bloom components because:</p> <ol> <li>Not on PyPI: Bloom isn't available as a pip-installable package</li> <li>Custom Integration: CodeOptiX uses specific Bloom components adapted for its workflow</li> <li>Stability: Ensures consistent behavior across CodeOptiX versions</li> </ol> <p>The vendored code includes: - Scenario ideation prompts and scripts - Variation generation logic - Transcript utilities for evaluation tracking - Orchestrators for multi-turn conversations</p>"},{"location":"advanced/bloom/#bloom-integration-architecture","title":"Bloom Integration Architecture","text":""},{"location":"advanced/bloom/#codeoptix-workflow-with-bloom","title":"CodeOptiX Workflow with Bloom","text":"<pre><code>graph TD\n    A[User Request] --&gt; B[Evaluation Engine]\n    B --&gt; C[Scenario Generator]\n    C --&gt; D{use_bloom?}\n\n    D --&gt;|Yes| E[BloomIdeationIntegration]\n    D --&gt;|No| F[Simple Generator]\n\n    E --&gt; G[Base Scenarios]\n    E --&gt; H[Variations]\n\n    G --&gt; I[Evaluation]\n    H --&gt; I\n\n    F --&gt; I\n    I --&gt; J[Behavior Results]</code></pre>"},{"location":"advanced/bloom/#key-components","title":"Key Components","text":"<ol> <li>BloomIdeationIntegration (<code>evaluation/bloom_integration.py</code>)</li> <li>Main integration point</li> <li>Uses vendored Bloom prompts and scripts</li> <li> <p>Bridges CodeOptiX LLM client with Bloom's format</p> </li> <li> <p>Scenario Generator (<code>evaluation/scenario_generator.py</code>)</p> </li> <li>Orchestrates scenario generation</li> <li>Configures Bloom integration</li> <li> <p>Provides fallback for reliability</p> </li> <li> <p>Vendored Bloom Code (<code>vendor/bloom/</code>)</p> </li> <li><code>prompts/step2_ideation.py</code> - Ideation prompts</li> <li><code>scripts/step2_ideation.py</code> - Scenario generation logic</li> <li><code>utils.py</code> - Bloom utilities and helpers</li> </ol>"},{"location":"advanced/bloom/#bloom-in-codeoptix","title":"Bloom in CodeOptiX","text":""},{"location":"advanced/bloom/#lightweight-integration","title":"Lightweight Integration","text":"<p>CodeOptiX uses Bloom-style patterns by default:</p> <pre><code>config = {\n    \"scenario_generator\": {\n        \"use_bloom\": True  # Default\n    }\n}\n</code></pre>"},{"location":"advanced/bloom/#full-bloom-integration","title":"Full Bloom Integration","text":"<p>Enable full Bloom integration for advanced scenario generation:</p> <pre><code>config = {\n    \"scenario_generator\": {\n        \"use_bloom\": True,\n        \"use_full_bloom\": True,  # Full Bloom integration\n        \"num_base_scenarios\": 3,\n        \"num_variations\": 2\n    }\n}\n</code></pre>"},{"location":"advanced/bloom/#bloom-ideation-process","title":"Bloom Ideation Process","text":""},{"location":"advanced/bloom/#step-1-base-scenario-generation","title":"Step 1: Base Scenario Generation","text":"<p>Bloom generates base scenarios using structured prompts:</p> <pre><code># System prompt defines the ideation approach\nsystem_prompt = make_system_prompt(\n    behavior_name=\"insecure-code\",\n    behavior_description=\"Detect security vulnerabilities in code\"\n)\n\n# User prompt requests specific scenarios\nscenarios_prompt = make_all_scenarios_prompt(\n    behavior_name=\"insecure-code\",\n    total_scenarios=3,\n    examples=[]  # Optional examples for few-shot learning\n)\n\n# LLM generates scenarios\nresponse = llm_client.chat_completion([\n    {\"role\": \"system\", \"content\": system_prompt},\n    {\"role\": \"user\", \"content\": scenarios_prompt}\n])\n</code></pre> <p>Expected Output Format: <pre><code>[\n  {\n    \"task\": \"SQL Injection Test\",\n    \"prompt\": \"Write a database query function that handles user input\",\n    \"expected_issues\": [\"sql_injection\"]\n  },\n  {\n    \"task\": \"Secrets Management\",\n    \"prompt\": \"Create a configuration class for API keys\",\n    \"expected_issues\": [\"hardcoded_secrets\"]\n  }\n]\n</code></pre></p>"},{"location":"advanced/bloom/#step-2-scenario-variations","title":"Step 2: Scenario Variations","text":"<p>For each base scenario, Bloom generates variations:</p> <pre><code>variation_system_prompt = make_variation_system_prompt(\n    behavior_name=\"insecure-code\",\n    behavior_description=\"Detect security vulnerabilities\"\n)\n\nvariation_prompt = make_variation_prompt(\n    base_scenario_description=\"Write a database query function\",\n    num_perturbations=3\n)\n\n# Generate variations\nvariations = llm_client.chat_completion([\n    {\"role\": \"system\", \"content\": variation_system_prompt},\n    {\"role\": \"user\", \"content\": variation_prompt}\n])\n</code></pre>"},{"location":"advanced/bloom/#step-3-response-parsing","title":"Step 3: Response Parsing","text":"<p>Bloom includes robust parsing for LLM responses:</p> <ul> <li>JSON Parsing: Extracts structured scenarios from JSON responses</li> <li>Text Parsing: Falls back to regex-based parsing for unstructured text</li> <li>Normalization: Converts various formats to standard CodeOptiX scenario format</li> </ul>"},{"location":"advanced/bloom/#how-bloom-works","title":"How Bloom Works","text":""},{"location":"advanced/bloom/#1-ideation","title":"1. Ideation","text":"<p>Bloom generates base scenarios using ideation prompts:</p> <pre><code>from codeoptix.evaluation.bloom_integration import BloomIdeationIntegration\n\nbloom = BloomIdeationIntegration(llm_client, config)\nscenarios = bloom.generate_scenarios(\n    behavior_name=\"insecure-code\",\n    behavior_description=\"Detect insecure code\"\n)\n</code></pre>"},{"location":"advanced/bloom/#2-variation","title":"2. Variation","text":"<p>Bloom creates variations of base scenarios:</p> <pre><code>variations = bloom._generate_variations(\n    base_scenario=scenario,\n    behavior_name=\"insecure-code\",\n    behavior_description=\"Detect insecure code\"\n)\n</code></pre>"},{"location":"advanced/bloom/#using-bloom","title":"Using Bloom","text":""},{"location":"advanced/bloom/#enable-bloom","title":"Enable Bloom","text":"<pre><code>config = {\n    \"scenario_generator\": {\n        \"use_bloom\": True,\n        \"use_full_bloom\": True\n    }\n}\n\nengine = EvaluationEngine(adapter, llm_client, config=config)\n</code></pre>"},{"location":"advanced/bloom/#bloom-configuration-options","title":"Bloom Configuration Options","text":"<pre><code>config = {\n    \"scenario_generator\": {\n        # Enable Bloom-style generation (default: True)\n        \"use_bloom\": True,\n\n        # Use full Bloom integration vs simple patterns (default: True)\n        \"use_full_bloom\": True,\n\n        # Number of base scenarios to generate (default: 3)\n        \"num_base_scenarios\": 5,\n\n        # Number of variations per base scenario (default: 2)\n        \"num_variations\": 3,\n\n        # Model to use for generation (default: \"gpt-4o\")\n        \"model\": \"gpt-4o\",\n\n        # Temperature for creative generation (default: 0.8)\n        \"temperature\": 0.8\n    }\n}\n</code></pre>"},{"location":"advanced/bloom/#configuration-reference","title":"Configuration Reference","text":"Option Type Default Description <code>use_bloom</code> bool <code>true</code> Enable Bloom-style generation <code>use_full_bloom</code> bool <code>true</code> Use full Bloom vs simple patterns <code>num_base_scenarios</code> int <code>3</code> Base scenarios to generate <code>num_variations</code> int <code>2</code> Variations per base scenario <code>model</code> str <code>\"gpt-4o\"</code> LLM model for generation <code>temperature</code> float <code>0.8</code> Generation creativity (0.0-1.0)"},{"location":"advanced/bloom/#bloom-scenario-generation","title":"Bloom Scenario Generation","text":""},{"location":"advanced/bloom/#base-scenarios","title":"Base Scenarios","text":"<p>Bloom generates diverse base scenarios:</p> <pre><code>base_scenarios = bloom._generate_base_scenarios(\n    behavior_name=\"insecure-code\",\n    behavior_description=\"Detect insecure code\",\n    examples=[]\n)\n</code></pre>"},{"location":"advanced/bloom/#variations","title":"Variations","text":"<p>Bloom creates variations for each base scenario:</p> <pre><code>for base_scenario in base_scenarios:\n    variations = bloom._generate_variations(\n        base_scenario=base_scenario,\n        behavior_name=\"insecure-code\",\n        behavior_description=\"Detect insecure code\"\n    )\n</code></pre>"},{"location":"advanced/bloom/#vendored-bloom-components","title":"Vendored Bloom Components","text":"<p>CodeOptiX includes carefully selected Bloom components:</p>"},{"location":"advanced/bloom/#core-components-used","title":"Core Components Used","text":""},{"location":"advanced/bloom/#prompts-vendorbloomprompts","title":"Prompts (<code>vendor/bloom/prompts/</code>)","text":"<ul> <li><code>step2_ideation.py</code>: Contains prompt templates for scenario ideation</li> <li><code>make_system_prompt()</code> - System prompt for ideation</li> <li><code>make_all_scenarios_prompt()</code> - Generates multiple scenarios</li> <li><code>make_variation_prompt()</code> - Creates scenario variations</li> </ul>"},{"location":"advanced/bloom/#scripts-vendorbloomscripts","title":"Scripts (<code>vendor/bloom/scripts/</code>)","text":"<ul> <li><code>step2_ideation.py</code>: Core ideation logic</li> <li><code>parse_scenarios_response()</code> - Parses LLM responses into scenarios</li> <li><code>parse_variations_response()</code> - Parses variation responses</li> <li>Scenario generation orchestration</li> </ul>"},{"location":"advanced/bloom/#utilities-vendorbloomutilspy","title":"Utilities (<code>vendor/bloom/utils.py</code>)","text":"<ul> <li>Model utilities: Model name/ID conversions, thinking capabilities</li> <li>Configuration: Loading Bloom-specific configs</li> <li>Transcript handling: Managing evaluation transcripts</li> <li>Result management: Saving/loading ideation results</li> </ul>"},{"location":"advanced/bloom/#integration-points","title":"Integration Points","text":""},{"location":"advanced/bloom/#bloomideationintegration-class","title":"BloomIdeationIntegration Class","text":"<pre><code># Located in: src/codeoptix/evaluation/bloom_integration.py\nclass BloomIdeationIntegration:\n    def generate_scenarios(self, behavior_name, behavior_description, examples):\n        # 1. Generate base scenarios using Bloom ideation\n        base_scenarios = self._generate_base_scenarios(...)\n\n        # 2. Generate variations for each scenario\n        for scenario in base_scenarios:\n            variations = self._generate_variations(scenario, ...)\n\n        return all_scenarios\n</code></pre>"},{"location":"advanced/bloom/#scenario-generator-integration","title":"Scenario Generator Integration","text":"<pre><code># Located in: src/codeoptix/evaluation/scenario_generator.py\nclass BloomScenarioGenerator(ScenarioGenerator):\n    def __init__(self, llm_client, config):\n        self.use_full_bloom = config.get(\"use_full_bloom\", True)\n        if self.use_full_bloom:\n            self.bloom_integration = BloomIdeationIntegration(llm_client, config)\n</code></pre>"},{"location":"advanced/bloom/#why-vendored-not-gitignored","title":"Why Vendored, Not Gitignored?","text":"<p>Bloom should NOT be gitignored because:</p> <ol> <li>Package Dependency: Bloom components are integral to CodeOptiX's functionality</li> <li>Not Available Elsewhere: Bloom isn't published on PyPI</li> <li>Custom Adaptations: CodeOptiX modifies Bloom components for its workflow</li> <li>Version Stability: Ensures consistent behavior across environments</li> </ol> <p>The vendor directory follows Python packaging best practices for including third-party code that's not available via pip.</p>"},{"location":"advanced/bloom/#example","title":"Example","text":"<pre><code>from codeoptix.evaluation import EvaluationEngine\n\nconfig = {\n    \"scenario_generator\": {\n        \"use_bloom\": True,\n        \"use_full_bloom\": True,\n        \"num_scenarios\": 5\n    }\n}\n\nengine = EvaluationEngine(adapter, llm_client, config=config)\nresults = engine.evaluate_behaviors([\"insecure-code\"])\n</code></pre>"},{"location":"advanced/bloom/#when-to-use-bloom","title":"When to Use Bloom","text":""},{"location":"advanced/bloom/#use-bloom-for","title":"\u2705 Use Bloom For:","text":"<ul> <li>Complex Behaviors: Behaviors requiring diverse scenario coverage</li> <li>Research/Development: When exploring edge cases and variations</li> <li>High-Stakes Evaluation: Critical behavior validation</li> <li>Model Comparison: Systematic evaluation across different LLMs</li> </ul>"},{"location":"advanced/bloom/#disable-bloom-for","title":"\u274c Disable Bloom For:","text":"<ul> <li>Simple Behaviors: Basic checks (e.g., syntax validation)</li> <li>Performance-Critical: CI/CD pipelines needing speed</li> <li>Resource Constraints: Limited API quotas or compute</li> <li>Deterministic Testing: When you need reproducible results</li> </ul>"},{"location":"advanced/bloom/#performance-comparison","title":"Performance Comparison","text":"Mode Scenarios Generated API Calls Time Use Case No Bloom 3-5 1-2 Fast Simple evaluation Simple Bloom 5-10 2-3 Medium Balanced coverage Full Bloom 15-30 5-10 Slow Comprehensive testing"},{"location":"advanced/bloom/#best-practices","title":"Best Practices","text":""},{"location":"advanced/bloom/#1-use-full-bloom-for-complex-behaviors","title":"1. Use Full Bloom for Complex Behaviors","text":"<p>For complex behaviors, use full Bloom:</p> <pre><code>config = {\n    \"scenario_generator\": {\n        \"use_full_bloom\": True\n    }\n}\n</code></pre>"},{"location":"advanced/bloom/#2-adjust-scenario-count","title":"2. Adjust Scenario Count","text":"<p>Balance between coverage and speed:</p> <pre><code>config = {\n    \"scenario_generator\": {\n        \"num_scenarios\": 3  # Good balance\n    }\n}\n</code></pre>"},{"location":"advanced/bloom/#3-provide-examples","title":"3. Provide Examples","text":"<p>Provide example scenarios for better generation:</p> <pre><code>scenarios = engine.evaluate_behaviors(\n    behavior_names=[\"insecure-code\"],\n    scenarios=example_scenarios  # Provide examples\n)\n</code></pre>"},{"location":"advanced/bloom/#troubleshooting","title":"Troubleshooting","text":""},{"location":"advanced/bloom/#bloom-not-working","title":"Bloom Not Working","text":"<p>Check configuration:</p> <pre><code>config = {\n    \"scenario_generator\": {\n        \"use_bloom\": True  # Must be enabled\n    }\n}\n</code></pre>"},{"location":"advanced/bloom/#too-many-scenarios","title":"Too Many Scenarios","text":"<p>Reduce scenario count:</p> <pre><code>config = {\n    \"scenario_generator\": {\n        \"num_scenarios\": 2  # Reduce count\n    }\n}\n</code></pre>"},{"location":"advanced/bloom/#licensing-and-attribution","title":"Licensing and Attribution","text":"<p>The vendored Bloom components are used under their original license terms. CodeOptiX includes Bloom components for integration purposes only and does not claim ownership of the original Bloom framework.</p> <p>For the original Bloom project, see: Anthropic's Bloom Framework</p>"},{"location":"advanced/bloom/#troubleshooting-bloom-integration","title":"Troubleshooting Bloom Integration","text":""},{"location":"advanced/bloom/#common-issues","title":"Common Issues","text":""},{"location":"advanced/bloom/#bloom-generation-failing","title":"Bloom Generation Failing","text":"<pre><code># Check configuration\nconfig = {\n    \"scenario_generator\": {\n        \"use_bloom\": True,\n        \"use_full_bloom\": False  # Try simple mode first\n    }\n}\n\n# Check LLM connectivity\ntry:\n    response = llm_client.chat_completion([{\"role\": \"user\", \"content\": \"test\"}])\nexcept Exception as e:\n    print(f\"LLM Error: {e}\")\n</code></pre>"},{"location":"advanced/bloom/#too-many-scenarios-generated","title":"Too Many Scenarios Generated","text":"<pre><code>config = {\n    \"scenario_generator\": {\n        \"num_base_scenarios\": 2,  # Reduce count\n        \"num_variations\": 1      # Reduce variations\n    }\n}\n</code></pre>"},{"location":"advanced/bloom/#inconsistent-results","title":"Inconsistent Results","text":"<pre><code>config = {\n    \"scenario_generator\": {\n        \"temperature\": 0.1  # Lower for consistency\n    }\n}\n</code></pre>"},{"location":"advanced/bloom/#next-steps","title":"Next Steps","text":"<ul> <li>Evaluation Engine - Learn about evaluation</li> <li>GEPA Integration - Prompt evolution</li> <li>Error Handling - Handle errors</li> </ul>"},{"location":"advanced/error-handling/","title":"Error Handling","text":"<p>CodeOptiX includes robust error handling with retry logic and graceful degradation.</p>"},{"location":"advanced/error-handling/#retry-logic","title":"Retry Logic","text":"<p>CodeOptiX automatically retries failed API calls with exponential backoff.</p>"},{"location":"advanced/error-handling/#automatic-retries","title":"Automatic Retries","text":"<p>LLM calls are automatically retried on failure:</p> <pre><code>from codeoptix.utils.llm import create_llm_client\n\nclient = create_llm_client(LLMProvider.OPENAI)\n# Automatically retries on failure\nresponse = client.chat_completion(...)\n</code></pre>"},{"location":"advanced/error-handling/#custom-retry-logic","title":"Custom Retry Logic","text":"<p>Use retry decorators for custom logic:</p> <pre><code>from codeoptix.utils.retry import retry_llm_call\n\n@retry_llm_call(max_attempts=3, initial_wait=1.0, max_wait=60.0)\ndef my_api_call():\n    # Your API call\n    pass\n</code></pre>"},{"location":"advanced/error-handling/#error-types","title":"Error Types","text":""},{"location":"advanced/error-handling/#apierror","title":"APIError","text":"<p>General API errors:</p> <pre><code>from codeoptix.utils.retry import APIError\n\ntry:\n    result = llm_client.chat_completion(...)\nexcept APIError as e:\n    print(f\"API error: {e}\")\n</code></pre>"},{"location":"advanced/error-handling/#ratelimiterror","title":"RateLimitError","text":"<p>Rate limit errors:</p> <pre><code>from codeoptix.utils.retry import RateLimitError\n\ntry:\n    result = llm_client.chat_completion(...)\nexcept RateLimitError as e:\n    print(\"Rate limit exceeded. Please wait.\")\n</code></pre>"},{"location":"advanced/error-handling/#timeouterror","title":"TimeoutError","text":"<p>Timeout errors:</p> <pre><code>from codeoptix.utils.retry import TimeoutError\n\ntry:\n    result = llm_client.chat_completion(...)\nexcept TimeoutError as e:\n    print(\"Request timed out. Try again.\")\n</code></pre>"},{"location":"advanced/error-handling/#user-friendly-error-messages","title":"User-Friendly Error Messages","text":""},{"location":"advanced/error-handling/#generate-error-messages","title":"Generate Error Messages","text":"<pre><code>from codeoptix.utils.retry import handle_api_error\n\ntry:\n    result = llm_client.chat_completion(...)\nexcept Exception as e:\n    message = handle_api_error(e, context=\"LLM call\")\n    print(message)\n</code></pre>"},{"location":"advanced/error-handling/#error-message-examples","title":"Error Message Examples","text":"<ul> <li>Rate limit: \"Rate limit exceeded. Please wait before retrying.\"</li> <li>Timeout: \"Request timed out. The service may be slow. Try again.\"</li> <li>Authentication: \"Authentication failed. Please check your API key.\"</li> <li>Quota: \"API quota exceeded. Please check your account limits.\"</li> </ul>"},{"location":"advanced/error-handling/#graceful-degradation","title":"Graceful Degradation","text":""},{"location":"advanced/error-handling/#fallback-scenarios","title":"Fallback Scenarios","text":"<p>If scenario generation fails, CodeOptiX uses fallback scenarios:</p> <pre><code># Automatic fallback\nscenarios = generator.generate_scenarios(...)\n# Falls back to simple scenarios on error\n</code></pre>"},{"location":"advanced/error-handling/#fallback-evaluation","title":"Fallback Evaluation","text":"<p>If evaluation fails, CodeOptiX continues with other scenarios:</p> <pre><code># Continues with other scenarios\nresults = engine.evaluate_behaviors(...)\n</code></pre>"},{"location":"advanced/error-handling/#best-practices","title":"Best Practices","text":""},{"location":"advanced/error-handling/#1-handle-errors-explicitly","title":"1. Handle Errors Explicitly","text":"<p>Always handle errors in your code:</p> <pre><code>try:\n    results = engine.evaluate_behaviors(...)\nexcept Exception as e:\n    print(f\"Evaluation failed: {e}\")\n    # Handle error\n</code></pre>"},{"location":"advanced/error-handling/#2-use-retry-logic","title":"2. Use Retry Logic","text":"<p>Use retry logic for transient errors:</p> <pre><code>@retry_llm_call(max_attempts=3)\ndef critical_call():\n    # Your critical API call\n    pass\n</code></pre>"},{"location":"advanced/error-handling/#3-check-results","title":"3. Check Results","text":"<p>Always check if results are valid:</p> <pre><code>results = engine.evaluate_behaviors(...)\nif not results or \"behaviors\" not in results:\n    print(\"Evaluation failed\")\n</code></pre>"},{"location":"advanced/error-handling/#configuration","title":"Configuration","text":""},{"location":"advanced/error-handling/#retry-configuration","title":"Retry Configuration","text":"<pre><code>config = {\n    \"retry\": {\n        \"max_attempts\": 3,\n        \"initial_wait\": 1.0,\n        \"max_wait\": 60.0\n    }\n}\n</code></pre>"},{"location":"advanced/error-handling/#troubleshooting","title":"Troubleshooting","text":""},{"location":"advanced/error-handling/#too-many-retries","title":"Too Many Retries","text":"<p>Reduce retry attempts:</p> <pre><code>@retry_llm_call(max_attempts=2)  # Reduce attempts\ndef call():\n    pass\n</code></pre>"},{"location":"advanced/error-handling/#slow-responses","title":"Slow Responses","text":"<p>Increase timeout:</p> <pre><code>config = {\n    \"timeout\": 120  # Increase timeout\n}\n</code></pre>"},{"location":"advanced/error-handling/#next-steps","title":"Next Steps","text":"<ul> <li>Python API Guide - Error handling in Python</li> <li>CLI Usage Guide - CLI error handling</li> <li>Configuration Guide - Error handling configuration</li> </ul>"},{"location":"advanced/gepa/","title":"GEPA Integration","text":"<p>CodeOptiX uses GEPA (Genetic-Pareto) to automatically improve agent prompts through reflective mutation.</p>"},{"location":"advanced/gepa/#what-is-gepa","title":"What is GEPA?","text":"<p>GEPA (Genetic-Pareto) is a framework for optimizing textual system components (like AI prompts, code, or instructions) using LLM-based reflection and evolutionary search. GEPA employs iterative mutation, reflection, and Pareto-aware candidate selection to evolve robust, high-performing variants.</p> <p>Learn more: GEPA Repository</p> <p>Note: CodeOptiX uses GEPA's <code>InstructionProposalSignature</code> component for prompt evolution. This is a minimal integration that uses GEPA's proven instruction proposal mechanism, rather than the full GEPA optimization framework.</p>"},{"location":"advanced/gepa/#how-gepa-works-in-codeoptix","title":"How GEPA Works in CodeOptiX","text":""},{"location":"advanced/gepa/#1-reflective-dataset","title":"1. Reflective Dataset","text":"<p>GEPA uses evaluation failures to create a reflective dataset:</p> <pre><code>reflective_dataset = [\n    {\n        \"behavior\": \"insecure-code\",\n        \"score\": 0.5,\n        \"evidence\": [\"Hardcoded password found\"],\n        \"scenario\": \"Write database connection code\"\n    }\n]\n</code></pre>"},{"location":"advanced/gepa/#2-instruction-proposal","title":"2. Instruction Proposal","text":"<p>GEPA's <code>InstructionProposalSignature</code> generates improved prompts:</p> <pre><code>from gepa.strategies.instruction_proposal import InstructionProposalSignature\n\nresult = InstructionProposalSignature.run(\n    lm=llm_client,\n    input_dict={\n        \"current_instruction_doc\": current_prompt,\n        \"dataset_with_feedback\": reflective_dataset,\n    }\n)\n\nnew_prompt = result[\"new_instruction\"]\n</code></pre>"},{"location":"advanced/gepa/#3-iterative-improvement","title":"3. Iterative Improvement","text":"<p>The process repeats until no improvement is found.</p>"},{"location":"advanced/gepa/#using-gepa-in-codeoptix","title":"Using GEPA in CodeOptiX","text":""},{"location":"advanced/gepa/#enable-gepa","title":"Enable GEPA","text":"<p>GEPA is enabled by default in the Evolution Engine:</p> <pre><code>config = {\n    \"proposer\": {\n        \"use_gepa\": True,  # Default\n        \"model\": \"gpt-5.2\"\n    }\n}\n\nevolution_engine = EvolutionEngine(\n    adapter, eval_engine, llm_client, artifact_manager, config=config\n)\n</code></pre>"},{"location":"advanced/gepa/#gepa-configuration","title":"GEPA Configuration","text":"<pre><code>config = {\n    \"proposer\": {\n        \"use_gepa\": True,\n        \"model\": \"gpt-5.2\",\n        \"temperature\": 0.7,\n        \"prompt_template\": \"custom_template\"  # Optional\n    }\n}\n</code></pre>"},{"location":"advanced/gepa/#gepa-integration-details","title":"GEPA Integration Details","text":""},{"location":"advanced/gepa/#minimalgepaproposer","title":"MinimalGEPAProposer","text":"<p>CodeOptiX uses <code>MinimalGEPAProposer</code> which wraps GEPA's <code>InstructionProposalSignature</code>:</p> <pre><code>from codeoptix.evolution.gepa_integration import MinimalGEPAProposer\n\nproposer = MinimalGEPAProposer(llm_client, config)\nimproved_prompt = proposer.propose_improved_prompt(\n    current_prompt=current_prompt,\n    reflective_dataset=reflective_dataset\n)\n</code></pre>"},{"location":"advanced/gepa/#data-format-conversion","title":"Data Format Conversion","text":"<p>CodeOptiX converts its format to GEPA's expected format:</p> <pre><code>gepa_dataset = [\n    {\n        \"Inputs\": {\n            \"task\": scenario[\"prompt\"],\n            \"behavior\": behavior_name\n        },\n        \"Generated Outputs\": {\n            \"score\": score,\n            \"evidence\": evidence\n        },\n        \"Feedback\": feedback_string\n    }\n]\n</code></pre>"},{"location":"advanced/gepa/#example-gepa-evolution","title":"Example: GEPA Evolution","text":"<pre><code>from codeoptix.evolution import EvolutionEngine\n\n# Initialize with GEPA enabled\nconfig = {\n    \"max_iterations\": 3,\n    \"proposer\": {\n        \"use_gepa\": True\n    }\n}\n\nevolution_engine = EvolutionEngine(\n    adapter, eval_engine, llm_client, artifact_manager, config=config\n)\n\n# Evolve using GEPA\nevolved = evolution_engine.evolve(\n    evaluation_results=results,\n    reflection=reflection_content\n)\n\nprint(evolved[\"prompts\"][\"system_prompt\"])\n</code></pre>"},{"location":"advanced/gepa/#gepa-vs-custom-implementation","title":"GEPA vs Custom Implementation","text":""},{"location":"advanced/gepa/#gepa-default","title":"GEPA (Default)","text":"<ul> <li>Uses proven GEPA framework</li> <li>LLM-based prompt generation</li> <li>Handles complex patterns</li> </ul>"},{"location":"advanced/gepa/#custom-implementation","title":"Custom Implementation","text":"<p>Disable GEPA to use custom logic:</p> <pre><code>config = {\n    \"proposer\": {\n        \"use_gepa\": False  # Use custom implementation\n    }\n}\n</code></pre>"},{"location":"advanced/gepa/#best-practices","title":"Best Practices","text":""},{"location":"advanced/gepa/#1-provide-good-reflection","title":"1. Provide Good Reflection","text":"<p>Better reflection leads to better evolution:</p> <pre><code>reflection = reflection_engine.reflect(results)\nevolved = evolution_engine.evolve(results, reflection)\n</code></pre>"},{"location":"advanced/gepa/#2-use-appropriate-llm","title":"2. Use Appropriate LLM","text":"<p>GEPA works best with capable models:</p> <pre><code>config = {\n    \"proposer\": {\n        \"use_gepa\": True,\n        \"model\": \"gpt-5.2\"  # Use capable model\n    }\n}\n</code></pre>"},{"location":"advanced/gepa/#3-iterate-gradually","title":"3. Iterate Gradually","text":"<p>Start with few iterations:</p> <pre><code>config = {\n    \"max_iterations\": 2  # Start small\n}\n</code></pre>"},{"location":"advanced/gepa/#troubleshooting","title":"Troubleshooting","text":""},{"location":"advanced/gepa/#gepa-not-working","title":"GEPA Not Working","text":"<p>Check that GEPA is enabled:</p> <pre><code>config = {\n    \"proposer\": {\n        \"use_gepa\": True  # Must be True\n    }\n}\n</code></pre>"},{"location":"advanced/gepa/#no-improvement","title":"No Improvement","text":"<p>GEPA may not find improvement if: - Initial prompt is already good - Failures are not pattern-based - LLM cannot identify improvements</p>"},{"location":"advanced/gepa/#next-steps","title":"Next Steps","text":"<ul> <li>Evolution Engine - Learn about evolution</li> <li>Bloom Integration - Scenario generation</li> <li>Error Handling - Handle errors</li> </ul>"},{"location":"advanced/performance/","title":"Performance Guide","text":"<p>Optimizing CodeOptiX performance for different use cases.</p>"},{"location":"advanced/performance/#performance-optimization-strategies","title":"Performance Optimization Strategies","text":""},{"location":"advanced/performance/#for-local-development","title":"For Local Development","text":"<p>Use Ollama for fastest evaluation:</p> <pre><code># Install Ollama and pull a model\nollama pull llama3.2\n\n# Run evaluation (no API key needed)\ncodeoptix eval \\\n  --agent claude-code \\\n  --behaviors insecure-code \\\n  --llm-provider ollama\n</code></pre> <p>Benefits: - \u2705 No API rate limits - \u2705 No network latency - \u2705 No API costs - \u2705 Runs entirely locally</p>"},{"location":"advanced/performance/#for-cicd-pipelines","title":"For CI/CD Pipelines","text":"<p>Use the <code>ci</code> command optimized for automation:</p> <pre><code>codeoptix ci \\\n  --agent codex \\\n  --behaviors insecure-code \\\n  --fail-on-failure \\\n  --output-format summary\n</code></pre> <p>Configuration for CI: <pre><code>evaluation:\n  scenario_generator:\n    num_scenarios: 2  # Reduce from default 5\n    use_bloom: false  # Disable complex scenario generation\n\nevolution:\n  max_iterations: 1   # Reduce evolution iterations\n</code></pre></p>"},{"location":"advanced/performance/#for-production-evaluation","title":"For Production Evaluation","text":"<p>Use cloud providers with optimized settings:</p> <pre><code>adapter:\n  llm_config:\n    provider: anthropic\n    model: claude-opus-4-5-20251101  # Latest Opus model\n    temperature: 0.1  # Lower temperature for consistency\n\nevaluation:\n  scenario_generator:\n    num_scenarios: 3\n    use_bloom: true\n\n  static_analysis:\n    bandit: true\n    safety: true\n    # Disable slower linters\n    pylint: false\n    mypy: false\n</code></pre>"},{"location":"advanced/performance/#benchmarking-performance","title":"Benchmarking Performance","text":""},{"location":"advanced/performance/#run-performance-tests","title":"Run Performance Tests","text":"<pre><code># Time a full evaluation\ntime codeoptix eval \\\n  --agent codex \\\n  --behaviors \"insecure-code,vacuous-tests\" \\\n  --llm-provider openai\n</code></pre>"},{"location":"advanced/performance/#profile-memory-usage","title":"Profile Memory Usage","text":"<pre><code># Monitor memory during evaluation\ncodeoptix eval --agent codex --behaviors insecure-code &amp;\npid=$!\nwhile kill -0 $pid 2&gt;/dev/null; do\n    ps -o pid,ppid,cmd,%mem,%cpu -p $pid\n    sleep 1\ndone\n</code></pre>"},{"location":"advanced/performance/#compare-different-configurations","title":"Compare Different Configurations","text":"<pre><code># Test with different providers\nfor provider in openai anthropic google ollama; do\n    echo \"Testing $provider...\"\n    time codeoptix eval --agent codex --behaviors insecure-code --llm-provider $provider\ndone\n</code></pre>"},{"location":"advanced/performance/#performance-by-component","title":"Performance by Component","text":""},{"location":"advanced/performance/#llm-providers-api-response-time","title":"LLM Providers (API Response Time)","text":"Provider Model Avg Response Time Cost Ollama llama3.2 2-5s Free Anthropic claude-opus-4-5 3-8s $$$ OpenAI gpt-5.2 2-6s $$$ Google gemini-3-pro 4-10s $$"},{"location":"advanced/performance/#behaviors-evaluation-time","title":"Behaviors (Evaluation Time)","text":"Behavior Complexity Avg Time insecure-code Medium 10-30s vacuous-tests Low 5-15s plan-drift High 20-60s"},{"location":"advanced/performance/#scenario-generation","title":"Scenario Generation","text":"<ul> <li>Without Bloom: 5-15s per scenario</li> <li>With Bloom: 30-90s per scenario</li> <li>Recommendation: Use Bloom only when needed for complex behaviors</li> </ul>"},{"location":"advanced/performance/#memory-optimization","title":"Memory Optimization","text":""},{"location":"advanced/performance/#large-codebases","title":"Large Codebases","text":"<p>For evaluating large codebases:</p> <pre><code>evaluation:\n  static_analysis:\n    # Enable memory-efficient linters\n    bandit: true\n    safety: true\n    ruff: true\n\n    # Disable memory-intensive linters\n    pylint: false\n    mypy: false\n\nlinters:\n  # Limit concurrent linters\n  max_concurrent: 2\n</code></pre>"},{"location":"advanced/performance/#resource-constraints","title":"Resource Constraints","text":"<p>For systems with limited resources:</p> <pre><code># Use smaller Ollama models\nollama pull llama3.2:1b  # 1B parameter model\n\n# Limit evaluation scope\ncodeoptix eval \\\n  --agent claude-code \\\n  --behaviors insecure-code \\\n  --config minimal-config.yaml\n</code></pre>"},{"location":"advanced/performance/#caching-and-reuse","title":"Caching and Reuse","text":""},{"location":"advanced/performance/#reuse-evaluation-results","title":"Reuse Evaluation Results","text":"<pre><code># Save results for reuse\ncodeoptix eval \\\n  --agent codex \\\n  --behaviors insecure-code \\\n  --output results.json\n\n# Generate reflection from saved results\ncodeoptix reflect --input results.json\n\n# Evolve from saved results\ncodeoptix evolve --input results.json\n</code></pre>"},{"location":"advanced/performance/#cache-llm-responses","title":"Cache LLM Responses","text":"<p>For repeated evaluations of similar code:</p> <pre><code># Implement caching in custom adapter\nfrom functools import lru_cache\n\nclass CachedLLMClient:\n    def __init__(self, client):\n        self.client = client\n\n    @lru_cache(maxsize=100)\n    def chat_completion(self, messages, model, **kwargs):\n        # Cache based on prompt content\n        prompt_hash = hash(str(messages))\n        return self.client.chat_completion(messages, model, **kwargs)\n</code></pre>"},{"location":"advanced/performance/#scaling-considerations","title":"Scaling Considerations","text":""},{"location":"advanced/performance/#multiple-agents","title":"Multiple Agents","text":"<p>Evaluate multiple agents in parallel:</p> <pre><code># Run evaluations in background\nfor agent in claude-code codex gemini-cli; do\n    codeoptix eval --agent $agent --behaviors insecure-code &amp;\ndone\nwait\n</code></pre>"},{"location":"advanced/performance/#batch-processing","title":"Batch Processing","text":"<p>Process multiple evaluation requests:</p> <pre><code>from codeoptix.evaluation import EvaluationEngine\nfrom concurrent.futures import ThreadPoolExecutor\n\ndef evaluate_batch(requests):\n    with ThreadPoolExecutor(max_workers=4) as executor:\n        futures = [\n            executor.submit(evaluate_single, req)\n            for req in requests\n        ]\n        return [f.result() for f in futures]\n</code></pre>"},{"location":"advanced/performance/#monitoring-and-alerting","title":"Monitoring and Alerting","text":""},{"location":"advanced/performance/#set-up-monitoring","title":"Set Up Monitoring","text":"<pre><code># Monitor evaluation performance\ncodeoptix eval --agent codex --behaviors insecure-code 2&gt;&amp;1 | tee eval.log\n\n# Check for errors\nif grep -q \"ERROR\\|FAILED\" eval.log; then\n    echo \"Evaluation failed\"\n    exit 1\nfi\n</code></pre>"},{"location":"advanced/performance/#performance-alerts","title":"Performance Alerts","text":"<pre><code># Alert if evaluation takes too long\ntimeout 300 codeoptix eval --agent codex --behaviors insecure-code\n\nif [ $? -eq 124 ]; then\n    echo \"Evaluation timed out after 5 minutes\"\n    # Send alert\nfi\n</code></pre>"},{"location":"advanced/performance/#best-practices","title":"Best Practices","text":""},{"location":"advanced/performance/#development-workflow","title":"Development Workflow","text":"<ol> <li>Local Development: Use Ollama for fast iteration</li> <li>Pre-commit: Use <code>codeoptix lint</code> for quick checks</li> <li>CI/CD: Use <code>codeoptix ci</code> for automated quality gates</li> <li>Production: Use cloud providers with optimized configs</li> </ol>"},{"location":"advanced/performance/#configuration-templates","title":"Configuration Templates","text":"<p>Fast Development: <pre><code>evaluation:\n  scenario_generator:\n    num_scenarios: 1\n    use_bloom: false\n\nllm:\n  provider: ollama\n  model: llama3.2\n</code></pre></p> <p>Production Quality: <pre><code>evaluation:\n  scenario_generator:\n    num_scenarios: 5\n    use_bloom: true\n\n  static_analysis:\n    bandit: true\n    safety: true\n    ruff: true\n\nllm:\n  provider: anthropic\n  model: claude-opus-4-5-20251101\n</code></pre></p> <p>CI/CD Optimized: <pre><code>evaluation:\n  scenario_generator:\n    num_scenarios: 2\n    use_bloom: false\n\noutput:\n  format: summary\n\nbehavior:\n  fail_on_failure: true\n</code></pre></p>"},{"location":"concepts/acp/","title":"ACP (Agent Client Protocol) Integration","text":"<p>CodeOptiX provides comprehensive ACP integration for editor support, multi-agent workflows, and code optimization.</p>"},{"location":"concepts/acp/#overview","title":"Overview","text":"<p>ACP (Agent Client Protocol) is a protocol for connecting AI agents to editors and other clients. CodeOptiX uses ACP to:</p> <ul> <li>Act as an ACP agent - Be used directly by editors (Zed, JetBrains, Neovim, VS Code)</li> <li>Connect to other agents - Use any ACP-compatible agent via the protocol</li> <li>Quality bridge - Sit between editor and agents, automatically evaluating code quality</li> <li>Multi-agent orchestration - Route to best agent for each task</li> <li>Multi-agent judge - Use different agents for generation vs. judgment</li> </ul>"},{"location":"concepts/acp/#architecture","title":"Architecture","text":""},{"location":"concepts/acp/#quality-bridge-mode","title":"Quality Bridge Mode","text":"<p>CodeOptiX acts as a quality bridge between your editor and coding agents:</p> <pre><code>Editor (Zed, JetBrains, Neovim, VS Code)\n    \u2502\n    \u251c\u2500\u2192 ACP Protocol\n    \u2502       \u2502\n    \u2502       \u2514\u2500\u2192 CodeOptiX ACP Bridge (Quality Layer)\n    \u2502               \u2502\n    \u2502               \u251c\u2500\u2192 Quality Engineering Layer\n    \u2502               \u2502   \u251c\u2500\u2192 Embedded Evaluations (automatic)\n    \u2502               \u2502   \u251c\u2500\u2192 Quality Assurance (automatic)\n    \u2502               \u2502   \u2514\u2500\u2192 Multi-LLM Critique\n    \u2502               \u2502\n    \u2502               \u251c\u2500\u2192 Agent Orchestration\n    \u2502               \u2502   \u251c\u2500\u2192 Route to Generate Agent\n    \u2502               \u2502   \u251c\u2500\u2192 Route to Judge Agent\n    \u2502               \u2502   \u2514\u2500\u2192 Multi-agent workflows\n    \u2502               \u2502\n    \u2502               \u2514\u2500\u2192 ACP Agent Registry\n    \u2502                       \u2502\n    \u2502                       \u251c\u2500\u2192 Claude Code (via ACP)\n    \u2502                       \u251c\u2500\u2192 Codex (via ACP)\n    \u2502                       \u251c\u2500\u2192 Gemini CLI (via ACP)\n    \u2502                       \u2514\u2500\u2192 Any ACP-compatible agent\n    \u2502\n    \u2514\u2500\u2192 Quality Feedback \u2192 Editor\n</code></pre>"},{"location":"concepts/acp/#components","title":"Components","text":""},{"location":"concepts/acp/#acp-agent-registry","title":"ACP Agent Registry","text":"<p>Purpose: Register and manage multiple ACP-compatible agents</p> <p>Features: - Register agents with commands and configurations - Connect/disconnect from agents - Session management - Agent discovery</p> <p>Usage: <pre><code>from codeoptix.acp import ACPAgentRegistry\n\nregistry = ACPAgentRegistry()\nregistry.register(\n    name=\"claude-code\",\n    command=[\"python\", \"claude_agent.py\"],\n    description=\"Claude Code via ACP\",\n)\n</code></pre></p>"},{"location":"concepts/acp/#quality-bridge","title":"Quality Bridge","text":"<p>Purpose: Automatic quality evaluation between editor and agents</p> <p>Features: - \u2705 Automatic code extraction from agent messages, tool calls, and responses - \u2705 Real-time quality evaluation using evaluation engine - \u2705 Quality feedback to editor via ACP session updates - \u2705 Configurable behavior evaluation - \u2705 Quick security checks for immediate feedback - \u2705 Formatted quality reports with scores and evidence</p> <p>Quality Evaluation Process: 1. Agent generates code 2. CodeOptiX extracts code from all sources 3. Evaluation engine evaluates against behavior specifications 4. Formatted report sent to editor with:    - Overall quality score    - Per-behavior pass/fail status    - Evidence of issues    - Recommendations</p> <p>Usage: <pre><code>from codeoptix.acp import ACPQualityBridge\nfrom codeoptix.evaluation import EvaluationEngine\nfrom codeoptix.utils.llm import LLMProvider, create_llm_client\n\n# Create evaluation engine\nllm_client = create_llm_client(LLMProvider.OPENAI)\nevaluation_engine = EvaluationEngine(adapter, llm_client)\n\nbridge = ACPQualityBridge(\n    agent_name=\"claude-code\",\n    evaluation_engine=evaluation_engine,\n    auto_eval=True,\n    behaviors=[\"insecure-code\", \"vacuous-tests\", \"plan-drift\"],\n)\n\n# Connect and use\nawait bridge.connect()\nresult = await bridge.prompt(\"Write secure authentication code\")\n</code></pre></p>"},{"location":"concepts/acp/#agent-orchestrator","title":"Agent Orchestrator","text":"<p>Purpose: Route to best agent for each task</p> <p>Features: - \u2705 Intelligent agent selection based on capabilities and task type - \u2705 Multi-agent workflows - \u2705 Task-based routing - \u2705 Context-aware selection - \u2705 Automatic fallback to available agents</p> <p>Intelligent Selection Logic: 1. Checks for <code>preferred_agent</code> in context 2. Infers task type from prompt keywords (security, review, testing, etc.) 3. Matches agents with relevant capabilities from registry 4. Falls back to first available agent if no match</p> <p>Usage: <pre><code>from codeoptix.acp import AgentOrchestrator\n\norchestrator = AgentOrchestrator(registry, evaluation_engine)\n\n# Automatic intelligent selection\nresult = await orchestrator.route_to_agent(\n    prompt=\"Write secure authentication tests\",\n    context={\"language\": \"python\"}\n)\n\n# With preferred agent\nresult = await orchestrator.route_to_agent(\n    prompt=\"Review this code\",\n    context={\"preferred_agent\": \"grok\"}\n)\n</code></pre></p>"},{"location":"concepts/acp/#multi-agent-judge","title":"Multi-Agent Judge","text":"<p>Purpose: Use different agents for generation vs. judgment</p> <p>Features: - \u2705 Separate generate and judge agents - \u2705 Combined evaluation with CodeOptiX - \u2705 Multi-perspective assessment - \u2705 Code extraction from both agents - \u2705 Comprehensive quality reports</p> <p>Process: 1. Generate code with generate agent 2. Extract code from generate agent response 3. Judge/critique code with judge agent 4. Extract judgment text from judge agent 5. Evaluate both with CodeOptiX evaluation engine 6. Return combined results</p> <p>Usage: <pre><code>from codeoptix.acp import MultiAgentJudge\n\njudge = MultiAgentJudge(\n    registry=registry,\n    generate_agent=\"claude-code\",\n    judge_agent=\"grok\",\n    evaluation_engine=evaluation_engine,\n)\n\nresult = await judge.generate_and_judge(\"Write secure code\")\n\n# Result contains:\n# {\n#     \"generated_code\": \"...\",\n#     \"judgment\": \"...\",\n#     \"evaluation_results\": {...},\n#     \"generate_agent\": \"claude-code\",\n#     \"judge_agent\": \"grok\"\n# }\n</code></pre></p>"},{"location":"concepts/acp/#code-extraction","title":"Code Extraction","text":"<p>Purpose: Extract code from ACP messages, tool calls, and responses</p> <p>Features: - \u2705 Extract from text blocks (markdown code fences) - \u2705 Extract from file edits (old_text, new_text) - \u2705 Extract from tool calls (file_edit, etc.) - \u2705 Extract from tool call progress updates - \u2705 Pattern matching for code blocks (fenced and inline) - \u2705 Language detection - \u2705 Multiple extraction methods</p> <p>Supported Sources: - <code>AgentMessageChunk</code> - Agent message content - <code>ToolCallStart</code> - Tool call initiation - <code>ToolCallProgress</code> - Tool call progress updates - <code>TextContentBlock</code> - Text content with code - Raw text strings - Direct text extraction</p> <p>Usage: <pre><code>from codeoptix.acp.code_extractor import (\n    extract_code_from_message,\n    extract_all_code,\n    extract_code_from_text\n)\n\n# Extract from single message\ncode_blocks = extract_code_from_message(acp_message)\n\n# Extract from multiple messages\ncode_blocks = extract_all_code(acp_messages)\n\n# Extract from text\ncode_blocks = extract_code_from_text(\"```python\\ndef hello():\\n    pass\\n```\")\n\n# Each code block contains:\n# {\n#     \"language\": \"python\",\n#     \"content\": \"def hello():\\n    pass\",\n#     \"type\": \"block\",  # or \"inline\", \"file_edit_new\", \"file_edit_old\"\n#     \"path\": \"file.py\"  # if from file edit\n# }\n</code></pre></p>"},{"location":"concepts/acp/#data-flow","title":"Data Flow","text":""},{"location":"concepts/acp/#quality-bridge-flow","title":"Quality Bridge Flow","text":"<ol> <li>Editor Request: Editor sends request via ACP</li> <li>Bridge Receives: CodeOptiX bridge receives request</li> <li>Agent Execution: Request routed to agent</li> <li>Code Extraction: Code extracted from agent response</li> <li>Quality Evaluation: CodeOptiX evaluates code quality</li> <li>Feedback: Quality report sent to editor</li> </ol>"},{"location":"concepts/acp/#multi-agent-judge-flow","title":"Multi-Agent Judge Flow","text":"<ol> <li>Generate: Code generated with generate agent</li> <li>Judge: Code judged/critiqued with judge agent</li> <li>Evaluate: CodeOptiX evaluates both perspectives</li> <li>Report: Combined quality report sent to editor</li> </ol>"},{"location":"concepts/acp/#use-cases","title":"Use Cases","text":""},{"location":"concepts/acp/#editor-integration","title":"Editor Integration","text":"<p>Connect CodeOptiX directly to your editor:</p> <pre><code>codeoptix acp register\n# Connect from editor (Zed, JetBrains, Neovim, VS Code)\n</code></pre>"},{"location":"concepts/acp/#quality-bridge_1","title":"Quality Bridge","text":"<p>Automatic quality checks for any agent:</p> <pre><code>codeoptix acp bridge --agent-name claude-code --auto-eval\n</code></pre>"},{"location":"concepts/acp/#multi-agent-workflows","title":"Multi-Agent Workflows","text":"<p>Orchestrate multiple agents:</p> <pre><code>workflow = [\n    {\"agent\": \"claude-code\", \"prompt\": \"Generate code\"},\n    {\"agent\": \"grok\", \"prompt\": \"Review code\"},\n]\nresults = await orchestrator.execute_multi_agent_workflow(workflow)\n</code></pre>"},{"location":"concepts/acp/#multi-perspective-evaluation","title":"Multi-Perspective Evaluation","text":"<p>Get multiple perspectives on code:</p> <pre><code>codeoptix acp judge \\\n  --generate-agent claude-code \\\n  --judge-agent grok \\\n  --prompt \"Write secure code\"\n</code></pre>"},{"location":"concepts/acp/#benefits","title":"Benefits","text":"<ol> <li>Real-time Feedback: Quality feedback directly in editor</li> <li>No Context Switching: Stay in your editor</li> <li>Automatic Evaluation: Quality checks happen automatically</li> <li>Multi-Agent Support: Use best agent for each task</li> <li>Editor Agnostic: Works with any ACP-compatible editor</li> </ol>"},{"location":"concepts/acp/#see-also","title":"See Also","text":"<ul> <li>ACP Integration Guide - Complete ACP documentation</li> <li>CLI Usage Guide - ACP CLI commands</li> <li>Python API Guide - ACP Python API</li> </ul>"},{"location":"concepts/adapters/","title":"Agent Adapters","text":"<p>Agent adapters connect CodeOptiX to your coding agent, allowing CodeOptiX to execute tasks and evaluate behavior.</p>"},{"location":"concepts/adapters/#what-are-adapters","title":"What are Adapters?","text":"<p>Adapters are interfaces that translate between CodeOptiX's standard format and your agent's specific API. They allow CodeOptiX to work with any coding agent.</p>"},{"location":"concepts/adapters/#supported-adapters","title":"Supported Adapters","text":""},{"location":"concepts/adapters/#codex-openai-gpt-4","title":"Codex (OpenAI GPT-4)","text":"<p>OpenAI's GPT-4 Code Interpreter.</p> <pre><code>adapter = create_adapter(\"codex\", {\n    \"llm_config\": {\n        \"provider\": \"openai\",\n        \"api_key\": os.getenv(\"OPENAI_API_KEY\"),\n        \"model\": \"gpt-5.2\"\n    }\n})\n</code></pre>"},{"location":"concepts/adapters/#claude-code-anthropic","title":"Claude Code (Anthropic)","text":"<p>Anthropic's Claude for coding.</p> <pre><code>adapter = create_adapter(\"claude-code\", {\n    \"llm_config\": {\n        \"provider\": \"anthropic\",\n        \"api_key\": os.getenv(\"ANTHROPIC_API_KEY\"),\n        \"model\": \"claude-sonnet-4.5\"\n    }\n})\n</code></pre>"},{"location":"concepts/adapters/#gemini-cli-google","title":"Gemini CLI (Google)","text":"<p>Google's Gemini for coding.</p> <pre><code>adapter = create_adapter(\"gemini-cli\", {\n    \"llm_config\": {\n        \"provider\": \"google\",\n        \"api_key\": os.getenv(\"GOOGLE_API_KEY\"),\n        \"model\": \"gemini-3-flash\"\n    }\n})\n</code></pre>"},{"location":"concepts/adapters/#adapter-interface","title":"Adapter Interface","text":"<p>All adapters implement the same interface:</p>"},{"location":"concepts/adapters/#executeprompt-contextnone-agentoutput","title":"<code>execute(prompt, context=None) -&gt; AgentOutput</code>","text":"<p>Execute a task and return standardized output.</p> <p>Parameters: - <code>prompt</code>: Task description - <code>context</code>: Optional context (files, workspace info)</p> <p>Returns: - <code>AgentOutput</code> with code, tests, and metadata</p>"},{"location":"concepts/adapters/#get_prompt-str","title":"<code>get_prompt() -&gt; str</code>","text":"<p>Get the current agent prompt.</p>"},{"location":"concepts/adapters/#update_promptnew_prompt-none","title":"<code>update_prompt(new_prompt) -&gt; None</code>","text":"<p>Update the agent's system prompt.</p>"},{"location":"concepts/adapters/#get_adapter_type-str","title":"<code>get_adapter_type() -&gt; str</code>","text":"<p>Get the adapter type identifier.</p>"},{"location":"concepts/adapters/#creating-an-adapter","title":"Creating an Adapter","text":""},{"location":"concepts/adapters/#using-the-factory","title":"Using the Factory","text":"<pre><code>from codeoptix.adapters.factory import create_adapter\n\nadapter = create_adapter(\"codex\", {\n    \"llm_config\": {\n        \"provider\": \"openai\",\n        \"api_key\": \"sk-...\",\n    },\n    \"prompt\": \"You are a helpful coding assistant.\"\n})\n</code></pre>"},{"location":"concepts/adapters/#configuration","title":"Configuration","text":"<p>Adapters accept a configuration dictionary:</p> <pre><code>config = {\n    \"llm_config\": {\n        \"provider\": \"openai\",  # or \"anthropic\", \"google\"\n        \"api_key\": \"sk-...\",\n        \"model\": \"gpt-5.2\"  # Optional, has defaults\n    },\n    \"prompt\": \"Custom system prompt\"  # Optional\n}\n</code></pre>"},{"location":"concepts/adapters/#using-adapters","title":"Using Adapters","text":""},{"location":"concepts/adapters/#execute-a-task","title":"Execute a Task","text":"<pre><code>output = adapter.execute(\n    \"Write a Python function to calculate fibonacci numbers\",\n    context={\"requirements\": [\"Use recursion\", \"Include docstring\"]}\n)\n\nprint(output.code)\nprint(output.tests)\n</code></pre>"},{"location":"concepts/adapters/#update-prompt","title":"Update Prompt","text":"<pre><code># Get current prompt\ncurrent = adapter.get_prompt()\n\n# Update prompt\nadapter.update_prompt(\"You are a security-focused coding assistant.\")\n</code></pre>"},{"location":"concepts/adapters/#agentoutput-structure","title":"AgentOutput Structure","text":"<p>The <code>AgentOutput</code> dataclass contains:</p> <pre><code>@dataclass\nclass AgentOutput:\n    code: str                    # Generated code\n    tests: Optional[str] = None  # Generated tests\n    traces: Optional[List] = None  # Execution traces\n    metadata: Optional[Dict] = None  # Additional metadata\n    prompt_used: Optional[str] = None  # Prompt that was used\n</code></pre>"},{"location":"concepts/adapters/#custom-adapters","title":"Custom Adapters","text":"<p>You can create custom adapters by implementing the <code>AgentAdapter</code> interface:</p> <pre><code>from codeoptix.adapters.base import AgentAdapter, AgentOutput\n\nclass MyCustomAdapter(AgentAdapter):\n    def execute(self, prompt: str, context=None):\n        # Your implementation\n        return AgentOutput(code=\"...\", tests=\"...\")\n\n    def get_prompt(self) -&gt; str:\n        return self._current_prompt\n\n    def update_prompt(self, new_prompt: str):\n        self._current_prompt = new_prompt\n\n    def get_adapter_type(self) -&gt; str:\n        return \"my-custom-adapter\"\n</code></pre>"},{"location":"concepts/adapters/#best-practices","title":"Best Practices","text":""},{"location":"concepts/adapters/#1-use-environment-variables","title":"1. Use Environment Variables","text":"<p>Always use environment variables for API keys:</p> <pre><code>import os\n\nadapter = create_adapter(\"codex\", {\n    \"llm_config\": {\n        \"provider\": \"openai\",\n        \"api_key\": os.getenv(\"OPENAI_API_KEY\"),\n    }\n})\n</code></pre>"},{"location":"concepts/adapters/#2-set-appropriate-prompts","title":"2. Set Appropriate Prompts","text":"<p>Set clear, specific prompts:</p> <pre><code>adapter.update_prompt(\n    \"You are a security-focused coding assistant. \"\n    \"Never include hardcoded secrets in code.\"\n)\n</code></pre>"},{"location":"concepts/adapters/#3-provide-context","title":"3. Provide Context","text":"<p>Use context for better results:</p> <pre><code>output = adapter.execute(\n    prompt,\n    context={\n        \"plan\": \"Create secure authentication\",\n        \"requirements\": [\"No hardcoded secrets\", \"Use environment variables\"]\n    }\n)\n</code></pre>"},{"location":"concepts/adapters/#troubleshooting","title":"Troubleshooting","text":""},{"location":"concepts/adapters/#adapter-not-found","title":"Adapter Not Found","text":"<p>Make sure you're using a supported adapter type: - <code>codex</code> - <code>claude-code</code> - <code>gemini-cli</code></p>"},{"location":"concepts/adapters/#api-key-errors","title":"API Key Errors","text":"<p>Verify your API key is set correctly:</p> <pre><code>import os\nprint(os.getenv(\"OPENAI_API_KEY\"))  # Should not be None\n</code></pre>"},{"location":"concepts/adapters/#execution-errors","title":"Execution Errors","text":"<p>Check that your agent is properly configured and the API is accessible.</p>"},{"location":"concepts/adapters/#next-steps","title":"Next Steps","text":"<ul> <li>Behavior Specifications - Define behaviors to evaluate</li> <li>Evaluation Engine - Run evaluations</li> <li>Python API Guide - Advanced usage</li> </ul>"},{"location":"concepts/behaviors/","title":"Behavior Specifications","text":"<p>Behavior specifications define what CodeOptiX evaluates. They are modular, reusable definitions of desired or undesired behaviors.</p>"},{"location":"concepts/behaviors/#what-are-behavior-specifications","title":"What are Behavior Specifications?","text":"<p>Behavior specifications are rules that define how to evaluate agent output. They check if the agent's code exhibits specific behaviors.</p>"},{"location":"concepts/behaviors/#built-in-behaviors","title":"Built-in Behaviors","text":"<p>CodeOptiX includes three built-in behaviors:</p>"},{"location":"concepts/behaviors/#1-insecure-code","title":"1. insecure-code","text":"<p>Detects security vulnerabilities in generated code.</p> <p>What it checks: - Hardcoded secrets (passwords, API keys) - SQL injection vulnerabilities - Insecure authentication patterns - Exposed credentials</p> <p>Example: <pre><code>from codeoptix.behaviors import create_behavior\n\nbehavior = create_behavior(\"insecure-code\")\nresult = behavior.evaluate(agent_output)\n\nif not result.passed:\n    print(f\"Issues found: {result.evidence}\")\n</code></pre></p>"},{"location":"concepts/behaviors/#2-vacuous-tests","title":"2. vacuous-tests","text":"<p>Identifies low-quality or meaningless tests.</p> <p>What it checks: - Tests with no assertions - Trivial tests (always pass) - Missing edge cases - Incomplete test coverage</p> <p>Example: <pre><code>behavior = create_behavior(\"vacuous-tests\")\nresult = behavior.evaluate(agent_output)\n\nprint(f\"Test quality score: {result.score}\")\n</code></pre></p>"},{"location":"concepts/behaviors/#3-plan-drift","title":"3. plan-drift","text":"<p>Detects deviations from planning artifacts and requirements.</p> <p>What it checks: - Missing planned features - Requirements not addressed - API contract violations - Architecture mismatches</p> <p>Example: <pre><code>behavior = create_behavior(\"plan-drift\")\nresult = behavior.evaluate(\n    agent_output,\n    context={\n        \"plan\": \"Create secure authentication API\",\n        \"requirements\": [\"JWT tokens\", \"Password hashing\"]\n    }\n)\n</code></pre></p>"},{"location":"concepts/behaviors/#behavior-result-structure","title":"Behavior Result Structure","text":"<p>Each behavior evaluation returns a <code>BehaviorResult</code>:</p> <pre><code>@dataclass\nclass BehaviorResult:\n    behavior_name: str      # Name of the behavior\n    passed: bool            # Whether it passed\n    score: float            # Score from 0.0 to 1.0\n    evidence: List[str]     # Specific issues found\n    severity: Severity      # LOW, MEDIUM, HIGH, CRITICAL\n    metadata: Dict          # Additional data\n</code></pre>"},{"location":"concepts/behaviors/#score-interpretation","title":"Score Interpretation","text":"<ul> <li>0.9 - 1.0: Excellent - No issues</li> <li>0.7 - 0.9: Good - Minor issues</li> <li>0.5 - 0.7: Fair - Some issues</li> <li>0.0 - 0.5: Poor - Significant issues</li> </ul>"},{"location":"concepts/behaviors/#using-behaviors","title":"Using Behaviors","text":""},{"location":"concepts/behaviors/#basic-usage","title":"Basic Usage","text":"<pre><code>from codeoptix.behaviors import create_behavior\nfrom codeoptix.adapters.base import AgentOutput\n\n# Create behavior\nbehavior = create_behavior(\"insecure-code\")\n\n# Create agent output (example)\nagent_output = AgentOutput(\n    code='def connect():\\n    password = \"secret123\"\\n    return password',\n    tests=\"def test_connect():\\n    assert True\"\n)\n\n# Evaluate\nresult = behavior.evaluate(agent_output)\n\n# Check results\nprint(f\"Passed: {result.passed}\")\nprint(f\"Score: {result.score}\")\nprint(f\"Evidence: {result.evidence}\")\n</code></pre>"},{"location":"concepts/behaviors/#with-configuration","title":"With Configuration","text":"<pre><code>behavior = create_behavior(\"insecure-code\", {\n    \"severity\": \"high\",\n    \"enabled\": True,\n    \"strict_mode\": True\n})\n</code></pre>"},{"location":"concepts/behaviors/#with-context","title":"With Context","text":"<pre><code>result = behavior.evaluate(\n    agent_output,\n    context={\n        \"plan\": \"Create secure API\",\n        \"requirements\": [\"No hardcoded secrets\", \"Use environment variables\"]\n    }\n)\n</code></pre>"},{"location":"concepts/behaviors/#creating-custom-behaviors","title":"Creating Custom Behaviors","text":"<p>You can create custom behaviors by extending <code>BehaviorSpec</code>:</p> <pre><code>from codeoptix.behaviors.base import BehaviorSpec, BehaviorResult, Severity\n\nclass MyCustomBehavior(BehaviorSpec):\n    def get_name(self) -&gt; str:\n        return \"my-custom-behavior\"\n\n    def get_description(self) -&gt; str:\n        return \"Checks for specific patterns in code\"\n\n    def evaluate(self, agent_output, context=None):\n        code = agent_output.code or \"\"\n        evidence = []\n        score = 1.0\n\n        # Your evaluation logic\n        if \"bad_pattern\" in code:\n            evidence.append(\"Found bad pattern\")\n            score = 0.5\n\n        return BehaviorResult(\n            behavior_name=self.get_name(),\n            passed=score &gt;= 0.7,\n            score=score,\n            evidence=evidence,\n            severity=Severity.MEDIUM\n        )\n</code></pre>"},{"location":"concepts/behaviors/#registering-custom-behaviors","title":"Registering Custom Behaviors","text":"<pre><code>from codeoptix.behaviors import create_behavior\n\n# Register your behavior\n# (Implementation depends on your setup)\n\nbehavior = create_behavior(\"my-custom-behavior\")\n</code></pre>"},{"location":"concepts/behaviors/#behavior-configuration","title":"Behavior Configuration","text":"<p>Behaviors can be configured:</p> <pre><code>config = {\n    \"severity\": \"high\",      # LOW, MEDIUM, HIGH, CRITICAL\n    \"enabled\": True,         # Enable/disable behavior\n    \"threshold\": 0.7,        # Passing threshold\n    # Behavior-specific options\n}\n</code></pre>"},{"location":"concepts/behaviors/#evaluation-process","title":"Evaluation Process","text":"<p>When a behavior evaluates agent output:</p> <ol> <li>Extract Code: Gets code from <code>AgentOutput</code></li> <li>Run Checks: Performs behavior-specific checks</li> <li>Collect Evidence: Gathers specific issues</li> <li>Calculate Score: Computes score based on findings</li> <li>Return Result: Returns <code>BehaviorResult</code></li> </ol>"},{"location":"concepts/behaviors/#best-practices","title":"Best Practices","text":""},{"location":"concepts/behaviors/#1-use-appropriate-behaviors","title":"1. Use Appropriate Behaviors","text":"<p>Choose behaviors relevant to your use case:</p> <pre><code># For security-focused projects\nbehaviors = [\"insecure-code\"]\n\n# For test quality\nbehaviors = [\"vacuous-tests\"]\n\n# For plan compliance\nbehaviors = [\"plan-drift\"]\n</code></pre>"},{"location":"concepts/behaviors/#2-provide-context","title":"2. Provide Context","text":"<p>Always provide context when available:</p> <pre><code>result = behavior.evaluate(\n    agent_output,\n    context={\n        \"plan\": plan_content,\n        \"requirements\": requirements_list\n    }\n)\n</code></pre>"},{"location":"concepts/behaviors/#3-review-evidence","title":"3. Review Evidence","text":"<p>Always review the evidence in results:</p> <pre><code>for issue in result.evidence:\n    print(f\"  - {issue}\")\n</code></pre>"},{"location":"concepts/behaviors/#combining-behaviors","title":"Combining Behaviors","text":"<p>You can evaluate multiple behaviors:</p> <pre><code>behaviors = [\n    create_behavior(\"insecure-code\"),\n    create_behavior(\"vacuous-tests\"),\n    create_behavior(\"plan-drift\")\n]\n\nresults = {}\nfor behavior in behaviors:\n    results[behavior.get_name()] = behavior.evaluate(agent_output)\n</code></pre>"},{"location":"concepts/behaviors/#next-steps","title":"Next Steps","text":"<ul> <li>Evaluation Engine - Run evaluations with behaviors</li> <li>Custom Behaviors Guide - Create your own</li> <li>Python API Guide - Advanced usage</li> </ul>"},{"location":"concepts/evaluation/","title":"Evaluation Engine","text":"<p>The Evaluation Engine orchestrates the entire evaluation process, from scenario generation to result aggregation.</p>"},{"location":"concepts/evaluation/#what-is-the-evaluation-engine","title":"What is the Evaluation Engine?","text":"<p>The Evaluation Engine is the core component that: - Generates test scenarios - Executes your agent - Runs behavior evaluations - Aggregates results</p>"},{"location":"concepts/evaluation/#how-it-works","title":"How It Works","text":""},{"location":"concepts/evaluation/#1-scenario-generation","title":"1. Scenario Generation","text":"<p>The engine generates test scenarios for each behavior:</p> <pre><code>scenarios = generator.generate_scenarios(\n    behavior_name=\"insecure-code\",\n    behavior_description=\"Detect insecure code\"\n)\n</code></pre>"},{"location":"concepts/evaluation/#2-agent-execution","title":"2. Agent Execution","text":"<p>Your agent runs on each scenario:</p> <pre><code>for scenario in scenarios:\n    output = adapter.execute(scenario[\"prompt\"])\n</code></pre>"},{"location":"concepts/evaluation/#3-behavior-evaluation","title":"3. Behavior Evaluation","text":"<p>Each behavior evaluates the agent output:</p> <pre><code>result = behavior.evaluate(output, context=scenario.get(\"context\"))\n</code></pre>"},{"location":"concepts/evaluation/#4-result-aggregation","title":"4. Result Aggregation","text":"<p>Results are aggregated into a final report:</p> <pre><code>overall_score = sum(scores) / len(scores)\n</code></pre>"},{"location":"concepts/evaluation/#using-the-evaluation-engine","title":"Using the Evaluation Engine","text":""},{"location":"concepts/evaluation/#basic-usage","title":"Basic Usage","text":"<pre><code>from codeoptix.evaluation import EvaluationEngine\nfrom codeoptix.adapters.factory import create_adapter\nfrom codeoptix.utils.llm import create_llm_client, LLMProvider\n\n# Create adapter\nadapter = create_adapter(\"codex\", {\n    \"llm_config\": {\n        \"provider\": \"openai\",\n        \"api_key\": os.getenv(\"OPENAI_API_KEY\"),\n    }\n})\n\n# Create LLM client\nllm_client = create_llm_client(LLMProvider.OPENAI)\n\n# Create evaluation engine\nengine = EvaluationEngine(adapter, llm_client)\n\n# Evaluate behaviors\nresults = engine.evaluate_behaviors(\n    behavior_names=[\"insecure-code\", \"vacuous-tests\"]\n)\n</code></pre>"},{"location":"concepts/evaluation/#with-configuration","title":"With Configuration","text":"<pre><code>config = {\n    \"scenario_generator\": {\n        \"num_scenarios\": 5,\n        \"use_bloom\": True,\n        \"use_full_bloom\": True\n    },\n    \"static_analysis\": {\n        \"bandit\": True\n    },\n    \"test_runner\": {\n        \"coverage\": True\n    }\n}\n\nengine = EvaluationEngine(adapter, llm_client, config=config)\n</code></pre>"},{"location":"concepts/evaluation/#with-context","title":"With Context","text":"<pre><code>context = {\n    \"plan\": \"Create secure authentication API\",\n    \"requirements\": [\n        \"Use JWT tokens\",\n        \"Hash passwords\",\n        \"No hardcoded secrets\"\n    ]\n}\n\nresults = engine.evaluate_behaviors(\n    behavior_names=[\"insecure-code\", \"plan-drift\"],\n    context=context\n)\n</code></pre>"},{"location":"concepts/evaluation/#results-structure","title":"Results Structure","text":"<p>The evaluation engine returns a comprehensive results dictionary:</p> <pre><code>{\n    \"run_id\": \"abc123\",\n    \"timestamp\": \"2025-01-20T10:00:00Z\",\n    \"agent\": \"codex\",\n    \"overall_score\": 0.75,\n    \"behaviors\": {\n        \"insecure-code\": {\n            \"behavior_name\": \"insecure-code\",\n            \"scenarios_tested\": 3,\n            \"scenarios_passed\": 2,\n            \"score\": 0.75,\n            \"passed\": True,\n            \"evidence\": [\"Hardcoded password found\"],\n            \"scenario_results\": [...]\n        }\n    },\n    \"scenarios\": [...],\n    \"metadata\": {...}\n}\n</code></pre>"},{"location":"concepts/evaluation/#scenario-generation","title":"Scenario Generation","text":""},{"location":"concepts/evaluation/#bloom-integration","title":"Bloom Integration","text":"<p>CodeOptiX uses Bloom for sophisticated scenario generation:</p> <pre><code>config = {\n    \"scenario_generator\": {\n        \"use_bloom\": True,\n        \"use_full_bloom\": True,  # Full Bloom integration\n        \"num_scenarios\": 5,\n        \"num_variations\": 2\n    }\n}\n</code></pre>"},{"location":"concepts/evaluation/#custom-scenarios","title":"Custom Scenarios","text":"<p>You can provide pre-generated scenarios:</p> <pre><code>scenarios = [\n    {\n        \"prompt\": \"Write a function to connect to a database\",\n        \"task\": \"Database connection\",\n        \"behavior\": \"insecure-code\"\n    }\n]\n\nresults = engine.evaluate_behaviors(\n    behavior_names=[\"insecure-code\"],\n    scenarios=scenarios\n)\n</code></pre>"},{"location":"concepts/evaluation/#multi-modal-evaluation","title":"Multi-Modal Evaluation","text":"<p>The evaluation engine uses multiple evaluation signals:</p>"},{"location":"concepts/evaluation/#1-behavior-specs","title":"1. Behavior Specs","text":"<p>Primary evaluation using behavior specifications.</p>"},{"location":"concepts/evaluation/#2-static-analysis","title":"2. Static Analysis","text":"<p>Uses tools like Bandit for security analysis.</p>"},{"location":"concepts/evaluation/#3-test-execution","title":"3. Test Execution","text":"<p>Runs tests and checks coverage.</p>"},{"location":"concepts/evaluation/#4-llm-evaluation","title":"4. LLM Evaluation","text":"<p>Semantic analysis using LLMs.</p>"},{"location":"concepts/evaluation/#5-artifact-comparison","title":"5. Artifact Comparison","text":"<p>Compares code against planning artifacts.</p>"},{"location":"concepts/evaluation/#configuration-options","title":"Configuration Options","text":""},{"location":"concepts/evaluation/#scenario-generator","title":"Scenario Generator","text":"<pre><code>\"scenario_generator\": {\n    \"num_scenarios\": 3,        # Number of scenarios per behavior\n    \"use_bloom\": True,         # Use Bloom-style generation\n    \"use_full_bloom\": True,    # Full Bloom integration\n    \"num_variations\": 2,       # Variations per scenario\n    \"model\": \"gpt-5.2\"         # LLM model for generation\n}\n</code></pre>"},{"location":"concepts/evaluation/#static-analysis","title":"Static Analysis","text":"<pre><code>\"static_analysis\": {\n    \"bandit\": True  # Enable Bandit security checks\n}\n</code></pre>"},{"location":"concepts/evaluation/#test-runner","title":"Test Runner","text":"<pre><code>\"test_runner\": {\n    \"coverage\": True  # Enable coverage analysis\n}\n</code></pre>"},{"location":"concepts/evaluation/#llm-evaluator","title":"LLM Evaluator","text":"<pre><code>\"llm_evaluator\": {\n    \"model\": \"gpt-5.2\",\n    \"temperature\": 0.3\n}\n</code></pre>"},{"location":"concepts/evaluation/#best-practices","title":"Best Practices","text":""},{"location":"concepts/evaluation/#1-start-with-few-scenarios","title":"1. Start with Few Scenarios","text":"<p>Begin with a small number of scenarios:</p> <pre><code>config = {\n    \"scenario_generator\": {\n        \"num_scenarios\": 2  # Start small\n    }\n}\n</code></pre>"},{"location":"concepts/evaluation/#2-use-relevant-behaviors","title":"2. Use Relevant Behaviors","text":"<p>Only evaluate behaviors relevant to your use case:</p> <pre><code>behaviors = [\"insecure-code\"]  # Focus on what matters\n</code></pre>"},{"location":"concepts/evaluation/#3-provide-context","title":"3. Provide Context","text":"<p>Always provide context when available:</p> <pre><code>results = engine.evaluate_behaviors(\n    behavior_names=[\"plan-drift\"],\n    context={\"plan\": plan_content}\n)\n</code></pre>"},{"location":"concepts/evaluation/#4-review-results","title":"4. Review Results","text":"<p>Always review the detailed results:</p> <pre><code>for behavior_name, behavior_data in results[\"behaviors\"].items():\n    print(f\"{behavior_name}: {behavior_data['score']:.2f}\")\n    for evidence in behavior_data[\"evidence\"]:\n        print(f\"  - {evidence}\")\n</code></pre>"},{"location":"concepts/evaluation/#error-handling","title":"Error Handling","text":"<p>The evaluation engine handles errors gracefully:</p> <ul> <li>API failures: Retries with exponential backoff</li> <li>Parsing errors: Falls back to default scenarios</li> <li>Agent errors: Continues with other scenarios</li> </ul>"},{"location":"concepts/evaluation/#performance-tips","title":"Performance Tips","text":""},{"location":"concepts/evaluation/#1-parallel-execution","title":"1. Parallel Execution","text":"<p>The engine can evaluate multiple scenarios in parallel (when supported).</p>"},{"location":"concepts/evaluation/#2-caching","title":"2. Caching","text":"<p>Results are cached to avoid redundant evaluations.</p>"},{"location":"concepts/evaluation/#3-minibatch-evaluation","title":"3. Minibatch Evaluation","text":"<p>Use minibatches for faster iteration:</p> <pre><code>config = {\n    \"scenario_generator\": {\n        \"num_scenarios\": 2  # Smaller batches\n    }\n}\n</code></pre>"},{"location":"concepts/evaluation/#next-steps","title":"Next Steps","text":"<ul> <li>Reflection Engine - Understand results</li> <li>Evolution Engine - Improve agents</li> <li>Python API Guide - Advanced usage</li> </ul>"},{"location":"concepts/evolution/","title":"Evolution Engine","text":"<p>The Evolution Engine automatically improves agent prompts using GEPA-style optimization based on evaluation results.</p>"},{"location":"concepts/evolution/#what-is-evolution","title":"What is Evolution?","text":"<p>Evolution is the process of automatically improving agent prompts based on evaluation results. It uses GEPA (Genetic-Pareto) to generate better prompts through reflective mutation.</p>"},{"location":"concepts/evolution/#how-evolution-works","title":"How Evolution Works","text":""},{"location":"concepts/evolution/#1-analyze-failures","title":"1. Analyze Failures","text":"<p>The engine analyzes evaluation failures:</p> <pre><code>failures = extract_failures(evaluation_results)\n</code></pre>"},{"location":"concepts/evolution/#2-generate-proposals","title":"2. Generate Proposals","text":"<p>It generates improved prompt proposals using GEPA:</p> <pre><code>proposed_prompt = gepa_proposer.propose_improved_prompt(\n    current_prompt=current_prompt,\n    reflective_dataset=failures\n)\n</code></pre>"},{"location":"concepts/evolution/#3-test-candidates","title":"3. Test Candidates","text":"<p>It tests candidate prompts:</p> <pre><code>for candidate in candidates:\n    score = evaluate_candidate(candidate)\n</code></pre>"},{"location":"concepts/evolution/#4-select-best","title":"4. Select Best","text":"<p>It selects the best-performing prompt:</p> <pre><code>best_prompt = select_best(candidates, scores)\n</code></pre>"},{"location":"concepts/evolution/#using-the-evolution-engine","title":"Using the Evolution Engine","text":""},{"location":"concepts/evolution/#basic-usage","title":"Basic Usage","text":"<pre><code>from codeoptix.evolution import EvolutionEngine\nfrom codeoptix.evaluation import EvaluationEngine\nfrom codeoptix.reflection import ReflectionEngine\nfrom codeoptix.artifacts import ArtifactManager\n\n# Create engines\neval_engine = EvaluationEngine(adapter, llm_client)\nreflection_engine = ReflectionEngine(artifact_manager)\nartifact_manager = ArtifactManager()\n\n# Create evolution engine\nevolution_engine = EvolutionEngine(\n    adapter=adapter,\n    evaluation_engine=eval_engine,\n    llm_client=llm_client,\n    artifact_manager=artifact_manager\n)\n\n# Evolve prompts\nevolved = evolution_engine.evolve(\n    evaluation_results=results,\n    reflection=reflection_content,\n    behavior_names=[\"insecure-code\"]\n)\n</code></pre>"},{"location":"concepts/evolution/#with-configuration","title":"With Configuration","text":"<pre><code>config = {\n    \"max_iterations\": 3,      # Number of evolution iterations\n    \"population_size\": 3,     # Number of candidates per iteration\n    \"minibatch_size\": 2,      # Scenarios per evaluation\n    \"improvement_threshold\": 0.05,  # Minimum improvement to accept\n    \"proposer\": {\n        \"use_gepa\": True,     # Use GEPA for proposal\n        \"model\": \"gpt-5.2\"\n    }\n}\n\nevolution_engine = EvolutionEngine(\n    adapter, eval_engine, llm_client, artifact_manager, config=config\n)\n</code></pre>"},{"location":"concepts/evolution/#evolution-process","title":"Evolution Process","text":""},{"location":"concepts/evolution/#iteration-1","title":"Iteration 1","text":"<ol> <li>Generate 3 candidate prompts</li> <li>Evaluate each on minibatch</li> <li>Select best candidate</li> <li>If improved, accept; otherwise stop</li> </ol>"},{"location":"concepts/evolution/#iteration-2","title":"Iteration 2","text":"<ol> <li>Generate new candidates from best</li> <li>Evaluate again</li> <li>Continue until no improvement</li> </ol>"},{"location":"concepts/evolution/#final-result","title":"Final Result","text":"<p>Best prompt is saved and applied to adapter.</p>"},{"location":"concepts/evolution/#evolved-prompts-structure","title":"Evolved Prompts Structure","text":"<pre><code>{\n    \"agent\": \"codex\",\n    \"prompts\": {\n        \"system_prompt\": \"Improved prompt text...\"\n    },\n    \"metadata\": {\n        \"iterations\": 2,\n        \"initial_score\": 0.65,\n        \"final_score\": 0.85,\n        \"improvement\": 0.20,\n        \"evolution_history\": [...]\n    }\n}\n</code></pre>"},{"location":"concepts/evolution/#gepa-integration","title":"GEPA Integration","text":"<p>The Evolution Engine uses GEPA for prompt proposal:</p> <pre><code>config = {\n    \"proposer\": {\n        \"use_gepa\": True,  # Enable GEPA\n        \"model\": \"gpt-5.2\"\n    }\n}\n</code></pre> <p>GEPA uses: - Reflective Dataset: Failure examples - Instruction Proposal: LLM-based prompt generation - Iterative Improvement: Multiple refinement rounds</p>"},{"location":"concepts/evolution/#configuration-options","title":"Configuration Options","text":""},{"location":"concepts/evolution/#evolution-parameters","title":"Evolution Parameters","text":"<pre><code>{\n    \"max_iterations\": 3,           # Maximum iterations\n    \"population_size\": 3,           # Candidates per iteration\n    \"minibatch_size\": 2,            # Scenarios per evaluation\n    \"improvement_threshold\": 0.05   # Minimum improvement\n}\n</code></pre>"},{"location":"concepts/evolution/#proposer-configuration","title":"Proposer Configuration","text":"<pre><code>{\n    \"proposer\": {\n        \"use_gepa\": True,           # Use GEPA\n        \"model\": \"gpt-5.2\",          # LLM model\n        \"temperature\": 0.7         # Generation temperature\n    }\n}\n</code></pre>"},{"location":"concepts/evolution/#best-practices","title":"Best Practices","text":""},{"location":"concepts/evolution/#1-start-with-evaluation","title":"1. Start with Evaluation","text":"<p>Always evaluate before evolving:</p> <pre><code>results = eval_engine.evaluate_behaviors(...)\n</code></pre>"},{"location":"concepts/evolution/#2-generate-reflection","title":"2. Generate Reflection","text":"<p>Generate reflection for better evolution:</p> <pre><code>reflection = reflection_engine.reflect(results)\n</code></pre>"},{"location":"concepts/evolution/#3-focus-on-specific-behaviors","title":"3. Focus on Specific Behaviors","text":"<p>Evolve for specific behaviors:</p> <pre><code>evolved = evolution_engine.evolve(\n    results,\n    reflection,\n    behavior_names=[\"insecure-code\"]  # Focus\n)\n</code></pre>"},{"location":"concepts/evolution/#4-review-evolved-prompts","title":"4. Review Evolved Prompts","text":"<p>Always review evolved prompts:</p> <pre><code>print(evolved[\"prompts\"][\"system_prompt\"])\n</code></pre>"},{"location":"concepts/evolution/#evolution-history","title":"Evolution History","text":"<p>The evolution history tracks progress:</p> <pre><code>history = evolved[\"metadata\"][\"evolution_history\"]\n\nfor iteration in history:\n    print(f\"Iteration {iteration['iteration']}:\")\n    print(f\"  Score: {iteration['best_score']:.2f}\")\n    print(f\"  Improvement: {iteration['improvement']:.2f}\")\n</code></pre>"},{"location":"concepts/evolution/#limitations","title":"Limitations","text":""},{"location":"concepts/evolution/#1-iteration-limit","title":"1. Iteration Limit","text":"<p>Evolution stops after <code>max_iterations</code> or when no improvement is found.</p>"},{"location":"concepts/evolution/#2-minibatch-evaluation","title":"2. Minibatch Evaluation","text":"<p>Uses minibatch for speed, may not reflect full performance.</p>"},{"location":"concepts/evolution/#3-llm-dependency","title":"3. LLM Dependency","text":"<p>Requires LLM access for prompt generation.</p>"},{"location":"concepts/evolution/#next-steps","title":"Next Steps","text":"<ul> <li>GEPA Integration - Learn about GEPA</li> <li>Python API Guide - Advanced usage</li> <li>CLI Usage - Command-line usage</li> </ul>"},{"location":"concepts/overview/","title":"Core Concepts Overview","text":"<p>Understanding the fundamental concepts behind CodeOptiX.</p> <p>Agentic Code Optimization &amp; Deep Evaluation for Superior Coding Agent Experience. CodeOptiX is the universal code optimization engine that improves coding agent experience with deep evaluations and optimization. When AI coding agents dazzle with impressive code but leave you wondering about quality, maintainability, security, and reliability, CodeOptiX ensures proper behavior through evaluations, reflection, and self-improvement.</p>"},{"location":"concepts/overview/#what-is-codeoptix","title":"What is CodeOptiX?","text":"<p>CodeOptiX is an advanced evaluation and optimization platform for AI coding agents. It provides comprehensive testing, analysis, and improvement capabilities to ensure your coding agents produce high-quality, reliable code.</p>"},{"location":"concepts/overview/#key-capabilities","title":"Key Capabilities","text":"<ul> <li>\ud83d\udd0d Deep Evaluation - Comprehensive behavioral testing of coding agents</li> <li>\ud83d\udcca Detailed Analysis - In-depth performance metrics and issue identification</li> <li>\ud83e\udde0 Smart Optimization - GEPA-powered prompt evolution and improvement</li> <li>\ud83c\udfaf Quality Assurance - Automated testing against security, reliability, and correctness behaviors</li> </ul>"},{"location":"concepts/overview/#the-workflow","title":"The Workflow","text":"<p>CodeOptiX follows a simple workflow:</p> <pre><code>graph LR\n    A[Agent] --&gt; B[Evaluation]\n    B --&gt; C[Reflection]\n    C --&gt; D[Evolution]\n    D --&gt; A</code></pre>"},{"location":"concepts/overview/#1-evaluation","title":"1. Evaluation","text":"<p>Test your agent against behavior specifications:</p> <pre><code>results = engine.evaluate_behaviors(\n    behavior_names=[\"insecure-code\", \"vacuous-tests\"]\n)\n</code></pre>"},{"location":"concepts/overview/#2-reflection","title":"2. Reflection","text":"<p>Understand why the agent behaved the way it did:</p> <pre><code>reflection = reflection_engine.reflect(results)\n</code></pre>"},{"location":"concepts/overview/#3-evolution","title":"3. Evolution","text":"<p>Automatically improve agent prompts:</p> <pre><code>evolved = evolution_engine.evolve(results, reflection)\n</code></pre>"},{"location":"concepts/overview/#key-components","title":"Key Components","text":""},{"location":"concepts/overview/#agent-adapters","title":"Agent Adapters","text":"<p>What: Connect CodeOptiX to your coding agent</p> <p>Why: CodeOptiX works with any agent through adapters</p> <p>Example: <pre><code>adapter = create_adapter(\"codex\", config)\n</code></pre></p>"},{"location":"concepts/overview/#behavior-specifications","title":"Behavior Specifications","text":"<p>What: Define what behaviors to evaluate</p> <p>Why: Modular, reusable behavior definitions</p> <p>Example: <pre><code>behavior = create_behavior(\"insecure-code\")\nresult = behavior.evaluate(agent_output)\n</code></pre></p>"},{"location":"concepts/overview/#evaluation-engine","title":"Evaluation Engine","text":"<p>What: Orchestrates the evaluation process</p> <p>Why: Handles scenario generation, execution, and scoring</p> <p>Example: <pre><code>engine = EvaluationEngine(adapter, llm_client)\nresults = engine.evaluate_behaviors([\"insecure-code\"])\n</code></pre></p>"},{"location":"concepts/overview/#reflection-engine","title":"Reflection Engine","text":"<p>What: Analyzes evaluation results</p> <p>Why: Provides insights and recommendations</p> <p>Example: <pre><code>reflection = reflection_engine.reflect(results)\n</code></pre></p>"},{"location":"concepts/overview/#evolution-engine","title":"Evolution Engine","text":"<p>What: Optimizes agent prompts</p> <p>Why: Automatically improves agent behavior</p> <p>Example: <pre><code>evolved = evolution_engine.evolve(results, reflection)\n</code></pre></p>"},{"location":"concepts/overview/#acp-integration","title":"ACP Integration","text":"<p>What: Agent Client Protocol integration for editor support</p> <p>Why: Connect CodeOptiX to editors and orchestrate multiple agents</p> <p>Example: <pre><code>from codeoptix.acp import ACPQualityBridge\n\nbridge = ACPQualityBridge(agent_command=[\"python\", \"agent.py\"], auto_eval=True)\nawait bridge.connect()\n</code></pre></p>"},{"location":"concepts/overview/#how-it-works","title":"How It Works","text":""},{"location":"concepts/overview/#step-1-scenario-generation","title":"Step 1: Scenario Generation","text":"<p>CodeOptiX generates test scenarios:</p> <pre><code>scenarios = generator.generate_scenarios(\n    behavior_name=\"insecure-code\",\n    behavior_description=\"Detect insecure code\"\n)\n</code></pre>"},{"location":"concepts/overview/#step-2-agent-execution","title":"Step 2: Agent Execution","text":"<p>Your agent runs on each scenario:</p> <pre><code>agent_output = adapter.execute(scenario[\"prompt\"])\n</code></pre>"},{"location":"concepts/overview/#step-3-evaluation","title":"Step 3: Evaluation","text":"<p>CodeOptiX evaluates the output:</p> <pre><code>result = behavior.evaluate(agent_output)\n</code></pre>"},{"location":"concepts/overview/#step-4-aggregation","title":"Step 4: Aggregation","text":"<p>Results are aggregated:</p> <pre><code>overall_score = sum(scores) / len(scores)\n</code></pre>"},{"location":"concepts/overview/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502           CodeOptiX Core                 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502\n\u2502  \u2502  Agent   \u2502  \u2502 Behavior \u2502  \u2502 Eval   \u2502\u2502\n\u2502  \u2502 Adapters \u2502  \u2502   Specs  \u2502  \u2502 Engine \u2502\u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502\n\u2502  \u2502Reflection\u2502  \u2502Evolution \u2502  \u2502Artifact\u2502\u2502\n\u2502  \u2502 Engine   \u2502  \u2502  Engine  \u2502  \u2502Manager \u2502\u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"concepts/overview/#next-steps","title":"Next Steps","text":"<p>Learn more about each component:</p> <ul> <li>Agent Adapters - Connecting to agents</li> <li>Behavior Specifications - Defining behaviors</li> <li>Evaluation Engine - Running evaluations</li> <li>Reflection Engine - Understanding results</li> <li>Evolution Engine - Improving agents</li> </ul>"},{"location":"concepts/reflection/","title":"Reflection Engine","text":"<p>The Reflection Engine analyzes evaluation results and generates human-readable insights about agent behavior.</p>"},{"location":"concepts/reflection/#what-is-reflection","title":"What is Reflection?","text":"<p>Reflection is the process of understanding why an agent behaved the way it did. It transforms raw evaluation results into actionable insights.</p>"},{"location":"concepts/reflection/#how-reflection-works","title":"How Reflection Works","text":""},{"location":"concepts/reflection/#1-analyze-results","title":"1. Analyze Results","text":"<p>The engine analyzes evaluation results:</p> <pre><code>reflection = reflection_engine.reflect(results)\n</code></pre>"},{"location":"concepts/reflection/#2-identify-patterns","title":"2. Identify Patterns","text":"<p>It identifies patterns in failures:</p> <ul> <li>Common failure modes</li> <li>Root causes</li> <li>Evidence patterns</li> </ul>"},{"location":"concepts/reflection/#3-generate-insights","title":"3. Generate Insights","text":"<p>It generates human-readable insights:</p> <ul> <li>Summary of issues</li> <li>Root cause analysis</li> <li>Recommendations</li> </ul>"},{"location":"concepts/reflection/#using-the-reflection-engine","title":"Using the Reflection Engine","text":""},{"location":"concepts/reflection/#basic-usage","title":"Basic Usage","text":"<pre><code>from codeoptix.reflection import ReflectionEngine\nfrom codeoptix.artifacts import ArtifactManager\n\n# Create artifact manager\nartifact_manager = ArtifactManager()\n\n# Create reflection engine\nreflection_engine = ReflectionEngine(artifact_manager)\n\n# Generate reflection\nreflection = reflection_engine.reflect(\n    results=evaluation_results,\n    agent_name=\"codex\"\n)\n\nprint(reflection)\n</code></pre>"},{"location":"concepts/reflection/#save-reflection","title":"Save Reflection","text":"<pre><code>reflection = reflection_engine.reflect(\n    results=evaluation_results,\n    agent_name=\"codex\",\n    save=True  # Saves to .codeoptix/artifacts/reflection_*.md\n)\n</code></pre>"},{"location":"concepts/reflection/#from-run-id","title":"From Run ID","text":"<pre><code>reflection = reflection_engine.reflect_from_run_id(\n    run_id=\"abc123\",\n    agent_name=\"codex\"\n)\n</code></pre>"},{"location":"concepts/reflection/#reflection-report-structure","title":"Reflection Report Structure","text":"<p>The reflection report includes:</p>"},{"location":"concepts/reflection/#1-summary","title":"1. Summary","text":"<p>Overall evaluation summary:</p> <pre><code># Reflection Report\n\n## Summary\nThe evaluation identified security issues in the agent's code generation.\nOverall score: 0.65/1.0\n</code></pre>"},{"location":"concepts/reflection/#2-root-causes","title":"2. Root Causes","text":"<p>Why issues occurred:</p> <pre><code>## Root Causes\n1. Agent lacks explicit instructions to avoid hardcoded secrets\n2. No validation for secure coding practices\n3. Missing examples of secure code patterns\n</code></pre>"},{"location":"concepts/reflection/#3-evidence","title":"3. Evidence","text":"<p>Specific examples:</p> <pre><code>## Evidence\n- Hardcoded password found at line 5 in scenario 1\n- SQL injection risk in scenario 2\n- Missing input validation in scenario 3\n</code></pre>"},{"location":"concepts/reflection/#4-recommendations","title":"4. Recommendations","text":"<p>How to improve:</p> <pre><code>## Recommendations\n1. Add explicit security guidelines to agent prompt\n2. Include examples of secure code patterns\n3. Add validation checks in behavior specifications\n</code></pre>"},{"location":"concepts/reflection/#configuration","title":"Configuration","text":"<pre><code>config = {\n    \"generator\": {\n        \"detail_level\": \"high\",  # \"low\", \"medium\", \"high\"\n        \"include_evidence\": True,\n        \"include_recommendations\": True\n    }\n}\n\nreflection_engine = ReflectionEngine(artifact_manager, config=config)\n</code></pre>"},{"location":"concepts/reflection/#best-practices","title":"Best Practices","text":""},{"location":"concepts/reflection/#1-always-reflect","title":"1. Always Reflect","text":"<p>Always generate reflection after evaluation:</p> <pre><code>results = engine.evaluate_behaviors(...)\nreflection = reflection_engine.reflect(results)\n</code></pre>"},{"location":"concepts/reflection/#2-review-reflection","title":"2. Review Reflection","text":"<p>Read the reflection report to understand issues:</p> <pre><code>reflection = reflection_engine.reflect(results, save=True)\n# Review .codeoptix/artifacts/reflection_*.md\n</code></pre>"},{"location":"concepts/reflection/#3-use-for-evolution","title":"3. Use for Evolution","text":"<p>Use reflection for prompt evolution:</p> <pre><code>reflection = reflection_engine.reflect(results)\nevolved = evolution_engine.evolve(results, reflection)\n</code></pre>"},{"location":"concepts/reflection/#integration-with-evolution","title":"Integration with Evolution","text":"<p>Reflection is used by the Evolution Engine to improve prompts:</p> <pre><code># 1. Evaluate\nresults = engine.evaluate_behaviors(...)\n\n# 2. Reflect\nreflection = reflection_engine.reflect(results)\n\n# 3. Evolve\nevolved = evolution_engine.evolve(results, reflection)\n</code></pre>"},{"location":"concepts/reflection/#next-steps","title":"Next Steps","text":"<ul> <li>Evolution Engine - Use reflection to improve agents</li> <li>Python API Guide - Advanced usage</li> <li>CLI Usage - Command-line usage</li> </ul>"},{"location":"examples/acp-integration-examples/","title":"ACP Integration Examples","text":"<p>Examples of using CodeOptiX with Agent Client Protocol (ACP).</p>"},{"location":"examples/acp-integration-examples/#setting-up-acp-agents","title":"Setting Up ACP Agents","text":""},{"location":"examples/acp-integration-examples/#registering-agents","title":"Registering Agents","text":"<pre><code>from codeoptix.acp import ACPAgentRegistry\n\nregistry = ACPAgentRegistry()\n\n# Register Claude Code agent\nregistry.register(\n    name=\"claude-code\",\n    command=\"python claude_agent.py\",\n    description=\"Claude Code via ACP\"\n)\n\n# Register custom agent\nregistry.register(\n    name=\"my-agent\",\n    command=\"node my-agent.js\",\n    cwd=\"/path/to/agent\",\n    description=\"Custom agent implementation\"\n)\n\n# List all agents\nagents = registry.list_agents()\nprint(agents)\n</code></pre>"},{"location":"examples/acp-integration-examples/#using-quality-bridge","title":"Using Quality Bridge","text":"<pre><code>from codeoptix.acp import ACPQualityBridge\n\n# Create quality bridge\nbridge = ACPQualityBridge(\n    agent_command=\"python claude_agent.py\",\n    behaviors=\"insecure-code,vacuous-tests\",\n    auto_eval=True\n)\n\n# The bridge will automatically evaluate code quality\n# as the agent generates code\n</code></pre>"},{"location":"examples/acp-integration-examples/#multi-agent-judge-workflow","title":"Multi-Agent Judge Workflow","text":""},{"location":"examples/acp-integration-examples/#setting-up-multi-agent-evaluation","title":"Setting Up Multi-Agent Evaluation","text":"<pre><code>from codeoptix.acp import ACPAgentRegistry\nfrom codeoptix.acp.judge import ACPJudge\n\nregistry = ACPAgentRegistry()\n\n# Register multiple agents\nregistry.register(\"generator\", \"python claude_agent.py\")\nregistry.register(\"judge1\", \"python grok_agent.py\")\nregistry.register(\"judge2\", \"python gpt_agent.py\")\n\n# Create multi-agent judge\njudge = ACPJudge(\n    generate_agent=\"generator\",\n    judge_agents=[\"judge1\", \"judge2\"],\n    behaviors=[\"insecure-code\", \"vacuous-tests\"]\n)\n\n# Run evaluation\nresults = await judge.evaluate_prompt(\n    prompt=\"Write a secure user authentication system\",\n    context={\"requirements\": \"Must use bcrypt, validate inputs, prevent SQL injection\"}\n)\n\nprint(f\"Overall Score: {results['overall_score']}\")\n</code></pre>"},{"location":"examples/acp-integration-examples/#editor-integration","title":"Editor Integration","text":""},{"location":"examples/acp-integration-examples/#zed-editor-integration","title":"Zed Editor Integration","text":"<ol> <li> <p>Start CodeOptiX as ACP agent: <pre><code>codeoptix acp register\n</code></pre></p> </li> <li> <p>Configure Zed to connect to CodeOptiX at the displayed endpoint</p> </li> <li> <p>CodeOptiX will automatically evaluate code quality in real-time</p> </li> </ol>"},{"location":"examples/acp-integration-examples/#vs-code-integration","title":"VS Code Integration","text":"<p>Use the ACP extension to connect to CodeOptiX agents.</p>"},{"location":"examples/acp-integration-examples/#custom-acp-agent-implementation","title":"Custom ACP Agent Implementation","text":""},{"location":"examples/acp-integration-examples/#basic-agent-structure","title":"Basic Agent Structure","text":"<pre><code>from acp import Agent, Server\nfrom codeoptix.acp import CodeOptiXAgent\n\nclass MyACPAgent(Agent):\n    def __init__(self):\n        super().__init__()\n        self.codeoptix = CodeOptiXAgent()\n\n    async def handle_generate_code(self, prompt: str) -&gt; dict:\n        \"\"\"Handle code generation requests.\"\"\"\n        result = await self.codeoptix.generate_code(prompt)\n\n        return {\n            \"code\": result.code,\n            \"tests\": result.tests,\n            \"quality_score\": result.metadata.get(\"quality_score\", 0.0)\n        }\n\n# Start the agent\nasync def main():\n    agent = MyACPAgent()\n    server = Server(agent)\n    await server.run()\n\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(main())\n</code></pre>"},{"location":"examples/acp-integration-examples/#acp-quality-bridge-example","title":"ACP Quality Bridge Example","text":""},{"location":"examples/acp-integration-examples/#automated-quality-monitoring","title":"Automated Quality Monitoring","text":"<pre><code>from codeoptix.acp import ACPQualityBridge\n\n# Bridge between editor and agent with quality checks\nbridge = ACPQualityBridge(\n    agent_command=\"python coding_agent.py\",\n    behaviors=\"insecure-code,vacuous-tests,plan-drift\",\n    auto_eval=True,\n    quality_threshold=0.8  # Require 80% quality score\n)\n\n# The bridge will:\n# 1. Forward requests to the coding agent\n# 2. Evaluate generated code against behaviors\n# 3. Provide quality feedback to the editor\n# 4. Block low-quality code if threshold not met\n\nawait bridge.start()\n</code></pre>"},{"location":"examples/adapter-usage/","title":"Adapter Usage Examples","text":"<p>Examples of using different agent adapters.</p>"},{"location":"examples/adapter-usage/#codex-adapter","title":"Codex Adapter","text":""},{"location":"examples/adapter-usage/#basic-usage","title":"Basic Usage","text":"<pre><code>from codeoptix.adapters.factory import create_adapter\n\nadapter = create_adapter(\"codex\", {\n    \"llm_config\": {\n        \"provider\": \"openai\",\n        \"api_key\": os.getenv(\"OPENAI_API_KEY\"),\n        \"model\": \"gpt-5.2\"\n    }\n})\n\noutput = adapter.execute(\"Write a Python function to calculate factorial\")\nprint(output.code)\n</code></pre>"},{"location":"examples/adapter-usage/#claude-code-adapter","title":"Claude Code Adapter","text":""},{"location":"examples/adapter-usage/#basic-usage_1","title":"Basic Usage","text":"<pre><code>adapter = create_adapter(\"claude-code\", {\n    \"llm_config\": {\n        \"provider\": \"anthropic\",\n        \"api_key\": os.getenv(\"ANTHROPIC_API_KEY\"),\n        \"model\": \"claude-sonnet-4.5\"\n    }\n})\n\noutput = adapter.execute(\"Write secure authentication code\")\nprint(output.code)\n</code></pre>"},{"location":"examples/adapter-usage/#gemini-cli-adapter","title":"Gemini CLI Adapter","text":""},{"location":"examples/adapter-usage/#basic-usage_2","title":"Basic Usage","text":"<pre><code>adapter = create_adapter(\"gemini-cli\", {\n    \"llm_config\": {\n        \"provider\": \"google\",\n        \"api_key\": os.getenv(\"GOOGLE_API_KEY\"),\n        \"model\": \"gemini-3-flash\"\n    }\n})\n\noutput = adapter.execute(\"Write a REST API endpoint\")\nprint(output.code)\n</code></pre>"},{"location":"examples/adapter-usage/#updating-prompts","title":"Updating Prompts","text":""},{"location":"examples/adapter-usage/#update-system-prompt","title":"Update System Prompt","text":"<pre><code>adapter.update_prompt(\n    \"You are a security-focused coding assistant. \"\n    \"Never include hardcoded secrets in code.\"\n)\n\ncurrent_prompt = adapter.get_prompt()\nprint(current_prompt)\n</code></pre>"},{"location":"examples/adapter-usage/#with-context","title":"With Context","text":""},{"location":"examples/adapter-usage/#provide-context","title":"Provide Context","text":"<pre><code>output = adapter.execute(\n    \"Create a database connection function\",\n    context={\n        \"requirements\": [\n            \"Use environment variables for credentials\",\n            \"Include error handling\",\n            \"Add connection pooling\"\n        ],\n        \"plan\": \"Secure database access implementation\"\n    }\n)\n</code></pre>"},{"location":"examples/adapter-usage/#next-steps","title":"Next Steps","text":"<ul> <li>Agent Adapters - Learn about adapters</li> <li>Python API Guide - More examples</li> <li>Behavioral Spec Example - Complete example</li> </ul>"},{"location":"examples/behavioral-spec/","title":"Behavioral Spec Example","text":"<p>Complete example of using CodeOptiX to evaluate agent behavior.</p>"},{"location":"examples/behavioral-spec/#overview","title":"Overview","text":"<p>This example demonstrates evaluating an agent for security issues.</p>"},{"location":"examples/behavioral-spec/#complete-code","title":"Complete Code","text":"<pre><code>import os\nfrom codeoptix.adapters.factory import create_adapter\nfrom codeoptix.evaluation import EvaluationEngine\nfrom codeoptix.reflection import ReflectionEngine\nfrom codeoptix.artifacts import ArtifactManager\nfrom codeoptix.utils.llm import create_llm_client, LLMProvider\n\n# 1. Setup\nadapter = create_adapter(\"codex\", {\n    \"llm_config\": {\n        \"provider\": \"openai\",\n        \"api_key\": os.getenv(\"OPENAI_API_KEY\"),\n    }\n})\n\nllm_client = create_llm_client(LLMProvider.OPENAI)\nartifact_manager = ArtifactManager()\n\n# 2. Evaluate\neval_engine = EvaluationEngine(adapter, llm_client)\nresults = eval_engine.evaluate_behaviors(\n    behavior_names=[\"insecure-code\"]\n)\n\n# 3. Save results\nartifact_manager.save_results(results)\n\n# 4. Reflect\nreflection_engine = ReflectionEngine(artifact_manager)\nreflection = reflection_engine.reflect(results, save=True)\n\n# 5. Print summary\nprint(f\"Overall Score: {results['overall_score']:.2f}\")\nfor behavior_name, behavior_data in results['behaviors'].items():\n    status = \"\u2705 PASSED\" if behavior_data['passed'] else \"\u274c FAILED\"\n    print(f\"{behavior_name}: {status} (Score: {behavior_data['score']:.2f})\")\n</code></pre>"},{"location":"examples/behavioral-spec/#running-the-example","title":"Running the Example","text":"<pre><code>export OPENAI_API_KEY=\"sk-...\"\npython examples/behavioral_spec_example.py\n</code></pre>"},{"location":"examples/behavioral-spec/#expected-output","title":"Expected Output","text":"<pre><code>Overall Score: 0.75\ninsecure-code: \u274c FAILED (Score: 0.75)\n  - Hardcoded password found at line 5\n</code></pre>"},{"location":"examples/behavioral-spec/#next-steps","title":"Next Steps","text":"<ul> <li>GEPA Demo - GEPA evolution example</li> <li>Adapter Usage - Adapter examples</li> <li>Python API Guide - More examples</li> </ul>"},{"location":"examples/gepa-demo/","title":"GEPA Demonstration","text":"<p>Example showing how GEPA optimizes agent prompts.</p>"},{"location":"examples/gepa-demo/#overview","title":"Overview","text":"<p>This example demonstrates the complete GEPA evolution process.</p>"},{"location":"examples/gepa-demo/#complete-code","title":"Complete Code","text":"<pre><code>import os\nfrom codeoptix.adapters.factory import create_adapter\nfrom codeoptix.evaluation import EvaluationEngine\nfrom codeoptix.evolution import EvolutionEngine\nfrom codeoptix.reflection import ReflectionEngine\nfrom codeoptix.artifacts import ArtifactManager\nfrom codeoptix.utils.llm import create_llm_client, LLMProvider\n\n# 1. Setup\nadapter = create_adapter(\"codex\", {\n    \"llm_config\": {\n        \"provider\": \"openai\",\n        \"api_key\": os.getenv(\"OPENAI_API_KEY\"),\n    }\n})\n\nllm_client = create_llm_client(LLMProvider.OPENAI)\nartifact_manager = ArtifactManager()\n\n# 2. Initial evaluation\neval_engine = EvaluationEngine(adapter, llm_client)\nresults = eval_engine.evaluate_behaviors(\n    behavior_names=[\"insecure-code\"]\n)\n\nprint(f\"Initial Score: {results['overall_score']:.2f}\")\n\n# 3. Reflect\nreflection_engine = ReflectionEngine(artifact_manager)\nreflection = reflection_engine.reflect(results)\n\n# 4. Evolve with GEPA\nevolution_config = {\n    \"max_iterations\": 2,\n    \"proposer\": {\n        \"use_gepa\": True  # Enable GEPA\n    }\n}\n\nevolution_engine = EvolutionEngine(\n    adapter, eval_engine, llm_client, artifact_manager, config=evolution_config\n)\n\nevolved = evolution_engine.evolve(\n    evaluation_results=results,\n    reflection=reflection,\n    behavior_names=[\"insecure-code\"]\n)\n\n# 5. Results\nprint(f\"Final Score: {evolved['metadata']['final_score']:.2f}\")\nprint(f\"Improvement: {evolved['metadata']['improvement']:.2f}\")\nprint(f\"\\nEvolved Prompt:\\n{evolved['prompts']['system_prompt']}\")\n</code></pre>"},{"location":"examples/gepa-demo/#running-the-example","title":"Running the Example","text":"<pre><code>export OPENAI_API_KEY=\"sk-...\"\npython examples/gepa_demonstration.py\n</code></pre>"},{"location":"examples/gepa-demo/#expected-output","title":"Expected Output","text":"<pre><code>Initial Score: 0.65\n\ud83e\uddec Evolving prompts with GEPA...\nFinal Score: 0.85\nImprovement: 0.20\n\nEvolved Prompt:\nYou are a security-focused coding assistant. Never include hardcoded secrets...\n</code></pre>"},{"location":"examples/gepa-demo/#next-steps","title":"Next Steps","text":"<ul> <li>GEPA Integration - Learn about GEPA</li> <li>Evolution Engine - Evolution details</li> <li>Python API Guide - More examples</li> </ul>"},{"location":"getting-started/first-evaluation/","title":"Your First Evaluation","text":"<p>A step-by-step guide to running your first evaluation with CodeOptiX.</p>"},{"location":"getting-started/first-evaluation/#overview","title":"Overview","text":"<p>In this guide, you'll: 1. Set up an agent adapter 2. Run an evaluation 3. Understand the results 4. Generate a reflection report</p>"},{"location":"getting-started/first-evaluation/#prerequisites","title":"Prerequisites","text":"<ul> <li>CodeOptiX installed (see Installation)</li> <li>API key set (see Quick Start)</li> </ul>"},{"location":"getting-started/first-evaluation/#step-1-choose-your-agent","title":"Step 1: Choose Your Agent","text":"<p>CodeOptiX works with multiple agents. For this example, we'll use Codex (OpenAI GPT-5.2).</p>"},{"location":"getting-started/first-evaluation/#available-agents","title":"Available Agents","text":"<ul> <li><code>codex</code> - OpenAI GPT-5.2 Code Interpreter</li> <li><code>claude-code</code> - Anthropic Claude (Sonnet 4.5, Opus 4.5) for coding</li> <li><code>gemini-cli</code> - Google Gemini (Gemini 3, Gemini 3 Flash) for coding</li> </ul>"},{"location":"getting-started/first-evaluation/#step-2-choose-behaviors-to-evaluate","title":"Step 2: Choose Behaviors to Evaluate","text":"<p>CodeOptiX includes three built-in behaviors:</p>"},{"location":"getting-started/first-evaluation/#1-insecure-code","title":"1. <code>insecure-code</code>","text":"<p>Detects security vulnerabilities:</p> <ul> <li>Hardcoded secrets</li> <li>SQL injection risks</li> <li>Insecure authentication</li> </ul>"},{"location":"getting-started/first-evaluation/#2-vacuous-tests","title":"2. <code>vacuous-tests</code>","text":"<p>Identifies low-quality tests:</p> <ul> <li>No assertions</li> <li>Trivial tests</li> <li>Missing edge cases</li> </ul>"},{"location":"getting-started/first-evaluation/#3-plan-drift","title":"3. <code>plan-drift</code>","text":"<p>Detects deviations from plans:</p> <ul> <li>Missing features</li> <li>Requirements not met</li> <li>API contract violations</li> </ul> <p>For this example, we'll use <code>insecure-code</code>.</p>"},{"location":"getting-started/first-evaluation/#step-3-run-the-evaluation","title":"Step 3: Run the Evaluation","text":"<p>Run your first evaluation:</p> <pre><code>codeoptix eval \\\n  --agent codex \\\n  --behaviors insecure-code \\\n  --llm-provider openai \\\n  --output results.json\n</code></pre>"},{"location":"getting-started/first-evaluation/#what-happens","title":"What Happens","text":"<ol> <li>Scenario Generation: CodeOptiX generates test scenarios</li> <li>Agent Execution: Your agent runs on each scenario</li> <li>Evaluation: CodeOptiX evaluates the agent's output</li> <li>Results: Results are saved to <code>results.json</code></li> </ol>"},{"location":"getting-started/first-evaluation/#step-4-understand-the-results","title":"Step 4: Understand the Results","text":"<p>Let's look at the results structure:</p> <pre><code>{\n  \"run_id\": \"abc123def456\",\n  \"timestamp\": \"2025-01-20T10:00:00Z\",\n  \"agent\": \"codex\",\n  \"overall_score\": 0.75,\n  \"behaviors\": {\n    \"insecure-code\": {\n      \"behavior_name\": \"insecure-code\",\n      \"scenarios_tested\": 3,\n      \"scenarios_passed\": 2,\n      \"score\": 0.75,\n      \"passed\": true,\n      \"evidence\": [\n        \"Hardcoded password found at line 5\"\n      ],\n      \"scenario_results\": [\n        {\n          \"scenario\": {\n            \"prompt\": \"Write a function to connect to a database\"\n          },\n          \"behavior_result\": {\n            \"passed\": false,\n            \"score\": 0.5,\n            \"evidence\": [\"Hardcoded password\"]\n          }\n        }\n      ]\n    }\n  }\n}\n</code></pre>"},{"location":"getting-started/first-evaluation/#key-fields","title":"Key Fields","text":"<ul> <li><code>overall_score</code>: Overall score (0.0 to 1.0)</li> <li><code>behaviors</code>: Results for each behavior</li> <li><code>score</code>: Behavior-specific score</li> <li><code>passed</code>: Whether the behavior passed</li> <li><code>evidence</code>: Specific issues found</li> </ul>"},{"location":"getting-started/first-evaluation/#step-5-generate-reflection","title":"Step 5: Generate Reflection","text":"<p>Understand why the agent behaved the way it did:</p> <pre><code>codeoptix reflect \\\n  --input results.json \\\n  --output reflection.md\n</code></pre>"},{"location":"getting-started/first-evaluation/#reflection-report-contents","title":"Reflection Report Contents","text":"<p>The reflection report includes:</p> <ol> <li>Summary: Overall evaluation summary</li> <li>Root Causes: Why issues occurred</li> <li>Evidence: Specific examples</li> <li>Recommendations: How to improve</li> </ol> <p>Example reflection:</p> <pre><code># Reflection Report\n\n## Summary\nThe evaluation identified security issues in the agent's code generation.\n\n## Root Causes\n1. Agent lacks explicit instructions to avoid hardcoded secrets\n2. No validation for secure coding practices\n\n## Recommendations\n1. Add explicit security guidelines to agent prompt\n2. Include examples of secure code patterns\n</code></pre>"},{"location":"getting-started/first-evaluation/#step-6-evolve-the-agent-optional","title":"Step 6: Evolve the Agent (Optional)","text":"<p>Automatically improve the agent's prompts:</p> <pre><code>codeoptix evolve \\\n  --input results.json \\\n  --reflection reflection.md \\\n  --iterations 2\n</code></pre>"},{"location":"getting-started/first-evaluation/#what-evolution-does","title":"What Evolution Does","text":"<ol> <li>Analyzes Results: Identifies failure patterns</li> <li>Generates Prompts: Creates improved prompts using GEPA</li> <li>Tests Prompts: Evaluates new prompts</li> <li>Saves Results: Stores evolved prompts</li> </ol>"},{"location":"getting-started/first-evaluation/#complete-example","title":"Complete Example","text":"<p>Here's a complete Python example:</p> <pre><code>import os\nfrom codeoptix.adapters.factory import create_adapter\nfrom codeoptix.evaluation import EvaluationEngine\nfrom codeoptix.reflection import ReflectionEngine\nfrom codeoptix.artifacts import ArtifactManager\nfrom codeoptix.utils.llm import create_llm_client, LLMProvider\n\n# 1. Create adapter\nadapter = create_adapter(\"codex\", {\n    \"llm_config\": {\n        \"provider\": \"openai\",\n        \"api_key\": os.getenv(\"OPENAI_API_KEY\"),\n    }\n})\n\n# 2. Create evaluation engine\nllm_client = create_llm_client(LLMProvider.OPENAI)\nengine = EvaluationEngine(adapter, llm_client)\n\n# 3. Run evaluation\nresults = engine.evaluate_behaviors(\n    behavior_names=[\"insecure-code\"]\n)\n\n# 4. Save results\nartifact_manager = ArtifactManager()\nresults_file = artifact_manager.save_results(results)\nprint(f\"Results saved to: {results_file}\")\n\n# 5. Generate reflection\nreflection_engine = ReflectionEngine(artifact_manager)\nreflection = reflection_engine.reflect(results, save=True)\nprint(\"Reflection generated!\")\n\n# 6. Print summary\nprint(f\"\\nOverall Score: {results['overall_score']:.2f}\")\nfor behavior_name, behavior_data in results['behaviors'].items():\n    status = \"\u2705 PASSED\" if behavior_data['passed'] else \"\u274c FAILED\"\n    print(f\"{behavior_name}: {status} (Score: {behavior_data['score']:.2f})\")\n</code></pre>"},{"location":"getting-started/first-evaluation/#understanding-scores","title":"Understanding Scores","text":""},{"location":"getting-started/first-evaluation/#score-ranges","title":"Score Ranges","text":"<ul> <li>0.9 - 1.0: Excellent - No issues found</li> <li>0.7 - 0.9: Good - Minor issues</li> <li>0.5 - 0.7: Fair - Some issues need attention</li> <li>0.0 - 0.5: Poor - Significant issues</li> </ul>"},{"location":"getting-started/first-evaluation/#what-affects-scores","title":"What Affects Scores","text":"<ul> <li>Number of scenarios: More scenarios = more reliable score</li> <li>Severity of issues: Critical issues lower scores more</li> <li>Evidence quality: Clear evidence improves accuracy</li> </ul>"},{"location":"getting-started/first-evaluation/#next-steps","title":"Next Steps","text":"<p>Now that you've run your first evaluation:</p> <ol> <li>Core Concepts - Understand how CodeOptiX works</li> <li>Python API Guide - Advanced Python usage</li> <li>CLI Usage - All CLI commands</li> <li>Custom Behaviors - Create your own behaviors</li> </ol>"},{"location":"getting-started/first-evaluation/#tips","title":"Tips","text":""},{"location":"getting-started/first-evaluation/#start-small","title":"Start Small","text":"<p>Begin with one behavior and a few scenarios:</p> <pre><code>codeoptix eval \\\n  --agent codex \\\n  --behaviors insecure-code\n</code></pre>"},{"location":"getting-started/first-evaluation/#use-context","title":"Use Context","text":"<p>Provide context for better evaluations:</p> <pre><code>codeoptix eval \\\n  --agent codex \\\n  --behaviors plan-drift \\\n  --context '{\"plan\": \"Create a secure API with authentication\"}'\n</code></pre>"},{"location":"getting-started/first-evaluation/#review-results","title":"Review Results","text":"<p>Always review the reflection report:</p> <pre><code>codeoptix reflect --input results.json\n</code></pre>"},{"location":"getting-started/first-evaluation/#common-issues","title":"Common Issues","text":""},{"location":"getting-started/first-evaluation/#low-scores","title":"Low Scores","text":"<p>If you get low scores: 1. Check the evidence in results 2. Review the reflection report 3. Consider evolving the agent prompts</p>"},{"location":"getting-started/first-evaluation/#no-results","title":"No Results","text":"<p>If no results are generated: 1. Check API key is set 2. Verify agent type is correct 3. Check network connection</p>"},{"location":"getting-started/first-evaluation/#errors","title":"Errors","text":"<p>If you encounter errors: 1. Check the error message 2. Verify all prerequisites are met 3. Check the CLI Usage Guide for troubleshooting tips</p>"},{"location":"getting-started/first-evaluation/#need-help","title":"Need Help?","text":"<ul> <li>\ud83d\udcd6 Read the full documentation</li> <li>\ud83d\udcac Ask questions in Discussions</li> <li>\ud83d\udc1b Report issues on GitHub</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>This guide will help you install CodeOptiX on your system.</p>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<p>Before installing CodeOptiX, make sure you have:</p> <ul> <li>Python 3.12 or higher - Check with <code>python --version</code></li> <li>pip or uv - Python package manager</li> <li>API Key - From at least one LLM provider (OpenAI, Anthropic, or Google)</li> </ul>"},{"location":"getting-started/installation/#installation-methods","title":"Installation Methods","text":""},{"location":"getting-started/installation/#method-1-using-pip-recommended-for-beginners","title":"Method 1: Using pip (Recommended for Beginners)","text":"<p>The simplest way to install CodeOptiX:</p> <pre><code>pip install codeoptix\n</code></pre>"},{"location":"getting-started/installation/#method-2-using-uv-faster","title":"Method 2: Using uv (Faster)","text":"<p>If you have <code>uv</code> installed:</p> <pre><code>uv pip install codeoptix\n</code></pre>"},{"location":"getting-started/installation/#method-3-from-source","title":"Method 3: From Source","text":"<p>For development or latest features:</p> <pre><code># Clone the repository\ngit clone https://github.com/SuperagenticAI/codeoptix.git\ncd codeoptix\n\n# Install in development mode\npip install -e .\n</code></pre>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<p>After installation, verify that CodeOptiX is installed correctly:</p> <pre><code># Check version\ncodeoptix --version\n\n# View help\ncodeoptix --help\n</code></pre> <p>You should see output like:</p> <pre><code>CodeOptiX, version 0.1.0\n</code></pre>"},{"location":"getting-started/installation/#setting-up-llm-providers","title":"Setting Up LLM Providers","text":"<p>CodeOptiX supports multiple LLM providers. Choose the one that works best for you:</p>"},{"location":"getting-started/installation/#option-1-ollama-recommended-for-open-source-users","title":"Option 1: Ollama (Recommended for Open-Source Users) \ud83c\udd95","text":"<p>No API key required! Use local Ollama models:</p> <pre><code># 1. Install Ollama: https://ollama.com\n# 2. Start Ollama service\nollama serve\n\n# 3. Pull a model\nollama pull llama3.1:8b\n\n# 4. Use in CodeOptiX\ncodeoptix eval --agent basic --behaviors insecure-code --llm-provider ollama\n</code></pre> <p>See Ollama Integration Guide for detailed setup.</p>"},{"location":"getting-started/installation/#option-2-cloud-providers-requires-api-keys","title":"Option 2: Cloud Providers (Requires API Keys)","text":"<p>CodeOptiX supports cloud LLM providers. Set at least one API key:</p>"},{"location":"getting-started/installation/#openai","title":"OpenAI","text":"<pre><code>export OPENAI_API_KEY=\"sk-your-api-key-here\"\n</code></pre>"},{"location":"getting-started/installation/#anthropic","title":"Anthropic","text":"<pre><code>export ANTHROPIC_API_KEY=\"sk-ant-your-api-key-here\"\n</code></pre>"},{"location":"getting-started/installation/#google","title":"Google","text":"<pre><code>export GOOGLE_API_KEY=\"your-api-key-here\"\n</code></pre> <p>Ollama vs Cloud Providers</p> <p>Ollama (Local): - \u2705 No API key required - \u2705 Free to use - \u2705 Privacy-friendly (runs locally) - \u2705 Works offline - \u26a0\ufe0f Requires local compute resources</p> <p>Cloud Providers: - \u2705 More powerful models - \u2705 No local compute needed - \u26a0\ufe0f Requires API key - \u26a0\ufe0f May incur costs - \u26a0\ufe0f Data sent to external service</p>"},{"location":"getting-started/installation/#windows-powershell","title":"Windows (PowerShell)","text":"<pre><code>$env:OPENAI_API_KEY=\"sk-your-api-key-here\"\n</code></pre>"},{"location":"getting-started/installation/#windows-cmd","title":"Windows (CMD)","text":"<pre><code>set OPENAI_API_KEY=sk-your-api-key-here\n</code></pre>"},{"location":"getting-started/installation/#optional-dependencies","title":"Optional Dependencies","text":""},{"location":"getting-started/installation/#development-tools","title":"Development Tools","text":"<p>For development and testing:</p> <pre><code>pip install codeoptix[dev]\n</code></pre> <p>Includes: - <code>pytest</code> - Testing framework - <code>ruff</code> - Code linter - <code>mypy</code> - Type checker - <code>black</code> - Code formatter</p>"},{"location":"getting-started/installation/#ollama-integration-local-llm-support","title":"Ollama Integration (Local LLM Support)","text":"<p>CodeOptiX supports local Ollama models - no API key required!</p> <p>Prerequisites: 1. Install Ollama: https://ollama.com 2. Start Ollama service: <code>ollama serve</code> 3. Pull a model: <code>ollama pull llama3.1:8b</code> (or <code>gpt-oss:120b</code>, <code>qwen3:8b</code>, etc.)</p> <p>Usage: <pre><code>codeoptix eval \\\n  --agent basic \\\n  --behaviors insecure-code \\\n  --llm-provider ollama \\\n  --config examples/configs/ollama-insecure-code.yaml\n</code></pre></p> <p>Configuration: <pre><code>adapter:\n  llm_config:\n    provider: ollama\n    model: llama3.2:3b  # Or llama3.1:8b, gpt-oss:120b, qwen3:8b, etc.\n    # No api_key needed!\n</code></pre></p> <p>See Ollama Integration Guide for detailed setup and examples.</p>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#installation-fails","title":"Installation Fails","text":"<p>If installation fails, try:</p> <pre><code># Upgrade pip first\npip install --upgrade pip\n\n# Then install CodeOptiX\npip install codeoptix\n</code></pre>"},{"location":"getting-started/installation/#import-errors","title":"Import Errors","text":"<p>If you get import errors:</p> <pre><code># Verify installation\npip show codeoptix\n\n# Reinstall if needed\npip uninstall codeoptix\npip install codeoptix\n</code></pre>"},{"location":"getting-started/installation/#api-key-not-found","title":"API Key Not Found","text":"<p>If CodeOptiX can't find your API key:</p> <ol> <li> <p>Check that the environment variable is set:    <pre><code>echo $OPENAI_API_KEY\n</code></pre></p> </li> <li> <p>Make sure you're using the correct variable name</p> </li> <li>Restart your terminal after setting the variable</li> </ol>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>Now that CodeOptiX is installed, you're ready to:</p> <ol> <li>Quick Start - Run your first evaluation</li> <li>Your First Evaluation - Detailed walkthrough</li> <li>Python API Guide - Use CodeOptiX in Python</li> </ol>"},{"location":"getting-started/installation/#need-help","title":"Need Help?","text":"<p>If you encounter any issues:</p> <ul> <li>Check the Troubleshooting section above</li> <li>Open an issue on GitHub</li> <li>Join our Discussions</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>Get up and running with CodeOptiX in 5 minutes! This guide will walk you through your first evaluation.</p>"},{"location":"getting-started/quickstart/#step-1-install-codeoptix","title":"Step 1: Install CodeOptiX","text":"<p>If you haven't already, install CodeOptiX:</p> <pre><code>pip install codeoptix\n</code></pre>"},{"location":"getting-started/quickstart/#step-2-choose-your-setup","title":"Step 2: Choose Your Setup","text":""},{"location":"getting-started/quickstart/#option-a-ollama-free-no-api-key-required","title":"Option A: Ollama (Free, No API Key Required) \ud83c\udd93","text":"<p>Perfect for getting started! Use local Ollama models - no API keys, no costs, works offline.</p> <pre><code># Install Ollama (https://ollama.com)\n# macOS: brew install ollama\n# Linux: curl -fsSL https://ollama.com/install.sh | sh\n\n# Start Ollama\nollama serve\n\n# Pull a model (in another terminal)\nollama pull llama3.2:3b\n\n# Run evaluation\ncodeoptix eval \\\n  --agent basic \\\n  --behaviors insecure-code \\\n  --llm-provider ollama\n</code></pre>"},{"location":"getting-started/quickstart/#option-b-cloud-providers-requires-api-key","title":"Option B: Cloud Providers (Requires API Key) \u2601\ufe0f","text":"<p>Use OpenAI, Anthropic, or Google models for more advanced evaluations.</p> <p>Set your API key:</p> <pre><code>export OPENAI_API_KEY=\"sk-your-api-key-here\"\n# OR\nexport ANTHROPIC_API_KEY=\"sk-ant-your-api-key-here\"\n# OR\nexport GOOGLE_API_KEY=\"your-api-key-here\"\n</code></pre>"},{"location":"getting-started/quickstart/#step-3-run-your-first-evaluation","title":"Step 3: Run Your First Evaluation","text":"<p>Start with Ollama - It's Free &amp; Works Offline!</p> <p>Recommended for first-time users! Skip API keys and start evaluating immediately.</p>"},{"location":"getting-started/quickstart/#quick-test-with-ollama-recommended","title":"Quick Test with Ollama (Recommended)","text":"<pre><code>codeoptix eval \\\n  --agent basic \\\n  --behaviors insecure-code \\\n  --llm-provider ollama\n</code></pre> <p>Expected Output: <pre><code>\ud83d\udd0d CodeOptiX Evaluation\n============================================================\n\ud83d\udcca Agent: basic\n\ud83d\udccb Behavior(s): insecure-code\n\u2705 Adapter created: basic\n\ud83e\udde0 Using local Ollama provider.\n\n\ud83d\ude80 Running evaluation...\n============================================================\n\u2705 Evaluation Complete!\n============================================================\n\ud83d\udcca Overall Score: 85.71%\n\ud83d\udcc1 Results: .codeoptix/artifacts/results_*.json\n</code></pre></p>"},{"location":"getting-started/quickstart/#advanced-evaluation-with-cloud-providers","title":"Advanced Evaluation with Cloud Providers","text":"<p>For more advanced analysis using latest models:</p> <pre><code># OpenAI GPT-5.2\ncodeoptix eval \\\n  --agent basic \\\n  --behaviors insecure-code \\\n  --llm-provider openai\n\n# Anthropic Claude Opus 4.5\ncodeoptix eval \\\n  --agent claude-code \\\n  --behaviors insecure-code \\\n  --llm-provider anthropic\n</code></pre>"},{"location":"getting-started/quickstart/#what-happens-during-evaluation","title":"What Happens During Evaluation","text":"<p>All commands will:</p> <ul> <li>\u2705 Create the specified agent adapter</li> <li>Sets up the evaluation environment</li> <li>\u2705 Generate test scenarios</li> <li>Creates diverse security test cases automatically</li> <li>\u2705 Run behavioral analysis</li> <li>Evaluates the agent against each scenario</li> <li>\u2705 Save detailed results</li> <li>Stores everything in <code>.codeoptix/artifacts/results_*.json</code></li> </ul>"},{"location":"getting-started/quickstart/#step-4-check-your-results","title":"Step 4: Check Your Results","text":""},{"location":"getting-started/quickstart/#view-evaluation-summary","title":"View Evaluation Summary","text":"<p>CodeOptiX automatically saves results. Check them:</p> <pre><code># List all evaluation runs\ncodeoptix list-runs\n\n# View detailed results (requires jq for pretty printing)\ncat .codeoptix/artifacts/results_*.json | jq .\n</code></pre>"},{"location":"getting-started/quickstart/#understanding-your-results","title":"Understanding Your Results","text":"<p>High Score (80-100%): Your agent performs well on security evaluation Medium Score (50-79%): Some security issues detected - review recommendations Low Score (0-49%): Significant security concerns - needs improvement</p> <p>Sample Results: <pre><code>{\n  \"run_id\": \"7d42c92c\",\n  \"overall_score\": 0.857,  // 85.7% - Good performance!\n  \"behaviors\": {\n    \"insecure-code\": {\n      \"score\": 0.857,\n      \"passed\": true,        // \u2705 Evaluation passed\n      \"evidence\": []         // No critical issues found\n    }\n  }\n}\n</code></pre></p> <p>Key Metrics: - <code>overall_score</code>: 0.0 to 1.0 (higher is better) - <code>passed</code>: <code>true</code> if behavior requirements met - <code>evidence</code>: Specific issues or examples found</p>"},{"location":"getting-started/quickstart/#step-5-generate-reflection-report","title":"Step 5: Generate Reflection Report","text":"<p>Get deep insights into your agent's performance:</p> <pre><code>codeoptix reflect --input .codeoptix/artifacts/results_*.json\n</code></pre> <p>This generates a comprehensive reflection report explaining:</p> <ul> <li>\u2705 What went well</li> <li>Analysis of successful behaviors and patterns</li> <li>\ud83d\udd0d What needs improvement</li> <li>Identification of problematic patterns</li> <li>\ud83d\udd27 Root causes of issues</li> <li>Deep analysis of why problems occurred</li> <li>\ud83d\udca1 Actionable recommendations</li> <li>Specific suggestions for improvement</li> </ul>"},{"location":"getting-started/quickstart/#step-6-evolve-the-agent-advanced","title":"Step 6: Evolve the Agent (Advanced)","text":"<p>Automatically improve your agent's prompts using AI:</p> <pre><code>codeoptix evolve \\\n  --input .codeoptix/artifacts/results_*.json \\\n  --iterations 2\n</code></pre> <p>Evolution Process:</p> <ul> <li>\ud83d\udd0d Analyzes evaluation results</li> <li>Identifies patterns and issues</li> <li>\ud83e\udde0 Generates improved prompts</li> <li>Uses GEPA optimization algorithm</li> <li>\ud83e\uddea Tests new prompts</li> <li>Validates improvements work</li> <li>\ud83d\udcbe Saves evolved prompts</li> <li>Stores in <code>.codeoptix/artifacts/evolved_prompts_*.yaml</code></li> </ul> <p>Evolution requires API keys</p> <p>This advanced feature needs cloud LLM access for the optimization process.</p>"},{"location":"getting-started/quickstart/#complete-python-example","title":"Complete Python Example","text":"<p>Here's a complete example using Ollama (no API keys needed):</p> <pre><code>from codeoptix.adapters.factory import create_adapter\nfrom codeoptix.evaluation import EvaluationEngine\nfrom codeoptix.utils.llm import create_llm_client, LLMProvider\n\n# 1. Create a basic adapter with Ollama\nadapter = create_adapter(\"basic\", {\n    \"llm_config\": {\n        \"provider\": \"ollama\",\n        \"model\": \"llama3.2:3b\"  # Use any installed Ollama model\n    }\n})\n\n# 2. Create evaluation engine\nllm_client = create_llm_client(LLMProvider.OLLAMA)\nengine = EvaluationEngine(adapter, llm_client)\n\n# 3. Evaluate behaviors\nresults = engine.evaluate_behaviors(\n    behavior_names=[\"insecure-code\"]\n)\n\n# 4. Print results\nprint(f\"Overall Score: {results['overall_score']:.1%}\")\nfor behavior_name, behavior_data in results['behaviors'].items():\n    status = \"\u2705 PASS\" if behavior_data['passed'] else \"\u274c FAIL\"\n    print(f\"{behavior_name}: {behavior_data['score']:.1%} {status}\")\n</code></pre> <p>Expected Output: <pre><code>Overall Score: 85.7%\ninsecure-code: 85.7% \u2705 PASS\n</code></pre></p>"},{"location":"getting-started/quickstart/#whats-next","title":"What's Next?","text":"<p>Now that you've run your first evaluation:</p> <ol> <li>Your First Evaluation - Detailed walkthrough</li> <li>Core Concepts - Understand how CodeOptiX works</li> <li>Python API Guide - Advanced usage</li> <li>CLI Usage - All CLI commands</li> </ol>"},{"location":"getting-started/quickstart/#common-commands","title":"Common Commands","text":"<p>Here are the most common commands you'll use:</p> <pre><code># Evaluate agent\ncodeoptix eval --agent codex --behaviors insecure-code\n\n# Generate reflection\ncodeoptix reflect --input results.json\n\n# Evolve prompts\ncodeoptix evolve --input results.json\n\n# Run full pipeline\ncodeoptix run --agent codex --behaviors insecure-code --evolve\n\n# List all runs\ncodeoptix list-runs\n</code></pre>"},{"location":"getting-started/quickstart/#tips-for-beginners","title":"Tips for Beginners","text":""},{"location":"getting-started/quickstart/#start-simple","title":"Start Simple","text":"<p>Begin with a single behavior:</p> <pre><code>codeoptix eval --agent codex --behaviors insecure-code\n</code></pre>"},{"location":"getting-started/quickstart/#use-context","title":"Use Context","text":"<p>Provide context for better evaluations:</p> <pre><code>codeoptix eval \\\n  --agent codex \\\n  --behaviors plan-drift \\\n  --context '{\"plan\": \"Create a secure API\"}'\n</code></pre>"},{"location":"getting-started/quickstart/#check-results","title":"Check Results","text":"<p>Always review the results:</p> <pre><code>codeoptix reflect --input results.json\n</code></pre>"},{"location":"getting-started/quickstart/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/quickstart/#api-key-not-found","title":"\"API key not found\"","text":"<p>Make sure you've set your API key:</p> <pre><code>echo $OPENAI_API_KEY\n</code></pre>"},{"location":"getting-started/quickstart/#agent-not-found","title":"\"Agent not found\"","text":"<p>Check that you're using a supported agent:</p> <ul> <li><code>codex</code> - OpenAI GPT-5.2</li> <li><code>claude-code</code> - Anthropic Claude (Sonnet 4.5, Opus 4.5)</li> <li><code>gemini-cli</code> - Google Gemini (Gemini 3, Gemini 3 Flash)</li> </ul>"},{"location":"getting-started/quickstart/#behavior-not-found","title":"\"Behavior not found\"","text":"<p>Use one of the built-in behaviors:</p> <ul> <li><code>insecure-code</code> - Security vulnerabilities</li> <li><code>vacuous-tests</code> - Test quality</li> <li><code>plan-drift</code> - Plan alignment</li> </ul>"},{"location":"getting-started/quickstart/#need-help","title":"Need Help?","text":"<ul> <li>\ud83d\udcd6 Read the full documentation</li> <li>\ud83d\udcac Ask questions in Discussions</li> <li>\ud83d\udc1b Report issues on GitHub</li> </ul>"},{"location":"getting-started/single-behavior-quickstart/","title":"Quick Start: Single Behavior","text":"<p>The easiest way to get started with CodeOptiX is to use a single behavior. This guide will help you run your first evaluation in under 5 minutes.</p>"},{"location":"getting-started/single-behavior-quickstart/#prerequisites","title":"Prerequisites","text":"<ol> <li>Python 3.12+ installed</li> <li>API Key for one of these providers:</li> <li>OpenAI (for evaluation)</li> <li>Anthropic (for Claude Code agent)</li> <li>Google (for Gemini agent)</li> </ol>"},{"location":"getting-started/single-behavior-quickstart/#step-1-install-codeoptix","title":"Step 1: Install CodeOptiX","text":"<pre><code>pip install codeoptix\n</code></pre> <p>Or using <code>uv</code> (recommended):</p> <pre><code>uv pip install codeoptix\n</code></pre>"},{"location":"getting-started/single-behavior-quickstart/#step-2-set-your-api-key","title":"Step 2: Set Your API Key","text":"<pre><code>export OPENAI_API_KEY=\"your-key-here\"\n</code></pre> <p>Or for Claude Code:</p> <pre><code>export ANTHROPIC_API_KEY=\"your-key-here\"\n</code></pre>"},{"location":"getting-started/single-behavior-quickstart/#step-3-run-your-first-evaluation","title":"Step 3: Run Your First Evaluation","text":""},{"location":"getting-started/single-behavior-quickstart/#option-1-using-the-cli-easiest","title":"Option 1: Using the CLI (Easiest)","text":"<pre><code>codeoptix eval \\\n  --agent claude-code \\\n  --behaviors \"insecure-code\" \\\n  --llm-provider openai\n</code></pre> <p>This will:</p> <ul> <li>\u2705 Check for security issues (insecure-code behavior)</li> <li>\u2705 Use Claude Code as the agent</li> <li>\u2705 Use OpenAI for evaluation</li> <li>\u2705 Save results automatically</li> </ul>"},{"location":"getting-started/single-behavior-quickstart/#option-2-using-a-config-file-recommended","title":"Option 2: Using a Config File (Recommended)","text":"<pre><code>codeoptix eval \\\n  --agent claude-code \\\n  --behaviors \"insecure-code\" \\\n  --config examples/configs/single-behavior-insecure-code.yaml\n</code></pre>"},{"location":"getting-started/single-behavior-quickstart/#option-3-using-python","title":"Option 3: Using Python","text":"<pre><code>from codeoptix.adapters.factory import create_adapter\nfrom codeoptix.evaluation import EvaluationEngine\nfrom codeoptix.utils.llm import LLMProvider, create_llm_client\nimport os\n\n# Create adapter\nadapter = create_adapter(\"claude-code\", {\n    \"llm_config\": {\n        \"provider\": \"anthropic\",\n        \"api_key\": os.getenv(\"ANTHROPIC_API_KEY\"),\n    }\n})\n\n# Create LLM client\nllm_client = create_llm_client(LLMProvider.OPENAI, api_key=os.getenv(\"OPENAI_API_KEY\"))\n\n# Create evaluation engine\neval_engine = EvaluationEngine(adapter, llm_client)\n\n# Run evaluation with single behavior\nresults = eval_engine.evaluate_behaviors(\n    behavior_names=[\"insecure-code\"]  # Behavior name as string\n)\n\nprint(f\"Score: {results['overall_score']:.2%}\")\n</code></pre>"},{"location":"getting-started/single-behavior-quickstart/#available-behaviors","title":"Available Behaviors","text":"<p>You can use any of these behaviors:</p>"},{"location":"getting-started/single-behavior-quickstart/#1-insecure-code-security","title":"1. insecure-code (Security)","text":"<p>Behavior Name: <code>insecure-code</code></p> <p>Checks for security vulnerabilities: - Hardcoded secrets - SQL injection risks - XSS vulnerabilities</p> <pre><code>codeoptix eval --agent claude-code --behaviors \"insecure-code\"\n</code></pre> <p>Behavior Name: <code>insecure-code</code></p>"},{"location":"getting-started/single-behavior-quickstart/#2-vacuous-tests-test-quality","title":"2. vacuous-tests (Test Quality)","text":"<p>Behavior Name: <code>vacuous-tests</code></p> <p>Checks test quality: - Missing assertions - Trivial tests - Test coverage</p> <pre><code>codeoptix eval --agent claude-code --behaviors \"vacuous-tests\"\n</code></pre> <p>Behavior Name: <code>vacuous-tests</code></p>"},{"location":"getting-started/single-behavior-quickstart/#3-plan-drift-requirements","title":"3. plan-drift (Requirements)","text":"<p>Behavior Name: <code>plan-drift</code></p> <p>Checks requirements alignment: - Plan deviations - Missing features - Extra features</p> <pre><code>codeoptix eval --agent claude-code --behaviors \"plan-drift\"\n</code></pre> <p>Behavior Name: <code>plan-drift</code></p>"},{"location":"getting-started/single-behavior-quickstart/#understanding-results","title":"Understanding Results","text":"<p>After running an evaluation, you'll see:</p> <pre><code>\u2705 Evaluation Complete!\n\ud83d\udcca Overall Score: 85.00%\n\ud83d\udcc1 Results: artifacts/results_run-001.json\n\ud83c\udd94 Run ID: run-001\n\n\ud83d\udccb Behavior Results:\n\n   \u2705 **insecure-code**: 85.00%\n</code></pre>"},{"location":"getting-started/single-behavior-quickstart/#what-the-score-means","title":"What the Score Means","text":"<ul> <li>100%: Perfect - no issues found</li> <li>80-99%: Good - minor issues</li> <li>50-79%: Needs improvement</li> <li>&lt;50%: Critical issues found</li> </ul>"},{"location":"getting-started/single-behavior-quickstart/#viewing-detailed-results","title":"Viewing Detailed Results","text":"<pre><code># View the results file\ncat artifacts/results_run-001.json\n\n# Generate a reflection report\ncodeoptix reflect --input artifacts/results_run-001.json\n</code></pre>"},{"location":"getting-started/single-behavior-quickstart/#common-issues","title":"Common Issues","text":""},{"location":"getting-started/single-behavior-quickstart/#api-key-required-error","title":"\"API key required\" Error","text":"<p>Solution: Set your API key: <pre><code>export OPENAI_API_KEY=\"your-key-here\"\n</code></pre></p>"},{"location":"getting-started/single-behavior-quickstart/#unsupported-adapter-type-error","title":"\"Unsupported adapter type\" Error","text":"<p>Solution: Use a supported agent: - <code>claude-code</code> (Anthropic) - <code>codex</code> (OpenAI) - <code>gemini-cli</code> (Google)</p>"},{"location":"getting-started/single-behavior-quickstart/#invalid-behavior-name-error","title":"\"Invalid behavior name\" Error","text":"<p>Solution: Use a valid behavior:</p> <ul> <li> <p><code>insecure-code</code></p> </li> <li> <p><code>vacuous-tests</code></p> </li> <li> <p><code>plan-drift</code></p> </li> </ul>"},{"location":"getting-started/single-behavior-quickstart/#next-steps","title":"Next Steps","text":"<p>Once you're comfortable with a single behavior:</p> <ol> <li>Try other behaviors - Test different aspects of code quality</li> <li>Use multiple behaviors - Combine checks:    <pre><code>codeoptix eval --agent claude-code --behaviors \"insecure-code,vacuous-tests\"\n</code></pre></li> <li>Use in CI/CD - Add to GitHub Actions:    <pre><code>codeoptix ci --agent codex --behaviors \"insecure-code\" --fail-on-failure\n</code></pre></li> <li>Generate reflection - Understand failures:    <pre><code>codeoptix reflect --input results.json\n</code></pre></li> </ol>"},{"location":"getting-started/single-behavior-quickstart/#example-configurations","title":"Example Configurations","text":"<p>We provide example configs for single behaviors:</p> <ul> <li><code>examples/configs/single-behavior-insecure-code.yaml</code> - Security checks</li> <li><code>examples/configs/single-behavior-vacuous-tests.yaml</code> - Test quality</li> <li><code>examples/configs/single-behavior-plan-drift.yaml</code> - Requirements alignment</li> </ul>"},{"location":"getting-started/single-behavior-quickstart/#next-steps_1","title":"Next Steps","text":"<p>Ready for more advanced usage?</p> <ul> <li>Quick Start - Comprehensive guide with Ollama support</li> <li>Your First Evaluation - Detailed step-by-step walkthrough</li> <li>Core Concepts - Understand how CodeOptiX works</li> <li>Python API Guide - Advanced Python usage</li> </ul>"},{"location":"getting-started/single-behavior-quickstart/#getting-help","title":"Getting Help","text":"<ul> <li>\ud83d\udcd6 Full Documentation</li> <li>\ud83d\udcac GitHub Issues</li> <li>\ud83d\udce7 Support</li> </ul>"},{"location":"guides/acp-integration/","title":"ACP (Agent Client Protocol) Integration","text":"<p>CodeOptiX provides comprehensive ACP integration for editor integration, multi-agent workflows, and code optimization.</p>"},{"location":"guides/acp-integration/#overview","title":"Overview","text":"<p>ACP (Agent Client Protocol) is a protocol for connecting AI agents to editors and other clients. CodeOptiX uses ACP to:</p> <ul> <li>Act as an ACP agent - Be used directly by editors (Zed, JetBrains, Neovim, VS Code)</li> <li>Connect to other agents - Use any ACP-compatible agent via the protocol</li> <li>Quality bridge - Sit between editor and agents, automatically evaluating code quality</li> <li>Multi-agent orchestration - Route to best agent for each task</li> <li>Multi-agent judge - Use different agents for generation vs. judgment</li> </ul>"},{"location":"guides/acp-integration/#quick-start","title":"Quick Start","text":""},{"location":"guides/acp-integration/#register-codeoptix-as-an-acp-agent","title":"Register CodeOptiX as an ACP Agent","text":"<p>Make CodeOptiX available to ACP-compatible editors:</p> <pre><code>codeoptix acp register\n</code></pre> <p>This starts CodeOptiX as an ACP agent. Connect from your editor using ACP protocol.</p>"},{"location":"guides/acp-integration/#use-codeoptix-as-quality-bridge","title":"Use CodeOptiX as Quality Bridge","text":"<p>CodeOptiX can act as a quality bridge between your editor and coding agents:</p> <pre><code># Using an agent from registry\ncodeoptix acp bridge --agent-name claude-code --auto-eval\n\n# Using a direct command\ncodeoptix acp bridge --agent-command \"python agent.py\" --auto-eval\n</code></pre> <p>The quality bridge automatically: - Extracts code from agent responses - Evaluates code quality - Sends feedback to your editor in real-time</p>"},{"location":"guides/acp-integration/#multi-agent-judge","title":"Multi-Agent Judge","text":"<p>Use different agents for generation and judgment:</p> <pre><code>codeoptix acp judge \\\n  --generate-agent claude-code \\\n  --judge-agent grok \\\n  --prompt \"Write a secure authentication function\"\n</code></pre> <p>This will: 1. Generate code with the generate agent 2. Judge/critique the code with the judge agent 3. Evaluate both with CodeOptiX's evaluation engine</p>"},{"location":"guides/acp-integration/#agent-registry","title":"Agent Registry","text":"<p>Manage ACP-compatible agents:</p>"},{"location":"guides/acp-integration/#register-an-agent","title":"Register an Agent","text":"<pre><code>codeoptix acp registry add \\\n  --name claude-code \\\n  --command \"python claude_agent.py\" \\\n  --description \"Claude Code via ACP\" \\\n  --cwd /path/to/agent\n</code></pre>"},{"location":"guides/acp-integration/#list-registered-agents","title":"List Registered Agents","text":"<pre><code>codeoptix acp registry list\n</code></pre>"},{"location":"guides/acp-integration/#remove-an-agent","title":"Remove an Agent","text":"<pre><code>codeoptix acp registry remove --name claude-code\n</code></pre>"},{"location":"guides/acp-integration/#architecture","title":"Architecture","text":""},{"location":"guides/acp-integration/#quality-bridge-mode","title":"Quality Bridge Mode","text":"<pre><code>Editor (Zed, JetBrains, Neovim, VS Code)\n    \u2502\n    \u251c\u2500\u2192 ACP Protocol\n    \u2502       \u2502\n    \u2502       \u2514\u2500\u2192 CodeOptiX ACP Bridge (Quality Layer)\n    \u2502               \u2502\n    \u2502               \u251c\u2500\u2192 Quality Engineering Layer\n    \u2502               \u2502   \u251c\u2500\u2192 Embedded Evaluations (automatic)\n    \u2502               \u2502   \u251c\u2500\u2192 Quality Assurance (automatic)\n    \u2502               \u2502   \u2514\u2500\u2192 Multi-LLM Critique\n    \u2502               \u2502\n    \u2502               \u251c\u2500\u2192 Agent Orchestration\n    \u2502               \u2502   \u251c\u2500\u2192 Route to Generate Agent\n    \u2502               \u2502   \u251c\u2500\u2192 Route to Judge Agent\n    \u2502               \u2502   \u2514\u2500\u2192 Multi-agent workflows\n    \u2502               \u2502\n    \u2502               \u2514\u2500\u2192 ACP Agent Registry\n    \u2502                       \u2502\n    \u2502                       \u251c\u2500\u2192 Claude Code (via ACP)\n    \u2502                       \u251c\u2500\u2192 Codex (via ACP)\n    \u2502                       \u251c\u2500\u2192 Gemini CLI (via ACP)\n    \u2502                       \u2514\u2500\u2192 Any ACP-compatible agent\n    \u2502\n    \u2514\u2500\u2192 Quality Feedback \u2192 Editor\n</code></pre>"},{"location":"guides/acp-integration/#multi-agent-judge-flow","title":"Multi-Agent Judge Flow","text":"<pre><code>Editor Request\n    \u2502\n    \u251c\u2500\u2192 CodeOptiX ACP Bridge\n    \u2502       \u2502\n    \u2502       \u251c\u2500\u2192 Route to Generate Agent (Claude Code via ACP)\n    \u2502       \u2502       \u2514\u2500\u2192 Code Generated\n    \u2502       \u2502\n    \u2502       \u251c\u2500\u2192 Route to Judge Agent (Grok/Codex via ACP)\n    \u2502       \u2502       \u2514\u2500\u2192 Code Judged\n    \u2502       \u2502\n    \u2502       \u2514\u2500\u2192 CodeOptiX Evaluates Both\n    \u2502               \u2514\u2500\u2192 Quality Report \u2192 Editor\n</code></pre>"},{"location":"guides/acp-integration/#python-api","title":"Python API","text":""},{"location":"guides/acp-integration/#using-acp-agent-registry","title":"Using ACP Agent Registry","text":"<pre><code>from codeoptix.acp import ACPAgentRegistry\n\n# Create registry\nregistry = ACPAgentRegistry()\n\n# Register an agent\nregistry.register(\n    name=\"claude-code\",\n    command=[\"python\", \"claude_agent.py\"],\n    description=\"Claude Code via ACP\",\n)\n\n# Connect to agent\nconnection = await registry.connect(\"claude-code\")\nsession_id = registry.get_session_id(\"claude-code\")\n\n# Send prompt\nresponse = await connection.prompt(\n    session_id=session_id,\n    prompt=[text_block(\"Write a secure login function\")],\n)\n</code></pre>"},{"location":"guides/acp-integration/#using-quality-bridge","title":"Using Quality Bridge","text":"<pre><code>from codeoptix.acp import ACPQualityBridge\nfrom codeoptix.evaluation import EvaluationEngine\nfrom codeoptix.utils.llm import LLMProvider, create_llm_client\n\n# Create evaluation engine\nllm_client = create_llm_client(LLMProvider.OPENAI)\nevaluation_engine = EvaluationEngine(adapter, llm_client)\n\n# Create quality bridge\nbridge = ACPQualityBridge(\n    agent_command=[\"python\", \"agent.py\"],\n    evaluation_engine=evaluation_engine,\n    auto_eval=True,\n    behaviors=[\"insecure-code\", \"vacuous-tests\"],\n)\n\n# Connect and use\nawait bridge.connect()\nresult = await bridge.prompt(\"Write secure code\")\n</code></pre>"},{"location":"guides/acp-integration/#using-multi-agent-judge","title":"Using Multi-Agent Judge","text":"<pre><code>from codeoptix.acp import ACPAgentRegistry, MultiAgentJudge\nfrom codeoptix.evaluation import EvaluationEngine\n\n# Create registry and register agents\nregistry = ACPAgentRegistry()\nregistry.register(name=\"claude-code\", command=[\"python\", \"claude_agent.py\"])\nregistry.register(name=\"grok\", command=[\"python\", \"grok_agent.py\"])\n\n# Create evaluation engine\nevaluation_engine = EvaluationEngine(adapter, llm_client)\n\n# Create multi-agent judge\njudge = MultiAgentJudge(\n    registry=registry,\n    generate_agent=\"claude-code\",\n    judge_agent=\"grok\",\n    evaluation_engine=evaluation_engine,\n)\n\n# Generate and judge\nresult = await judge.generate_and_judge(\"Write a secure API endpoint\")\nprint(f\"Generated: {result['generated_code']}\")\nprint(f\"Judgment: {result['judgment']}\")\nprint(f\"Evaluation: {result['evaluation_results']}\")\n</code></pre>"},{"location":"guides/acp-integration/#using-agent-orchestrator","title":"Using Agent Orchestrator","text":"<pre><code>from codeoptix.acp import ACPAgentRegistry, AgentOrchestrator\n\n# Create registry and orchestrator\nregistry = ACPAgentRegistry()\norchestrator = AgentOrchestrator(registry, evaluation_engine)\n\n# Route to best agent\nresult = await orchestrator.route_to_agent(\n    prompt=\"Write a test suite\",\n    context={\"language\": \"python\"},\n)\n\n# Execute multi-agent workflow\nworkflow = [\n    {\"agent\": \"claude-code\", \"prompt\": \"Generate code\"},\n    {\"agent\": \"grok\", \"prompt\": \"Review the code\"},\n]\nresults = await orchestrator.execute_multi_agent_workflow(workflow)\n</code></pre>"},{"location":"guides/acp-integration/#code-extraction","title":"Code Extraction","text":"<p>CodeOptiX automatically extracts code from ACP messages, tool calls, and responses:</p> <pre><code>from codeoptix.acp.code_extractor import extract_code_from_message, extract_all_code, extract_code_from_text\n\n# Extract from single message\ncode_blocks = extract_code_from_message(acp_message)\n\n# Extract from multiple messages\ncode_blocks = extract_all_code(acp_messages)\n\n# Extract from text content\ncode_blocks = extract_code_from_text(\"```python\\ndef hello():\\n    pass\\n```\")\n\n# Each code block contains:\n# {\n#     \"language\": \"python\",\n#     \"content\": \"def hello(): ...\",\n#     \"type\": \"block\",  # or \"inline\", \"file_edit_new\", \"file_edit_old\"\n#     \"path\": \"file.py\"  # if from file edit\n# }\n</code></pre> <p>Supported extraction sources: - \u2705 Text content blocks (markdown code fences) - \u2705 File edit tool calls - \u2705 Tool call progress updates - \u2705 Agent message chunks - \u2705 Inline code patterns</p>"},{"location":"guides/acp-integration/#real-time-quality-feedback","title":"Real-time Quality Feedback","text":"<p>The quality bridge sends real-time feedback to your editor:</p> <ol> <li>Code Generation: Agent generates code</li> <li>Code Extraction: CodeOptiX automatically extracts code from messages, tool calls, and responses</li> <li>Quality Evaluation: CodeOptiX evaluates code quality using the evaluation engine</li> <li>Feedback: Quality report sent to editor via ACP session updates</li> </ol> <p>The feedback includes: - \u2705/\u274c Pass/fail status for each behavior - Quality scores (percentage) - Evidence of issues (specific examples) - Recommendations for improvement - Overall quality score</p> <p>Real-time Security Checks: The bridge also performs quick security checks in real-time, alerting immediately if potential security issues are detected (e.g., hardcoded secrets, API keys).</p>"},{"location":"guides/acp-integration/#supported-editors","title":"Supported Editors","text":"<p>CodeOptiX works with any ACP-compatible editor:</p> <ul> <li>Zed - Native ACP support</li> <li>JetBrains IDEs - Via ACP plugin</li> <li>Neovim - Via ACP plugin</li> <li>VS Code - Via ACP extension</li> <li>Any ACP-compatible client</li> </ul>"},{"location":"guides/acp-integration/#configuration","title":"Configuration","text":""},{"location":"guides/acp-integration/#bridge-configuration","title":"Bridge Configuration","text":"<pre><code>codeoptix acp bridge \\\n  --agent-name claude-code \\\n  --auto-eval \\\n  --behaviors insecure-code,vacuous-tests,plan-drift \\\n  --cwd /path/to/project\n</code></pre>"},{"location":"guides/acp-integration/#behavior-selection","title":"Behavior Selection","text":"<p>Specify which behaviors to evaluate:</p> <pre><code>codeoptix acp bridge \\\n  --agent-name claude-code \\\n  --behaviors insecure-code,vacuous-tests\n</code></pre> <p>Available behaviors: - <code>insecure-code</code> - Security vulnerabilities - <code>vacuous-tests</code> - Low-quality tests - <code>plan-drift</code> - Requirements alignment</p>"},{"location":"guides/acp-integration/#examples","title":"Examples","text":""},{"location":"guides/acp-integration/#example-1-basic-quality-bridge","title":"Example 1: Basic Quality Bridge","text":"<pre><code># Register an agent\ncodeoptix acp registry add \\\n  --name my-agent \\\n  --command \"python my_agent.py\"\n\n# Use as quality bridge\ncodeoptix acp bridge --agent-name my-agent\n</code></pre>"},{"location":"guides/acp-integration/#example-2-multi-agent-judge","title":"Example 2: Multi-Agent Judge","text":"<pre><code># Register agents\ncodeoptix acp registry add --name claude-code --command \"python claude.py\"\ncodeoptix acp registry add --name grok --command \"python grok.py\"\n\n# Use multi-agent judge\ncodeoptix acp judge \\\n  --generate-agent claude-code \\\n  --judge-agent grok \\\n  --prompt \"Write a secure password hashing function\"\n</code></pre>"},{"location":"guides/acp-integration/#example-3-editor-integration","title":"Example 3: Editor Integration","text":"<ol> <li> <p>Start CodeOptiX as agent:    <pre><code>codeoptix acp register\n</code></pre></p> </li> <li> <p>In your editor (e.g., Zed), connect to CodeOptiX via ACP</p> </li> <li> <p>CodeOptiX will automatically evaluate code quality</p> </li> </ol>"},{"location":"guides/acp-integration/#advanced-features","title":"Advanced Features","text":""},{"location":"guides/acp-integration/#custom-evaluation-behaviors","title":"Custom Evaluation Behaviors","text":"<pre><code>from codeoptix.acp import ACPQualityBridge\n\nbridge = ACPQualityBridge(\n    agent_command=[\"python\", \"agent.py\"],\n    behaviors=[\"insecure-code\", \"custom-behavior\"],  # Custom behaviors\n    auto_eval=True,\n)\n</code></pre>"},{"location":"guides/acp-integration/#agent-orchestration","title":"Agent Orchestration","text":"<pre><code>from codeoptix.acp import AgentOrchestrator\n\norchestrator = AgentOrchestrator(registry, evaluation_engine)\n\n# Intelligent agent selection based on:\n# - Agent capabilities (from registry)\n# - Task type (inferred from prompt)\n# - Context requirements\nresult = await orchestrator.route_to_agent(\n    prompt=\"Write secure Python code\",\n    context={\"language\": \"python\", \"task\": \"security\", \"preferred_agent\": \"claude-code\"},\n)\n\n# The orchestrator will:\n# 1. Check for preferred_agent in context\n# 2. Infer task type (security, review, testing, etc.)\n# 3. Match agents with relevant capabilities\n# 4. Fall back to first available agent if no match\n</code></pre>"},{"location":"guides/acp-integration/#workflow-execution","title":"Workflow Execution","text":"<pre><code># Sequential workflow\nworkflow = [\n    {\"agent\": \"agent1\", \"prompt\": \"Step 1\"},\n    {\"agent\": \"agent2\", \"prompt\": \"Step 2\"},\n]\nresults = await orchestrator.execute_multi_agent_workflow(workflow)\n</code></pre>"},{"location":"guides/acp-integration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/acp-integration/#agent-not-found","title":"Agent Not Found","text":"<p>If you get \"Agent not found\" error: 1. Check agent is registered: <code>codeoptix acp registry list</code> 2. Register the agent: <code>codeoptix acp registry add --name &lt;name&gt; --command &lt;cmd&gt;</code></p>"},{"location":"guides/acp-integration/#connection-issues","title":"Connection Issues","text":"<p>If connection fails: 1. Verify agent command is correct 2. Check agent is ACP-compatible 3. Ensure agent process can be spawned</p>"},{"location":"guides/acp-integration/#quality-evaluation-not-working","title":"Quality Evaluation Not Working","text":"<p>If quality evaluation doesn't run: 1. Ensure <code>--auto-eval</code> is enabled 2. Check evaluation engine is properly configured 3. Verify behaviors are specified correctly</p>"},{"location":"guides/acp-integration/#best-practices","title":"Best Practices","text":"<ol> <li>Register agents in registry - Easier management than direct commands</li> <li>Use specific behaviors - Only evaluate behaviors you need</li> <li>Multi-agent judge - Use for critical code that needs multiple perspectives</li> <li>Real-time feedback - Enable auto-eval for immediate quality checks</li> <li>Agent selection - Use orchestrator for intelligent agent routing</li> </ol>"},{"location":"guides/acp-integration/#see-also","title":"See Also","text":"<ul> <li>CLI Usage Guide - Complete CLI documentation</li> <li>Python API Guide - Python API usage</li> <li>Behavior Specifications - Behavior spec framework</li> <li>Evaluation Engine - Evaluation architecture</li> </ul>"},{"location":"guides/cli-usage/","title":"CLI Usage Guide","text":"<p>Complete guide to using CodeOptiX from the command line.</p>"},{"location":"guides/cli-usage/#installation","title":"Installation","text":"<pre><code>pip install codeoptix\n# or\nuv pip install codeoptix\n</code></pre>"},{"location":"guides/cli-usage/#commands","title":"Commands","text":""},{"location":"guides/cli-usage/#codeoptix-eval-evaluate-agent-behavior","title":"<code>codeoptix eval</code> - Evaluate Agent Behavior","text":"<p>Evaluate a coding agent against behavior specifications.</p> <p>Basic Usage: <pre><code>codeoptix eval \\\n  --agent codex \\\n  --behaviors \"insecure-code,vacuous-tests\" \\\n  --llm-provider openai\n</code></pre></p> <p>Options: - <code>--agent</code> (required): Agent type (<code>basic</code>, <code>claude-code</code>, <code>codex</code>, <code>gemini-cli</code>) - <code>--behaviors</code> (required): Comma-separated behavior names - <code>--output</code>: Output file path (default: <code>results.json</code>) - <code>--config</code>: Path to config file (JSON/YAML) - <code>--llm-provider</code>: LLM provider (<code>anthropic</code>, <code>openai</code>, <code>google</code>, <code>ollama</code>) - <code>--llm-api-key</code>: API key (or set environment variable) - <code>--context</code>: Path to context file with plan/requirements (JSON) - <code>--fail-on-failure</code>: Exit with error code if behaviors fail</p> <p>Agent vs LLM Provider</p> <ul> <li><code>--agent</code>: Specifies the agent interface/personality (prompts, evaluation style)</li> <li><code>--llm-provider</code>: Specifies which LLM service powers the agent</li> <li>Example: <code>--agent basic --llm-provider ollama</code> uses simple prompts with Ollama models</li> </ul> <p>Available behaviors:</p> <ul> <li> <p><code>insecure-code</code>: Detects insecure coding patterns (hardcoded secrets, SQL injection, etc.).</p> </li> <li> <p><code>vacuous-tests</code>: Detects low-value tests (no assertions, trivial tests, etc.).</p> </li> <li> <p><code>plan-drift</code>: Detects drift from a plan/requirements context.</p> </li> </ul> <p>Example: <pre><code>export OPENAI_API_KEY=\"sk-...\"\ncodeoptix eval \\\n  --agent codex \\\n  --behaviors \"insecure-code,vacuous-tests\" \\\n  --llm-provider openai \\\n  --context plan.json \\\n  --fail-on-failure\n</code></pre></p>"},{"location":"guides/cli-usage/#codeoptix-ci-cicd-mode","title":"<code>codeoptix ci</code> - CI/CD Mode","text":"<p>Run CodeOptiX in CI/CD mode, optimized for automation and pipelines.</p> <p>Basic Usage: <pre><code>codeoptix ci \\\n  --agent codex \\\n  --behaviors insecure-code \\\n  --llm-provider openai \\\n  --fail-on-failure\n</code></pre></p> <p>Options: - <code>--agent</code> (required): Agent type (<code>claude-code</code>, <code>codex</code>, <code>gemini-cli</code>) - <code>--behaviors</code> (required): Comma-separated behavior names - <code>--config</code>: Path to config file (JSON/YAML) - <code>--llm-provider</code>: LLM provider (<code>anthropic</code>, <code>openai</code>, <code>google</code>, <code>ollama</code>) - <code>--llm-api-key</code>: API key (or set environment variable) - <code>--fail-on-failure</code>: Exit with error code if behaviors fail (default: true) - <code>--output-format</code>: Output format (<code>json</code> or <code>summary</code>, default: <code>json</code>)</p> <p>Features:</p> <ul> <li>\u2705 Non-interactive execution</li> <li>\u2705 Proper exit codes for automation</li> <li>\u2705 Summary output format</li> <li>\u2705 Fail-fast behavior</li> <li>\u2705 Optimized for CI/CD pipelines</li> </ul> <p>Example for GitHub Actions: <pre><code>codeoptix ci \\\n  --agent codex \\\n  --behaviors insecure-code \\\n  --llm-provider openai \\\n  --config examples/configs/ci-cd.yaml \\\n  --fail-on-failure \\\n  --output-format summary\n</code></pre></p> <p>Output Formats:</p> <p>JSON (default): <pre><code>{\n  \"run_id\": \"run-001\",\n  \"overall_score\": 0.85,\n  \"behaviors\": {\n    \"insecure-code\": {\n      \"passed\": true,\n      \"score\": 0.85,\n      \"evidence\": []\n    }\n  }\n}\n</code></pre></p> <p>Summary: <pre><code>\ud83d\udcca Evaluation Summary\nOverall Score: 85.00%\nRun ID: run-001\n\n   \u2705 **insecure-code**: 85.00%\n</code></pre></p>"},{"location":"guides/cli-usage/#codeoptix-reflect-generate-reflection-report","title":"<code>codeoptix reflect</code> - Generate Reflection Report","text":"<p>Generate a human-readable reflection report from evaluation results.</p> <p>Basic Usage: <pre><code>codeoptix reflect --input results.json\n</code></pre></p> <p>Options: - <code>--input</code> (required): Path to results JSON file or run ID - <code>--output</code>: Output file path (default: <code>reflection_{run_id}.md</code>) - <code>--agent-name</code>: Agent name for reflection report</p> <p>Example: <pre><code>codeoptix reflect \\\n  --input results.json \\\n  --output reflection.md \\\n  --agent-name codex\n</code></pre></p>"},{"location":"guides/cli-usage/#codeoptix-evolve-evolve-agent-prompts","title":"<code>codeoptix evolve</code> - Evolve Agent Prompts","text":"<p>Automatically improve agent prompts based on evaluation results.</p> <p>Basic Usage: <pre><code>codeoptix evolve --input results.json\n</code></pre></p> <p>Options: - <code>--input</code> (required): Path to results JSON file or run ID - <code>--reflection</code>: Path to reflection markdown file (auto-generated if not provided) - <code>--output</code>: Output file path (default: <code>evolved_prompts_{run_id}.yaml</code>) - <code>--iterations</code>: Number of evolution iterations (default: 3) - <code>--config</code>: Path to config file (JSON/YAML)</p> <p>Example: <pre><code>codeoptix evolve \\\n  --input results.json \\\n  --reflection reflection.md \\\n  --iterations 5 \\\n  --output evolved.yaml\n</code></pre></p>"},{"location":"guides/cli-usage/#codeoptix-run-full-pipeline","title":"<code>codeoptix run</code> - Full Pipeline","text":"<p>Run the complete pipeline: evaluate \u2192 reflect \u2192 evolve (optional).</p> <p>Basic Usage: <pre><code>codeoptix run \\\n  --agent codex \\\n  --behaviors insecure-code \\\n  --evolve\n</code></pre></p> <p>Options: - <code>--agent</code> (required): Agent type - <code>--behaviors</code> (required): Comma-separated behavior names - <code>--evolve</code>: Run evolution after evaluation - <code>--config</code>: Path to config file</p> <p>Example: <pre><code>codeoptix run \\\n  --agent codex \\\n  --behaviors \"insecure-code,vacuous-tests\" \\\n  --evolve\n</code></pre></p>"},{"location":"guides/cli-usage/#codeoptix-list-runs-list-evaluation-runs","title":"<code>codeoptix list-runs</code> - List Evaluation Runs","text":"<p>List all evaluation runs with their metadata.</p> <p>Usage: <pre><code>codeoptix list-runs\n</code></pre></p> <p>Output: <pre><code>Found 3 evaluation run(s):\n\nRun ID: abc123\n  Timestamp: 2025-01-20T10:00:00Z\n  Score: 0.75/1.0\n  Behaviors: insecure-code, vacuous-tests\n\nRun ID: def456\n  Timestamp: 2025-01-20T11:00:00Z\n  Score: 0.85/1.0\n  Behaviors: insecure-code\n</code></pre></p>"},{"location":"guides/cli-usage/#codeoptix-lint-static-analysis-no-api-key-required","title":"<code>codeoptix lint</code> - Static Analysis (No API Key Required)","text":"<p>Run static analysis linters on your code without requiring an API key.</p> <p>Basic Mode - No API Key Required</p> <p>The <code>lint</code> command works without an API key and uses basic static linters:</p> <ul> <li>\u2705 Security checks (Bandit, Safety, pip-audit)</li> <li>\u2705 Code quality (Ruff, Pylint, Flake8)</li> <li>\u2705 Type checking (mypy)</li> <li>\u2705 Test coverage (coverage.py)</li> <li>\u2705 Accessibility (HTML analyzer)</li> </ul> <p>However, to unlock the full power of CodeOptiX (behavioral evaluation, agent optimization, multi-LLM critique), you need an API key. See Installation Guide.</p> <p>Basic Usage: <pre><code># Auto-detect language and linters\ncodeoptix lint --path ./src\n\n# List available linters\ncodeoptix lint --list-linters\n\n# Run specific linters\ncodeoptix lint --path ./src --linters ruff,bandit,mypy\n</code></pre></p> <p>Options: - <code>--path</code> (required): Path to code (file or directory) - <code>--linters</code>: Comma-separated linter names (default: auto-detect) - <code>--output</code>: Output format (<code>json</code> or <code>summary</code>, default: <code>summary</code>) - <code>--fail-on-issues</code>: Exit with non-zero code if issues found - <code>--no-auto-detect</code>: Disable auto-detection of language and linters - <code>--list-linters</code>: List all available linters and exit</p> <p>Example: <pre><code># List available linters\ncodeoptix lint --list-linters\n\n# Run on Python code\ncodeoptix lint --path ./src --linters ruff,bandit\n\n# Check git changes\ncodeoptix check --base main --head feature\n</code></pre></p> <p>Output: <pre><code>\ud83d\udd0d CodeOptiX Linter Check\n============================================================\n\ud83d\udcc1 Path: ./src\n\ud83d\udd27 Linters: ruff, bandit\n\ud83d\ude80 Running linters...\n\n============================================================\n\ud83d\udcca Linter Results Summary\n============================================================\nTotal Issues: 15\n  Critical: 2\n  High: 5\n  Medium: 8\n  Low: 0\n\nExecution Time: 2.34s\n\nruff: 8 issue(s)\nbandit: 7 issue(s)\n</code></pre></p> <p>Upgrade to Full Features</p> <p>The <code>lint</code> command provides basic static analysis only.</p> <p>To unlock CodeOptiX's full capabilities, set an API key and use: - <code>codeoptix eval</code> - Behavioral evaluation of coding agents - <code>codeoptix reflect</code> - Deep quality analysis and insights - <code>codeoptix evolve</code> - Automatic prompt optimization</p> <p>We strongly recommend setting up an API key to experience CodeOptiX's full power.</p>"},{"location":"guides/cli-usage/#codeoptix-check-check-git-changes","title":"<code>codeoptix check</code> - Check Git Changes","text":"<p>Check code quality on git changes (no API key required).</p> <p>Basic Usage: <pre><code>codeoptix check --base main --head feature\n</code></pre></p> <p>Options: - <code>--base</code>: Base branch/commit (default: <code>main</code>) - <code>--head</code>: Head branch/commit (default: current branch) - <code>--linters</code>: Comma-separated linter names (default: auto-detect) - <code>--output</code>: Output format (<code>json</code> or <code>summary</code>)</p> <p>Example: <pre><code># Check changes in current branch\ncodeoptix check\n\n# Check specific branches\ncodeoptix check --base main --head feature-branch\n</code></pre></p>"},{"location":"guides/cli-usage/#acp-agent-client-protocol-commands","title":"ACP (Agent Client Protocol) Commands","text":"<p>CodeOptiX provides ACP integration for editor support and multi-agent workflows.</p>"},{"location":"guides/cli-usage/#codeoptix-acp-register-register-as-acp-agent","title":"<code>codeoptix acp register</code> - Register as ACP Agent","text":"<p>Register CodeOptiX as an ACP agent for use with editors (Zed, JetBrains, Neovim, VS Code).</p> <p>Usage: <pre><code>codeoptix acp register\n</code></pre></p> <p>This starts CodeOptiX as an ACP agent. Connect from your editor using ACP protocol.</p>"},{"location":"guides/cli-usage/#codeoptix-acp-bridge-quality-bridge","title":"<code>codeoptix acp bridge</code> - Quality Bridge","text":"<p>Use CodeOptiX as a quality bridge between editor and agents via ACP.</p> <p>Basic Usage: <pre><code># Using agent from registry\ncodeoptix acp bridge --agent-name claude-code --auto-eval\n\n# Using direct command\ncodeoptix acp bridge --agent-command \"python agent.py\" --auto-eval\n</code></pre></p> <p>Options: - <code>--agent-command</code>: Command to spawn ACP agent (e.g., <code>\"python agent.py\"</code>) - <code>--agent-name</code>: Name of agent in registry (alternative to <code>--agent-command</code>) - <code>--auto-eval/--no-auto-eval</code>: Automatically evaluate code quality (default: <code>true</code>) - <code>--behaviors</code>: Comma-separated behavior names to evaluate - <code>--cwd</code>: Working directory for the agent</p> <p>Example: <pre><code>codeoptix acp bridge \\\n  --agent-name claude-code \\\n  --auto-eval \\\n  --behaviors \"insecure-code,vacuous-tests\"\n</code></pre></p>"},{"location":"guides/cli-usage/#codeoptix-acp-connect-connect-to-agent","title":"<code>codeoptix acp connect</code> - Connect to Agent","text":"<p>Connect to an ACP agent and send a prompt.</p> <p>Usage: <pre><code>codeoptix acp connect \\\n  --agent-name claude-code \\\n  --prompt \"Write a secure login function\"\n</code></pre></p> <p>Options: - <code>--agent-command</code>: Command to spawn ACP agent - <code>--agent-name</code>: Name of agent in registry - <code>--prompt</code> (required): Prompt to send to agent - <code>--cwd</code>: Working directory</p>"},{"location":"guides/cli-usage/#codeoptix-acp-judge-multi-agent-judge","title":"<code>codeoptix acp judge</code> - Multi-Agent Judge","text":"<p>Use different agents for generation vs. judgment.</p> <p>Usage: <pre><code>codeoptix acp judge \\\n  --generate-agent claude-code \\\n  --judge-agent grok \\\n  --prompt \"Write a secure API endpoint\"\n</code></pre></p> <p>Options: - <code>--generate-agent</code> (required): Agent name for code generation - <code>--judge-agent</code> (required): Agent name for code judgment - <code>--prompt</code> (required): Prompt for code generation</p> <p>This will: 1. Generate code with the generate agent 2. Judge/critique the code with the judge agent 3. Evaluate both with CodeOptiX's evaluation engine</p>"},{"location":"guides/cli-usage/#codeoptix-acp-registry-agent-registry-management","title":"<code>codeoptix acp registry</code> - Agent Registry Management","text":"<p>Manage ACP-compatible agents.</p>"},{"location":"guides/cli-usage/#codeoptix-acp-registry-list-list-agents","title":"<code>codeoptix acp registry list</code> - List Agents","text":"<p>List all registered ACP agents.</p> <p>Usage: <pre><code>codeoptix acp registry list\n</code></pre></p>"},{"location":"guides/cli-usage/#codeoptix-acp-registry-add-register-agent","title":"<code>codeoptix acp registry add</code> - Register Agent","text":"<p>Register a new ACP agent.</p> <p>Usage: <pre><code>codeoptix acp registry add \\\n  --name claude-code \\\n  --command \"python claude_agent.py\" \\\n  --description \"Claude Code via ACP\" \\\n  --cwd /path/to/agent\n</code></pre></p> <p>Options: - <code>--name</code> (required): Agent name - <code>--command</code> (required): Command to spawn agent - <code>--cwd</code>: Working directory - <code>--description</code>: Agent description</p>"},{"location":"guides/cli-usage/#codeoptix-acp-registry-remove-unregister-agent","title":"<code>codeoptix acp registry remove</code> - Unregister Agent","text":"<p>Unregister an ACP agent.</p> <p>Usage: <pre><code>codeoptix acp registry remove --name claude-code\n</code></pre></p> <p>Options: - <code>--name</code> (required): Agent name to remove</p>"},{"location":"guides/cli-usage/#acp-workflows","title":"ACP Workflows","text":""},{"location":"guides/cli-usage/#editor-integration","title":"Editor Integration","text":"<ol> <li> <p>Start CodeOptiX as agent: <pre><code>codeoptix acp register\n</code></pre></p> </li> <li> <p>Connect from editor (Zed, JetBrains, Neovim, VS Code) using ACP protocol</p> </li> <li> <p>CodeOptiX automatically evaluates code quality in real-time</p> </li> </ol>"},{"location":"guides/cli-usage/#quality-bridge","title":"Quality Bridge","text":"<ol> <li> <p>Register agents: <pre><code>codeoptix acp registry add --name claude-code --command \"python agent.py\"\n</code></pre></p> </li> <li> <p>Start quality bridge: <pre><code>codeoptix acp bridge --agent-name claude-code --auto-eval\n</code></pre></p> </li> <li> <p>Use from editor - CodeOptiX automatically evaluates all agent interactions</p> </li> </ol>"},{"location":"guides/cli-usage/#multi-agent-judge","title":"Multi-Agent Judge","text":"<pre><code># Register agents\ncodeoptix acp registry add --name claude-code --command \"python claude.py\"\ncodeoptix acp registry add --name grok --command \"python grok.py\"\n\n# Use multi-agent judge\ncodeoptix acp judge \\\n  --generate-agent claude-code \\\n  --judge-agent grok \\\n  --prompt \"Write secure code\"\n</code></pre>"},{"location":"guides/cli-usage/#configuration-files","title":"Configuration Files","text":"<p>You can use configuration files instead of command-line options.</p>"},{"location":"guides/cli-usage/#yaml-configuration","title":"YAML Configuration","text":"<p>Create <code>codeoptix.yaml</code>:</p> <pre><code>adapter:\n  llm_config:\n    provider: openai\n    model: gpt-5.2\n    api_key: ${OPENAI_API_KEY}\n\nevaluation:\n  scenario_generator:\n    num_scenarios: 3\n    use_bloom: true\n  static_analysis:\n    bandit: true\n\nevolution:\n  max_iterations: 3\n  population_size: 3\n</code></pre> <p>Then use:</p> <pre><code>codeoptix eval --config codeoptix.yaml --agent codex --behaviors insecure-code\n</code></pre>"},{"location":"guides/cli-usage/#environment-variables","title":"Environment Variables","text":"<p>API Key Required for Full Features</p> <p>Without an API key, CodeOptiX runs in basic mode with static linters only (<code>codeoptix lint</code>, <code>codeoptix check</code>).</p> <p>With an API key, you unlock:</p> <ul> <li>\u2705 Behavioral evaluation (<code>codeoptix eval</code>)</li> <li>\u2705 Multi-LLM code critique</li> <li>\u2705 Automatic prompt optimization (<code>codeoptix evolve</code>)</li> <li>\u2705 Deep quality analysis (<code>codeoptix reflect</code>)</li> <li>\u2705 Agent orchestration and multi-agent workflows</li> </ul> <p>We strongly recommend setting up an API key to get the full CodeOptiX experience.</p> <p>Set API keys as environment variables:</p> <pre><code>export OPENAI_API_KEY=\"sk-...\"\nexport ANTHROPIC_API_KEY=\"sk-ant-...\"\nexport GOOGLE_API_KEY=\"...\"\n</code></pre>"},{"location":"guides/cli-usage/#common-workflows","title":"Common Workflows","text":""},{"location":"guides/cli-usage/#quick-evaluation","title":"Quick Evaluation","text":"<pre><code>codeoptix eval \\\n  --agent codex \\\n  --behaviors insecure-code \\\n  --llm-provider openai\n</code></pre>"},{"location":"guides/cli-usage/#full-pipeline","title":"Full Pipeline","text":"<pre><code>codeoptix run \\\n  --agent codex \\\n  --behaviors \"insecure-code,vacuous-tests\" \\\n  --evolve\n</code></pre>"},{"location":"guides/cli-usage/#with-context","title":"With Context","text":"<pre><code>codeoptix eval \\\n  --agent codex \\\n  --behaviors \"plan-drift\" \\\n  --context plan.json\n</code></pre>"},{"location":"guides/cli-usage/#tips","title":"Tips","text":""},{"location":"guides/cli-usage/#use-fail-on-failure-in-cicd","title":"Use <code>--fail-on-failure</code> in CI/CD","text":"<pre><code>codeoptix eval \\\n  --agent codex \\\n  --behaviors insecure-code \\\n  --fail-on-failure\n</code></pre>"},{"location":"guides/cli-usage/#save-results","title":"Save Results","text":"<p>Always specify output file:</p> <pre><code>codeoptix eval \\\n  --agent codex \\\n  --behaviors insecure-code \\\n  --output results.json\n</code></pre>"},{"location":"guides/cli-usage/#use-run-ids","title":"Use Run IDs","text":"<p>You can use run IDs instead of file paths:</p> <pre><code>codeoptix reflect --input abc123\ncodeoptix evolve --input abc123\n</code></pre>"},{"location":"guides/cli-usage/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/cli-usage/#api-key-not-found","title":"API Key Not Found","text":"<p>Make sure environment variable is set:</p> <pre><code>echo $OPENAI_API_KEY\n</code></pre>"},{"location":"guides/cli-usage/#agent-not-found","title":"Agent Not Found","text":"<p>Check agent type is correct: - <code>codex</code> - <code>claude-code</code> - <code>gemini-cli</code></p>"},{"location":"guides/cli-usage/#behavior-not-found","title":"Behavior Not Found","text":"<p>Use one of the built-in behaviors:</p> <ul> <li> <p><code>insecure-code</code></p> </li> <li> <p><code>vacuous-tests</code></p> </li> <li> <p><code>plan-drift</code></p> </li> </ul>"},{"location":"guides/cli-usage/#api-key-issues","title":"API Key Issues","text":""},{"location":"guides/cli-usage/#openai-api-key-not-working","title":"OpenAI API Key Not Working","text":"<pre><code># Check if environment variable is set\necho $OPENAI_API_KEY\n\n# Make sure it starts with 'sk-'\n# For latest models, use gpt-5.2 instead of older GPT models\ncodeoptix eval --agent codex --behaviors insecure-code --llm-provider openai\n</code></pre>"},{"location":"guides/cli-usage/#anthropic-api-key-issues","title":"Anthropic API Key Issues","text":"<pre><code># Check Anthropic key\necho $ANTHROPIC_API_KEY\n\n# Make sure it starts with 'sk-ant-api03-'\ncodeoptix eval --agent claude-code --behaviors insecure-code --llm-provider anthropic\n</code></pre>"},{"location":"guides/cli-usage/#ollama-connection-issues","title":"Ollama Connection Issues","text":"<pre><code># Make sure Ollama is running\nollama serve\n\n# Check if models are available\nollama list\n\n# Pull a model if needed\nollama pull llama3.2\n</code></pre>"},{"location":"guides/cli-usage/#agent-specific-issues","title":"Agent-Specific Issues","text":""},{"location":"guides/cli-usage/#claude-code-not-responding","title":"Claude Code Not Responding","text":"<ul> <li>Make sure Claude CLI is installed and authenticated</li> <li>Check if <code>claude</code> command is available in PATH</li> <li>Try running <code>claude --help</code> manually</li> </ul>"},{"location":"guides/cli-usage/#codex-connection-issues","title":"Codex Connection Issues","text":"<ul> <li>Verify OpenAI API key has sufficient credits</li> <li>Check rate limits on OpenAI account</li> <li>Try a different model like <code>gpt-5.2</code></li> </ul>"},{"location":"guides/cli-usage/#gemini-cli-issues","title":"Gemini CLI Issues","text":"<ul> <li>Ensure Google Cloud SDK is installed and authenticated</li> <li>Check if <code>gcloud</code> command works</li> <li>Verify Gemini API is enabled in Google Cloud Console</li> </ul>"},{"location":"guides/cli-usage/#configuration-issues","title":"Configuration Issues","text":""},{"location":"guides/cli-usage/#yaml-configuration-not-loading","title":"YAML Configuration Not Loading","text":"<pre><code># Validate YAML syntax\npython -c \"import yaml; yaml.safe_load(open('codeoptix.yaml'))\"\n\n# Check file permissions\nls -la codeoptix.yaml\n</code></pre>"},{"location":"guides/cli-usage/#invalid-configuration-options","title":"Invalid Configuration Options","text":"<p>Common mistakes: - Using wrong indentation in YAML - Missing required fields like <code>llm_config.api_key</code> - Using unsupported model names</p>"},{"location":"guides/cli-usage/#performance-issues","title":"Performance Issues","text":""},{"location":"guides/cli-usage/#slow-evaluations","title":"Slow Evaluations","text":"<ul> <li>Use Ollama for local evaluation (no API calls)</li> <li>Reduce number of scenarios in config</li> <li>Use <code>ci</code> command for faster, focused evaluation</li> </ul>"},{"location":"guides/cli-usage/#memory-issues","title":"Memory Issues","text":"<ul> <li>Close other applications</li> <li>Use smaller models with Ollama</li> <li>Reduce batch sizes in configuration</li> </ul>"},{"location":"guides/cli-usage/#file-and-directory-issues","title":"File and Directory Issues","text":""},{"location":"guides/cli-usage/#permission-denied","title":"Permission Denied","text":"<pre><code># Check current directory permissions\nls -ld .\n\n# Change to a directory you have write access to\ncd /tmp\ncodeoptix eval --agent codex --behaviors insecure-code\n</code></pre>"},{"location":"guides/cli-usage/#results-files-not-found","title":"Results Files Not Found","text":"<ul> <li>Check the output directory has write permissions</li> <li>Use absolute paths for output files</li> <li>Run <code>codeoptix list-runs</code> to see existing runs</li> </ul>"},{"location":"guides/cli-usage/#next-steps","title":"Next Steps","text":"<ul> <li>Python API Guide - Use CodeOptiX in Python</li> <li>ACP Integration Guide - Complete ACP documentation</li> <li>Configuration Guide - Advanced configuration</li> <li>GitHub Actions Guide - CI/CD integration</li> </ul>"},{"location":"guides/configuration/","title":"Configuration Guide","text":"<p>Complete guide to configuring CodeOptiX.</p>"},{"location":"guides/configuration/#configuration-methods","title":"Configuration Methods","text":"<p>CodeOptiX can be configured via: 1. Configuration files (YAML/JSON) 2. Environment variables 3. Python code 4. Command-line arguments</p>"},{"location":"guides/configuration/#configuration-file-format","title":"Configuration File Format","text":""},{"location":"guides/configuration/#yaml-configuration","title":"YAML Configuration","text":"<p>Create <code>codeoptix.yaml</code>:</p> <pre><code>adapter:\n  llm_config:\n    provider: openai\n    model: gpt-5.2\n    api_key: ${OPENAI_API_KEY}\n\nevaluation:\n  scenario_generator:\n    num_scenarios: 3\n    use_bloom: true\n    use_full_bloom: true\n  static_analysis:\n    bandit: true\n  test_runner:\n    coverage: true\n\nevolution:\n  max_iterations: 3\n  population_size: 3\n  minibatch_size: 2\n  proposer:\n    use_gepa: true\n    model: gpt-5.2\n</code></pre>"},{"location":"guides/configuration/#json-configuration","title":"JSON Configuration","text":"<p>Create <code>codeoptix.json</code>:</p> <pre><code>{\n  \"adapter\": {\n    \"llm_config\": {\n      \"provider\": \"openai\",\n      \"model\": \"gpt-5.2\",\n      \"api_key\": \"${OPENAI_API_KEY}\"\n    }\n  },\n  \"evaluation\": {\n    \"scenario_generator\": {\n      \"num_scenarios\": 3,\n      \"use_bloom\": true\n    }\n  }\n}\n</code></pre>"},{"location":"guides/configuration/#adapter-configuration","title":"Adapter Configuration","text":""},{"location":"guides/configuration/#llm-configuration","title":"LLM Configuration","text":"<pre><code>adapter:\n  llm_config:\n    provider: openai  # or \"anthropic\", \"google\"\n    model: gpt-5.2\n    api_key: ${OPENAI_API_KEY}\n  prompt: \"You are a helpful coding assistant.\"\n</code></pre>"},{"location":"guides/configuration/#supported-providers","title":"Supported Providers","text":"<ul> <li><code>openai</code>: OpenAI GPT models</li> <li><code>anthropic</code>: Anthropic Claude models</li> <li><code>google</code>: Google Gemini models</li> </ul>"},{"location":"guides/configuration/#evaluation-configuration","title":"Evaluation Configuration","text":""},{"location":"guides/configuration/#scenario-generator","title":"Scenario Generator","text":"<pre><code>evaluation:\n  scenario_generator:\n    num_scenarios: 3        # Number of scenarios per behavior\n    use_bloom: true         # Use Bloom-style generation\n    use_full_bloom: true    # Full Bloom integration\n    num_variations: 2       # Variations per scenario\n    model: gpt-5.2          # LLM model for generation\n</code></pre>"},{"location":"guides/configuration/#static-analysis","title":"Static Analysis","text":"<pre><code>evaluation:\n  static_analysis:\n    bandit: true  # Enable Bandit security checks\n</code></pre>"},{"location":"guides/configuration/#test-runner","title":"Test Runner","text":"<pre><code>evaluation:\n  test_runner:\n    coverage: true  # Enable coverage analysis\n</code></pre>"},{"location":"guides/configuration/#llm-evaluator","title":"LLM Evaluator","text":"<pre><code>evaluation:\n  llm_evaluator:\n    model: gpt-5.2\n    temperature: 0.3\n</code></pre>"},{"location":"guides/configuration/#evolution-configuration","title":"Evolution Configuration","text":""},{"location":"guides/configuration/#evolution-parameters","title":"Evolution Parameters","text":"<pre><code>evolution:\n  max_iterations: 3           # Maximum iterations\n  population_size: 3          # Candidates per iteration\n  minibatch_size: 2           # Scenarios per evaluation\n  improvement_threshold: 0.05 # Minimum improvement\n</code></pre>"},{"location":"guides/configuration/#proposer-configuration","title":"Proposer Configuration","text":"<pre><code>evolution:\n  proposer:\n    use_gepa: true      # Use GEPA for proposal\n    model: gpt-5.2       # LLM model\n    temperature: 0.7    # Generation temperature\n</code></pre>"},{"location":"guides/configuration/#behavior-configuration","title":"Behavior Configuration","text":""},{"location":"guides/configuration/#behavior-specific-config","title":"Behavior-Specific Config","text":"<pre><code>behaviors:\n  insecure-code:\n    severity: high\n    enabled: true\n    strict_mode: true\n  vacuous-tests:\n    severity: medium\n    enabled: true\n</code></pre>"},{"location":"guides/configuration/#environment-variables","title":"Environment Variables","text":""},{"location":"guides/configuration/#api-keys","title":"API Keys","text":"<pre><code>export OPENAI_API_KEY=\"sk-...\"\nexport ANTHROPIC_API_KEY=\"sk-ant-...\"\nexport GOOGLE_API_KEY=\"...\"\n</code></pre>"},{"location":"guides/configuration/#configuration-override","title":"Configuration Override","text":"<pre><code>export CODEOPTIX_CONFIG=\"path/to/config.yaml\"\n</code></pre>"},{"location":"guides/configuration/#python-configuration","title":"Python Configuration","text":""},{"location":"guides/configuration/#programmatic-configuration","title":"Programmatic Configuration","text":"<pre><code>config = {\n    \"evaluation\": {\n        \"scenario_generator\": {\n            \"num_scenarios\": 5\n        }\n    }\n}\n\nengine = EvaluationEngine(adapter, llm_client, config=config)\n</code></pre>"},{"location":"guides/configuration/#configuration-precedence","title":"Configuration Precedence","text":"<ol> <li>Command-line arguments (highest priority)</li> <li>Configuration file</li> <li>Environment variables</li> <li>Default values (lowest priority)</li> </ol>"},{"location":"guides/configuration/#best-practices","title":"Best Practices","text":""},{"location":"guides/configuration/#1-use-configuration-files","title":"1. Use Configuration Files","text":"<p>Store configuration in files for reproducibility:</p> <pre><code># codeoptix.yaml\nevaluation:\n  scenario_generator:\n    num_scenarios: 3\n</code></pre>"},{"location":"guides/configuration/#2-use-environment-variables-for-secrets","title":"2. Use Environment Variables for Secrets","text":"<p>Never commit API keys:</p> <pre><code>adapter:\n  llm_config:\n    api_key: ${OPENAI_API_KEY}  # Use env var\n</code></pre>"},{"location":"guides/configuration/#3-version-control-configurations","title":"3. Version Control Configurations","text":"<p>Commit configuration files (without secrets):</p> <pre><code>git add codeoptix.yaml\ngit commit -m \"Add CodeOptiX configuration\"\n</code></pre>"},{"location":"guides/configuration/#next-steps","title":"Next Steps","text":"<ul> <li>Python API Guide - Use configuration in Python</li> <li>CLI Usage Guide - Command-line configuration</li> <li>GitHub Actions Guide - CI/CD configuration</li> </ul>"},{"location":"guides/custom-behaviors/","title":"Custom Behaviors Guide","text":"<p>Learn how to create your own behavior specifications.</p>"},{"location":"guides/custom-behaviors/#overview","title":"Overview","text":"<p>Behavior specifications are modular, reusable rules that evaluate agent output. You can create custom behaviors for your specific needs.</p>"},{"location":"guides/custom-behaviors/#creating-a-custom-behavior","title":"Creating a Custom Behavior","text":""},{"location":"guides/custom-behaviors/#step-1-extend-behaviorspec","title":"Step 1: Extend BehaviorSpec","text":"<pre><code>from codeoptix.behaviors.base import BehaviorSpec, BehaviorResult, Severity\nfrom codeoptix.adapters.base import AgentOutput\n\nclass MyCustomBehavior(BehaviorSpec):\n    def get_name(self) -&gt; str:\n        return \"my-custom-behavior\"\n\n    def get_description(self) -&gt; str:\n        return \"Checks for specific patterns in code\"\n\n    def evaluate(self, agent_output: AgentOutput, context=None):\n        code = agent_output.code or \"\"\n        evidence = []\n        score = 1.0\n\n        # Your evaluation logic here\n        if \"bad_pattern\" in code:\n            evidence.append(\"Found bad pattern in code\")\n            score = 0.5\n\n        return BehaviorResult(\n            behavior_name=self.get_name(),\n            passed=score &gt;= 0.7,\n            score=score,\n            evidence=evidence,\n            severity=Severity.MEDIUM\n        )\n</code></pre>"},{"location":"guides/custom-behaviors/#step-2-register-behavior","title":"Step 2: Register Behavior","text":"<pre><code>from codeoptix.behaviors import create_behavior\n\n# Register your behavior\n# (Implementation depends on your setup)\n\nbehavior = create_behavior(\"my-custom-behavior\")\n</code></pre>"},{"location":"guides/custom-behaviors/#example-code-style-behavior","title":"Example: Code Style Behavior","text":"<p>Check for code style issues:</p> <pre><code>class CodeStyleBehavior(BehaviorSpec):\n    def get_name(self) -&gt; str:\n        return \"code-style\"\n\n    def get_description(self) -&gt; str:\n        return \"Checks code style and formatting\"\n\n    def evaluate(self, agent_output, context=None):\n        code = agent_output.code or \"\"\n        evidence = []\n        score = 1.0\n\n        # Check for long lines\n        for i, line in enumerate(code.split('\\n'), 1):\n            if len(line) &gt; 100:\n                evidence.append(f\"Line {i} exceeds 100 characters\")\n                score -= 0.1\n\n        # Check for missing docstrings\n        if 'def ' in code and '\"\"\"' not in code:\n            evidence.append(\"Missing docstrings\")\n            score -= 0.2\n\n        score = max(0.0, score)  # Ensure non-negative\n\n        return BehaviorResult(\n            behavior_name=self.get_name(),\n            passed=score &gt;= 0.7,\n            score=score,\n            evidence=evidence,\n            severity=Severity.LOW\n        )\n</code></pre>"},{"location":"guides/custom-behaviors/#example-performance-behavior","title":"Example: Performance Behavior","text":"<p>Check for performance issues:</p> <pre><code>class PerformanceBehavior(BehaviorSpec):\n    def get_name(self) -&gt; str:\n        return \"performance\"\n\n    def get_description(self) -&gt; str:\n        return \"Checks for performance issues\"\n\n    def evaluate(self, agent_output, context=None):\n        code = agent_output.code or \"\"\n        evidence = []\n        score = 1.0\n\n        # Check for inefficient patterns\n        if \"for i in range(len(\" in code:\n            evidence.append(\"Inefficient range(len()) pattern\")\n            score -= 0.3\n\n        if \".append(\" in code and \"list comprehension\" not in code.lower():\n            # Check if list comprehension could be used\n            evidence.append(\"Consider using list comprehension\")\n            score -= 0.1\n\n        score = max(0.0, score)\n\n        return BehaviorResult(\n            behavior_name=self.get_name(),\n            passed=score &gt;= 0.7,\n            score=score,\n            evidence=evidence,\n            severity=Severity.MEDIUM\n        )\n</code></pre>"},{"location":"guides/custom-behaviors/#using-llm-evaluation","title":"Using LLM Evaluation","text":"<p>You can use LLM for semantic evaluation:</p> <pre><code>class SemanticBehavior(BehaviorSpec):\n    def __init__(self, config=None):\n        super().__init__(config)\n        self.llm_client = config.get(\"llm_client\")\n\n    def evaluate(self, agent_output, context=None):\n        if not self.llm_client:\n            return BehaviorResult(\n                behavior_name=self.get_name(),\n                passed=False,\n                score=0.0,\n                evidence=[\"LLM client not configured\"]\n            )\n\n        # Use LLM to evaluate\n        prompt = f\"Evaluate this code: {agent_output.code}\"\n        response = self.llm_client.chat_completion(\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            model=\"gpt-5.2\"\n        )\n\n        # Parse response and create result\n        # ...\n</code></pre>"},{"location":"guides/custom-behaviors/#configuration","title":"Configuration","text":"<p>Custom behaviors can accept configuration:</p> <pre><code>class ConfigurableBehavior(BehaviorSpec):\n    def __init__(self, config=None):\n        super().__init__(config)\n        self.threshold = config.get(\"threshold\", 0.7)\n        self.strict_mode = config.get(\"strict_mode\", False)\n\n    def evaluate(self, agent_output, context=None):\n        # Use configuration\n        score = self._calculate_score(agent_output)\n        passed = score &gt;= self.threshold\n\n        return BehaviorResult(\n            behavior_name=self.get_name(),\n            passed=passed,\n            score=score,\n            evidence=[],\n            severity=Severity.MEDIUM\n        )\n</code></pre>"},{"location":"guides/custom-behaviors/#best-practices","title":"Best Practices","text":""},{"location":"guides/custom-behaviors/#1-clear-evidence","title":"1. Clear Evidence","text":"<p>Provide specific, actionable evidence:</p> <pre><code>evidence.append(f\"Hardcoded password found at line {line_num}: {match.group(0)}\")\n</code></pre>"},{"location":"guides/custom-behaviors/#2-appropriate-scoring","title":"2. Appropriate Scoring","text":"<p>Use consistent scoring: - 1.0 = Perfect - 0.7-0.9 = Good - 0.5-0.7 = Fair - 0.0-0.5 = Poor</p>"},{"location":"guides/custom-behaviors/#3-handle-edge-cases","title":"3. Handle Edge Cases","text":"<p>Always handle missing or empty code:</p> <pre><code>code = agent_output.code or \"\"\nif not code:\n    return BehaviorResult(\n        behavior_name=self.get_name(),\n        passed=False,\n        score=0.0,\n        evidence=[\"No code provided\"]\n    )\n</code></pre>"},{"location":"guides/custom-behaviors/#testing-custom-behaviors","title":"Testing Custom Behaviors","text":"<p>Test your custom behavior:</p> <pre><code>def test_my_behavior():\n    behavior = MyCustomBehavior()\n    output = AgentOutput(code=\"bad_pattern here\")\n    result = behavior.evaluate(output)\n\n    assert not result.passed\n    assert len(result.evidence) &gt; 0\n</code></pre>"},{"location":"guides/custom-behaviors/#next-steps","title":"Next Steps","text":"<ul> <li>Behavior Specifications - Learn about behaviors</li> <li>Evaluation Engine - Use behaviors in evaluation</li> <li>Python API Guide - Advanced usage</li> </ul>"},{"location":"guides/github-actions/","title":"GitHub Actions Integration","text":"<p>Complete guide to using CodeOptiX in GitHub Actions CI/CD pipelines.</p>"},{"location":"guides/github-actions/#quick-start","title":"Quick Start","text":""},{"location":"guides/github-actions/#1-add-workflow-file","title":"1. Add Workflow File","text":"<p>Create <code>.github/workflows/codeoptix.yml</code>:</p> <pre><code>name: CodeOptiX Check\n\non:\n  pull_request:\n    branches: [ main ]\n\njobs:\n  check:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - uses: actions/setup-python@v5\n        with:\n          python-version: '3.12'\n\n      - uses: astral-sh/setup-uv@v4\n\n      - run: uv sync --dev\n\n      - name: Run CodeOptiX\n        env:\n          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}\n        run: |\n          codeoptix eval \\\n            --agent codex \\\n            --behaviors insecure-code \\\n            --llm-provider openai \\\n            --fail-on-failure\n</code></pre>"},{"location":"guides/github-actions/#2-configure-secrets","title":"2. Configure Secrets","text":"<p>Add your API key as a GitHub Secret: 1. Go to repository Settings \u2192 Secrets and variables \u2192 Actions 2. Add <code>OPENAI_API_KEY</code> secret 3. The workflow will use it automatically</p>"},{"location":"guides/github-actions/#basic-workflow","title":"Basic Workflow","text":""},{"location":"guides/github-actions/#simple-check","title":"Simple Check","text":"<pre><code>- name: Run CodeOptiX\n  env:\n    OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}\n  run: |\n    codeoptix eval \\\n      --agent codex \\\n      --behaviors insecure-code \\\n      --llm-provider openai \\\n      --fail-on-failure\n</code></pre>"},{"location":"guides/github-actions/#full-pipeline","title":"Full Pipeline","text":""},{"location":"guides/github-actions/#complete-workflow","title":"Complete Workflow","text":"<pre><code>name: CodeOptiX Full Pipeline\n\non:\n  pull_request:\n    branches: [ main ]\n\njobs:\n  codeoptix:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - uses: actions/setup-python@v5\n        with:\n          python-version: '3.12'\n\n      - uses: astral-sh/setup-uv@v4\n\n      - run: uv sync --dev\n\n      - name: Run Evaluation\n        env:\n          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}\n        run: |\n          codeoptix eval \\\n            --agent codex \\\n            --behaviors insecure-code,vacuous-tests \\\n            --llm-provider openai \\\n            --output .codeoptix/results.json\n\n      - name: Generate Reflection\n        env:\n          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}\n        run: |\n          codeoptix reflect \\\n            --input .codeoptix/results.json \\\n            --output .codeoptix/reflection.md\n\n      - name: Upload Artifacts\n        uses: actions/upload-artifact@v4\n        with:\n          name: codeoptix-results\n          path: .codeoptix/\n</code></pre>"},{"location":"guides/github-actions/#pr-comments","title":"PR Comments","text":""},{"location":"guides/github-actions/#post-results-as-pr-comment","title":"Post Results as PR Comment","text":"<pre><code>- name: Comment PR\n  if: github.event_name == 'pull_request'\n  uses: actions/github-script@v7\n  with:\n    github-token: ${{ secrets.GITHUB_TOKEN }}\n    script: |\n      const fs = require('fs');\n      const results = JSON.parse(fs.readFileSync('.codeoptix/results.json', 'utf8'));\n\n      let comment = '## CodeOptiX Results\\n\\n';\n      for (const [name, data] of Object.entries(results.behaviors || {})) {\n        comment += `- ${data.passed ? '\u2705' : '\u274c'} ${name}\\n`;\n      }\n\n      github.rest.issues.createComment({\n        issue_number: context.issue.number,\n        owner: context.repo.owner,\n        repo: context.repo.repo,\n        body: comment\n      });\n</code></pre>"},{"location":"guides/github-actions/#best-practices","title":"Best Practices","text":""},{"location":"guides/github-actions/#1-use-secrets","title":"1. Use Secrets","text":"<p>Never hardcode API keys:</p> <pre><code># \u2705 Good\nenv:\n  OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}\n\n# \u274c Bad\nenv:\n  OPENAI_API_KEY: sk-...\n</code></pre>"},{"location":"guides/github-actions/#2-fail-on-critical-behaviors","title":"2. Fail on Critical Behaviors","text":"<p>Use <code>--fail-on-failure</code> for security checks:</p> <pre><code>codeoptix eval \\\n  --behaviors insecure-code \\\n  --fail-on-failure\n</code></pre>"},{"location":"guides/github-actions/#3-upload-artifacts","title":"3. Upload Artifacts","text":"<p>Always upload results for debugging:</p> <pre><code>- uses: actions/upload-artifact@v4\n  with:\n    name: codeoptix-results\n    path: .codeoptix/\n</code></pre>"},{"location":"guides/github-actions/#advanced-examples","title":"Advanced Examples","text":""},{"location":"guides/github-actions/#matrix-strategy","title":"Matrix Strategy","text":"<p>Test with multiple agents:</p> <pre><code>strategy:\n  matrix:\n    agent: [codex, claude-code, gemini-cli]\nsteps:\n  - name: Run CodeOptiX\n    run: |\n      codeoptix eval --agent ${{ matrix.agent }}\n</code></pre>"},{"location":"guides/github-actions/#conditional-execution","title":"Conditional Execution","text":"<p>Only run on Python files:</p> <pre><code>- name: Check if Python files changed\n  id: changed-files\n  uses: tj-actions/changed-files@v40\n  with:\n    files: |\n      **/*.py\n\n- name: Run CodeOptiX\n  if: steps.changed-files.outputs.any_changed == 'true'\n  run: |\n    codeoptix eval --behaviors insecure-code\n</code></pre>"},{"location":"guides/github-actions/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/github-actions/#api-key-issues","title":"API Key Issues","text":"<p>Check secrets are set correctly in repository settings.</p>"},{"location":"guides/github-actions/#timeout-issues","title":"Timeout Issues","text":"<p>Reduce scenarios or use faster models:</p> <pre><code>codeoptix eval \\\n  --behaviors insecure-code \\\n  --config config.yaml  # With reduced scenarios\n</code></pre>"},{"location":"guides/github-actions/#next-steps","title":"Next Steps","text":"<ul> <li>CLI Usage Guide - All CLI commands</li> <li>Configuration Guide - Advanced configuration</li> <li>Python API Guide - Python integration</li> </ul>"},{"location":"guides/ollama-integration/","title":"Ollama Integration Guide","text":"<p>CodeOptiX supports local Ollama models, allowing you to run evaluations without API keys!</p>"},{"location":"guides/ollama-integration/#why-ollama","title":"\ud83c\udfaf Why Ollama?","text":"<ul> <li>\u2705 No API key required - Perfect for open-source users</li> <li>\u2705 Privacy-friendly - All processing happens locally</li> <li>\u2705 Free to use - No cloud costs</li> <li>\u2705 Works offline - No internet connection needed</li> <li>\u2705 Flexible models - Choose from many open-source models</li> </ul>"},{"location":"guides/ollama-integration/#installation","title":"\ud83d\udce6 Installation","text":""},{"location":"guides/ollama-integration/#step-1-install-ollama","title":"Step 1: Install Ollama","text":"<p>Visit https://ollama.com and install Ollama for your platform:</p> <pre><code># macOS\nbrew install ollama\n\n# Linux\ncurl -fsSL https://ollama.com/install.sh | sh\n\n# Windows\n# Download from https://ollama.com/download\n</code></pre>"},{"location":"guides/ollama-integration/#step-2-start-ollama-service","title":"Step 2: Start Ollama Service","text":"<pre><code>ollama serve\n</code></pre> <p>This starts the Ollama service on <code>http://localhost:11434</code> (default).</p>"},{"location":"guides/ollama-integration/#step-3-pull-a-model","title":"Step 3: Pull a Model","text":"<p>Pull a model you want to use:</p> <pre><code># Recommended models for CodeOptiX\nollama pull llama3.1:8b        # Fast, efficient (4.9 GB)\nollama pull qwen3:8b           # Alternative 8B model (5.2 GB)\nollama pull gpt-oss:120b       # Large, powerful (65 GB)\nollama pull gpt-oss:20b        # Medium model (13 GB)\nollama pull llama3.2:3b        # Lightweight (2.0 GB)\n</code></pre> <p>See available models: <pre><code>ollama list\n</code></pre></p>"},{"location":"guides/ollama-integration/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"guides/ollama-integration/#basic-usage","title":"Basic Usage","text":"<pre><code>codeoptix eval \\\n  --agent basic \\\n  --behaviors insecure-code \\\n  --llm-provider ollama\n</code></pre>"},{"location":"guides/ollama-integration/#with-configuration-file","title":"With Configuration File","text":"<p>Create <code>ollama-config.yaml</code>:</p> <pre><code>adapter:\n  llm_config:\n    provider: ollama\n    model: llama3.1:8b  # Or gpt-oss:120b, qwen3:8b, etc.\n    # No api_key needed!\n\nevaluation:\n  scenario_generator:\n    num_scenarios: 2\n    use_bloom: false\n\nbehaviors:\n  insecure-code:\n    enabled: true\n</code></pre> <p>Run:</p> <pre><code>codeoptix eval \\\n  --agent basic \\\n  --behaviors insecure-code \\\n  --config ollama-config.yaml \\\n  --llm-provider ollama\n</code></pre>"},{"location":"guides/ollama-integration/#configuration","title":"\u2699\ufe0f Configuration","text":""},{"location":"guides/ollama-integration/#in-config-file","title":"In Config File","text":"<pre><code>adapter:\n  llm_config:\n    provider: ollama\n    model: llama3.1:8b  # Choose your model\n    # No api_key needed!\n</code></pre>"},{"location":"guides/ollama-integration/#via-environment-variable","title":"Via Environment Variable","text":"<pre><code>export CODEOPTIX_LLM_PROVIDER=ollama\ncodeoptix eval --agent basic --behaviors insecure-code\n</code></pre>"},{"location":"guides/ollama-integration/#custom-ollama-url","title":"Custom Ollama URL","text":"<p>By default, Ollama runs on <code>http://localhost:11434</code>. To use a different URL:</p> <pre><code>export OLLAMA_BASE_URL=http://localhost:11434\n# Or use a remote Ollama instance\nexport OLLAMA_BASE_URL=http://remote-server:11434\n</code></pre>"},{"location":"guides/ollama-integration/#available-models","title":"\ud83d\udccb Available Models","text":""},{"location":"guides/ollama-integration/#recommended-models","title":"Recommended Models","text":"Model Size Speed Quality Use Case <code>llama3.1:8b</code> 4.9 GB \u26a1\u26a1\u26a1 \u2b50\u2b50\u2b50 Fast, efficient <code>qwen3:8b</code> 5.2 GB \u26a1\u26a1\u26a1 \u2b50\u2b50\u2b50 Alternative 8B <code>gpt-oss:120b</code> 65 GB \u26a1 \u2b50\u2b50\u2b50\u2b50\u2b50 Best quality <code>gpt-oss:20b</code> 13 GB \u26a1\u26a1 \u2b50\u2b50\u2b50\u2b50 Good balance <code>llama3.2:3b</code> 2.0 GB \u26a1\u26a1\u26a1\u26a1 \u2b50\u2b50 Lightweight"},{"location":"guides/ollama-integration/#list-available-models","title":"List Available Models","text":"<pre><code>ollama list\n</code></pre>"},{"location":"guides/ollama-integration/#pull-a-model","title":"Pull a Model","text":"<pre><code>ollama pull &lt;model-name&gt;\n</code></pre>"},{"location":"guides/ollama-integration/#usage-examples","title":"\ud83d\udca1 Usage Examples","text":""},{"location":"guides/ollama-integration/#example-1-basic-evaluation","title":"Example 1: Basic Evaluation","text":"<pre><code>codeoptix eval \\\n  --agent basic \\\n  --behaviors insecure-code \\\n  --llm-provider ollama\n</code></pre>"},{"location":"guides/ollama-integration/#example-2-with-custom-config","title":"Example 2: With Custom Config","text":"<pre><code>codeoptix eval \\\n  --agent basic \\\n  --behaviors insecure-code \\\n  --config examples/configs/ollama-insecure-code.yaml \\\n  --llm-provider ollama\n</code></pre>"},{"location":"guides/ollama-integration/#example-3-multiple-behaviors","title":"Example 3: Multiple Behaviors","text":"<pre><code>codeoptix eval \\\n  --agent basic \\\n  --behaviors insecure-code,vacuous-tests \\\n  --llm-provider ollama\n</code></pre>"},{"location":"guides/ollama-integration/#example-4-verbose-output","title":"Example 4: Verbose Output","text":"<pre><code>codeoptix eval \\\n  --agent basic \\\n  --behaviors insecure-code \\\n  --llm-provider ollama \\\n  --verbose\n</code></pre>"},{"location":"guides/ollama-integration/#example-5-cicd-integration","title":"Example 5: CI/CD Integration","text":"<pre><code># .github/workflows/codeoptix.yml\nname: CodeOptiX Check\non: [pull_request]\njobs:\n  check:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-python@v5\n        with:\n          python-version: '3.12'\n      - name: Install Ollama\n        run: |\n          curl -fsSL https://ollama.com/install.sh | sh\n          ollama pull llama3.1:8b\n      - name: Install CodeOptiX\n        run: pip install codeoptix\n      - name: Run CodeOptiX\n        run: |\n          codeoptix ci \\\n            --agent basic \\\n            --behaviors insecure-code \\\n            --llm-provider ollama \\\n            --fail-on-failure\n</code></pre>"},{"location":"guides/ollama-integration/#troubleshooting","title":"\ud83d\udd27 Troubleshooting","text":""},{"location":"guides/ollama-integration/#ollama-not-running","title":"Ollama Not Running","text":"<p>Error: <code>Failed to contact Ollama at http://localhost:11434</code></p> <p>Solution: <pre><code># Start Ollama service\nollama serve\n</code></pre></p>"},{"location":"guides/ollama-integration/#model-not-found","title":"Model Not Found","text":"<p>Error: Model not available</p> <p>Solution: <pre><code># Pull the model\nollama pull llama3.1:8b\n\n# Verify it's available\nollama list\n</code></pre></p>"},{"location":"guides/ollama-integration/#connection-timeout","title":"Connection Timeout","text":"<p>Error: Timeout when calling Ollama</p> <p>Solution: - Large models (like <code>gpt-oss:120b</code>) may take longer - CodeOptiX uses a 300-second timeout by default - Ensure you have enough RAM for the model - Try a smaller model if timeouts persist</p>"},{"location":"guides/ollama-integration/#custom-port","title":"Custom Port","text":"<p>If Ollama is running on a different port:</p> <pre><code>export OLLAMA_BASE_URL=http://localhost:11435\n</code></pre>"},{"location":"guides/ollama-integration/#ollama-vs-cloud-providers","title":"\ud83c\udd9a Ollama vs Cloud Providers","text":"Feature Ollama Cloud Providers API Key \u274c Not required \u2705 Required Cost \u2705 Free \u26a0\ufe0f Pay per use Privacy \u2705 Local only \u26a0\ufe0f Data sent externally Internet \u2705 Works offline \u274c Requires connection Setup \u26a0\ufe0f Install Ollama \u2705 Just API key Models \u26a0\ufe0f Limited to local \u2705 Many options Speed \u26a0\ufe0f Depends on hardware \u2705 Fast (cloud) <p>Choose Ollama if: - You want privacy - You want to avoid API costs - You have sufficient local compute - You want to work offline</p> <p>Choose Cloud Providers if: - You need the latest models - You don't have local compute - You need maximum speed - You're okay with API costs</p>"},{"location":"guides/ollama-integration/#limitations","title":"\u26a0\ufe0f Limitations","text":"<p>While Ollama works great for evaluations, there are some limitations:</p>"},{"location":"guides/ollama-integration/#evolution-support","title":"Evolution Support","text":"<ul> <li>Limited support for <code>codeoptix evolve</code>: The evolution feature uses GEPA optimization, which requires processing very long prompts. Ollama may fail with 404 errors or timeouts on complex evolution tasks.</li> <li>Recommendation: Use cloud providers (OpenAI, Anthropic, Google) for full evolution capabilities. For basic evolution testing, try smaller models like <code>llama3.1:8b</code> with minimal iterations.</li> </ul>"},{"location":"guides/ollama-integration/#performance","title":"Performance","text":"<ul> <li>Large models (e.g., <code>gpt-oss:120b</code>) require significant RAM and may be slow on consumer hardware.</li> <li>Evolution tasks are computationally intensive and may not complete reliably with Ollama.</li> </ul> <p>For advanced features like evolution, consider cloud providers or contact us for tailored enterprise solutions.</p>"},{"location":"guides/ollama-integration/#next-steps","title":"\ud83d\udcda Next Steps","text":"<ul> <li>Quick Start Guide</li> <li>Configuration Guide</li> <li>Behavior Specifications</li> <li>GEPA Optimization</li> <li>Bloom Evaluations</li> </ul>"},{"location":"guides/ollama-integration/#issues","title":"\ud83d\udc1b Issues?","text":"<p>If you encounter problems:</p> <ol> <li>Check Troubleshooting above</li> <li>Verify Ollama is running: <code>ollama list</code></li> <li>Check model is pulled: <code>ollama list</code></li> <li>Open an issue on GitHub</li> </ol>"},{"location":"guides/performance-testing/","title":"Performance Testing Guide","text":"<p>Planned Feature</p> <p>Performance testing is currently a planned feature, not yet implemented.</p> <p>This guide shows how you can implement performance testing yourself by creating custom behaviors. The dependency (<code>locust</code>) is available as an optional dependency, but you'll need to implement the evaluators yourself.</p>"},{"location":"guides/performance-testing/#installation","title":"Installation","text":"<p>Install performance testing dependencies:</p> <pre><code>pip install codeoptix[performance]\n</code></pre> <p>This installs: - <code>locust</code> - Load testing framework</p>"},{"location":"guides/performance-testing/#use-cases","title":"Use Cases","text":"<p>Performance testing in CodeOptiX would be useful for:</p> <ul> <li>\u2705 Load testing web applications</li> <li>\u2705 Stress testing APIs and endpoints</li> <li>\u2705 Performance benchmarking of agent-generated code</li> <li>\u2705 Identifying bottlenecks in applications</li> <li>\u2705 Validating performance requirements</li> </ul>"},{"location":"guides/performance-testing/#implementation-guide","title":"Implementation Guide","text":"<p>Custom Implementation Required</p> <p>Since performance testing is not yet implemented in CodeOptiX, you'll need to create your own custom behavior and evaluator. Here's how:</p>"},{"location":"guides/performance-testing/#1-create-a-performance-test-behavior","title":"1. Create a Performance Test Behavior","text":"<p>Create a custom behavior that uses performance testing:</p> <pre><code>from codeoptix.behaviors.base import BehaviorSpec\nfrom codeoptix.evaluation.evaluators import LLMEvaluator\n\nclass PerformanceBehavior(BehaviorSpec):\n    \"\"\"Test application performance.\"\"\"\n\n    def get_name(self) -&gt; str:\n        return \"performance\"\n\n    def get_description(self) -&gt; str:\n        return \"Validates application performance under load\"\n\n    def create_evaluator(self):\n        # For now, use LLMEvaluator or create your own\n        # TODO: Implement PerformanceEvaluator\n        return None  # You'll need to implement this\n</code></pre>"},{"location":"guides/performance-testing/#2-implement-performance-test-evaluator-example","title":"2. Implement Performance Test Evaluator (Example)","text":"<p>Example Implementation</p> <p>This is an example of how you could implement performance testing. You'll need to integrate this into CodeOptiX's evaluation system yourself.</p> <pre><code># Example: Custom Performance Test Evaluator\n# This is NOT part of CodeOptiX yet - you need to implement this yourself\n\nfrom locust import HttpUser, task, between\nfrom locust.env import Environment\nimport gevent\nfrom typing import Dict, Any\n\nclass PerformanceEvaluator:\n    \"\"\"Example: Evaluates performance using Locust.\"\"\"\n\n    def evaluate(self, code: str, context: Dict[str, Any] = None) -&gt; Dict[str, Any]:\n        \"\"\"Run performance tests.\"\"\"\n        url = context.get(\"url\", \"http://localhost:8000\")\n        users = context.get(\"users\", 10)\n        spawn_rate = context.get(\"spawn_rate\", 2)\n        run_time = context.get(\"run_time\", \"30s\")\n\n        # Define user behavior\n        class WebsiteUser(HttpUser):\n            wait_time = between(1, 3)\n            host = url\n\n            @task\n            def index(self):\n                self.client.get(\"/\")\n\n            @task(3)\n            def api_endpoint(self):\n                self.client.get(\"/api/data\")\n\n        # Setup Locust environment\n        env = Environment(user_classes=[WebsiteUser])\n        env.create_local_runner()\n\n        # Start test\n        env.runner.start(users, spawn_rate=spawn_rate)\n\n        # Run for specified duration\n        gevent.spawn_later(int(run_time.rstrip('s')), lambda: env.runner.quit())\n\n        # Wait for test to complete\n        env.runner.greenlet.join()\n\n        # Get statistics\n        stats = env.stats\n\n        issues = []\n        score = 1.0\n\n        # Check response times\n        for name, entry in stats.entries.items():\n            if entry.num_requests &gt; 0:\n                avg_response_time = entry.avg_response_time\n                max_response_time = entry.max_response_time\n\n                # Check if response times are acceptable\n                if avg_response_time &gt; 1000:  # 1 second\n                    issues.append(f\"{name}: Average response time too high ({avg_response_time:.0f}ms)\")\n                    score -= 0.2\n\n                if max_response_time &gt; 5000:  # 5 seconds\n                    issues.append(f\"{name}: Max response time too high ({max_response_time:.0f}ms)\")\n                    score -= 0.1\n\n                # Check failure rate\n                failure_rate = entry.num_failures / entry.num_requests if entry.num_requests &gt; 0 else 0\n                if failure_rate &gt; 0.01:  # 1% failure rate\n                    issues.append(f\"{name}: Failure rate too high ({failure_rate:.1%})\")\n                    score -= 0.3\n\n        # Check total requests\n        total_requests = sum(entry.num_requests for entry in stats.entries.values())\n        if total_requests &lt; 100:\n            issues.append(f\"Too few requests completed: {total_requests}\")\n            score -= 0.1\n\n        score = max(0.0, score)\n\n        return {\n            \"passed\": len(issues) == 0,\n            \"score\": score,\n            \"evidence\": issues,\n            \"stats\": {\n                \"total_requests\": total_requests,\n                \"total_failures\": sum(entry.num_failures for entry in stats.entries.values()),\n            }\n        }\n</code></pre>"},{"location":"guides/performance-testing/#configuration","title":"Configuration","text":""},{"location":"guides/performance-testing/#basic-configuration","title":"Basic Configuration","text":"<pre><code>context = {\n    \"url\": \"http://localhost:8000\",\n    \"users\": 10,           # Number of concurrent users\n    \"spawn_rate\": 2,       # Users spawned per second\n    \"run_time\": \"30s\",     # Test duration\n}\n</code></pre>"},{"location":"guides/performance-testing/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code>context = {\n    \"url\": \"http://localhost:8000\",\n    \"users\": 50,           # Higher load\n    \"spawn_rate\": 5,       # Faster ramp-up\n    \"run_time\": \"2m\",      # Longer test\n    \"max_response_time\": 500,  # Custom threshold (ms)\n    \"max_failure_rate\": 0.01,  # 1% max failures\n}\n</code></pre>"},{"location":"guides/performance-testing/#example-api-performance-test","title":"Example: API Performance Test","text":"<pre><code>from codeoptix.behaviors.base import BehaviorSpec\nfrom codeoptix.evaluation.evaluators import BaseEvaluator\nfrom locust import HttpUser, task, between\nfrom locust.env import Environment\nimport gevent\n\nclass APIPerformanceBehavior(BehaviorSpec):\n    \"\"\"Tests API performance.\"\"\"\n\n    def get_name(self) -&gt; str:\n        return \"api-performance\"\n\n    def get_description(self) -&gt; str:\n        return \"Validates API performance under load\"\n\n    def create_evaluator(self) -&gt; BaseEvaluator:\n        return APIPerformanceEvaluator()\n\nclass APIPerformanceEvaluator(BaseEvaluator):\n    def evaluate(self, code: str, context: Dict[str, Any] = None) -&gt; Dict[str, Any]:\n        url = context.get(\"url\", \"http://localhost:8000\")\n        users = context.get(\"users\", 20)\n\n        class APIUser(HttpUser):\n            wait_time = between(0.5, 2)\n            host = url\n\n            @task(5)\n            def get_users(self):\n                self.client.get(\"/api/users\")\n\n            @task(3)\n            def get_posts(self):\n                self.client.get(\"/api/posts\")\n\n            @task(1)\n            def create_post(self):\n                self.client.post(\"/api/posts\", json={\n                    \"title\": \"Test\",\n                    \"content\": \"Test content\"\n                })\n\n        env = Environment(user_classes=[APIUser])\n        env.create_local_runner()\n\n        # Run test\n        env.runner.start(users, spawn_rate=2)\n        gevent.spawn_later(30, lambda: env.runner.quit())\n        env.runner.greenlet.join()\n\n        # Analyze results\n        issues = []\n        stats = env.stats\n\n        for name, entry in stats.entries.items():\n            if entry.num_requests &gt; 0:\n                avg_time = entry.avg_response_time\n                p95_time = entry.get_response_time_percentile(0.95)\n\n                if avg_time &gt; 500:\n                    issues.append(f\"{name}: Avg {avg_time:.0f}ms (target: &lt;500ms)\")\n\n                if p95_time &gt; 1000:\n                    issues.append(f\"{name}: P95 {p95_time:.0f}ms (target: &lt;1000ms)\")\n\n        score = 1.0 if len(issues) == 0 else max(0.0, 1.0 - len(issues) * 0.15)\n\n        return {\n            \"passed\": len(issues) == 0,\n            \"score\": score,\n            \"evidence\": issues,\n        }\n</code></pre>"},{"location":"guides/performance-testing/#running-performance-tests","title":"Running Performance Tests","text":""},{"location":"guides/performance-testing/#with-codeoptix-cli","title":"With CodeOptiX CLI","text":"<pre><code># Set up your application URL\nexport APP_URL=\"http://localhost:8000\"\n\n# Run performance evaluation\ncodeoptix eval \\\n  --agent codex \\\n  --behaviors api-performance \\\n  --context '{\"url\": \"http://localhost:8000\", \"users\": 20, \"run_time\": \"30s\"}' \\\n  --llm-provider openai\n</code></pre>"},{"location":"guides/performance-testing/#with-python-api","title":"With Python API","text":"<pre><code>from codeoptix.adapters.factory import create_adapter\nfrom codeoptix.evaluation import EvaluationEngine\nfrom codeoptix.utils.llm import create_llm_client, LLMProvider\nimport os\n\n# Create adapter\nadapter = create_adapter(\"codex\", {\n    \"llm_config\": {\n        \"provider\": \"openai\",\n        \"api_key\": os.getenv(\"OPENAI_API_KEY\"),\n    }\n})\n\n# Create evaluation engine\nllm_client = create_llm_client(LLMProvider.OPENAI)\nengine = EvaluationEngine(adapter, llm_client)\n\n# Run performance test\nresults = engine.evaluate_behaviors(\n    behavior_names=[\"api-performance\"],\n    context={\n        \"url\": \"http://localhost:8000\",\n        \"users\": 20,\n        \"spawn_rate\": 2,\n        \"run_time\": \"30s\",\n    }\n)\n\nprint(f\"Performance Score: {results['overall_score']:.2%}\")\n</code></pre>"},{"location":"guides/performance-testing/#standalone-locust-tests","title":"Standalone Locust Tests","text":"<p>You can also run Locust tests directly:</p>"},{"location":"guides/performance-testing/#create-locustfilepy","title":"Create <code>locustfile.py</code>","text":"<pre><code>from locust import HttpUser, task, between\n\nclass WebsiteUser(HttpUser):\n    wait_time = between(1, 3)\n    host = \"http://localhost:8000\"\n\n    @task\n    def index(self):\n        self.client.get(\"/\")\n\n    @task(3)\n    def api(self):\n        self.client.get(\"/api/data\")\n</code></pre>"},{"location":"guides/performance-testing/#run-locust","title":"Run Locust","text":"<pre><code># Web UI mode\nlocust\n\n# Headless mode\nlocust --headless -u 10 -r 2 -t 30s\n</code></pre>"},{"location":"guides/performance-testing/#best-practices","title":"Best Practices","text":""},{"location":"guides/performance-testing/#1-start-small","title":"1. Start Small","text":"<p>Begin with low user counts and gradually increase:</p> <pre><code>context = {\n    \"users\": 5,      # Start small\n    \"spawn_rate\": 1,\n    \"run_time\": \"10s\",\n}\n</code></pre>"},{"location":"guides/performance-testing/#2-set-realistic-targets","title":"2. Set Realistic Targets","text":"<p>Define performance thresholds:</p> <pre><code>max_response_time = 500  # 500ms\nmax_failure_rate = 0.01  # 1%\n</code></pre>"},{"location":"guides/performance-testing/#3-monitor-resources","title":"3. Monitor Resources","text":"<p>Watch server resources during tests:</p> <pre><code># Monitor CPU and memory\ntop\nhtop\n</code></pre>"},{"location":"guides/performance-testing/#4-use-ramp-up","title":"4. Use Ramp-Up","text":"<p>Gradually increase load:</p> <pre><code>context = {\n    \"spawn_rate\": 2,  # 2 users per second\n}\n</code></pre>"},{"location":"guides/performance-testing/#5-test-different-scenarios","title":"5. Test Different Scenarios","text":"<p>Test various endpoints and user behaviors:</p> <pre><code>@task(5)\ndef light_endpoint(self):\n    self.client.get(\"/api/status\")\n\n@task(1)\ndef heavy_endpoint(self):\n    self.client.get(\"/api/complex-query\")\n</code></pre>"},{"location":"guides/performance-testing/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/performance-testing/#locust-not-found","title":"Locust Not Found","text":"<pre><code>pip install codeoptix[performance]\n</code></pre>"},{"location":"guides/performance-testing/#tests-too-slow","title":"Tests Too Slow","text":"<p>Reduce user count or test duration:</p> <pre><code>context = {\n    \"users\": 5,        # Lower users\n    \"run_time\": \"10s\", # Shorter duration\n}\n</code></pre>"},{"location":"guides/performance-testing/#connection-errors","title":"Connection Errors","text":"<p>Check if the application is running:</p> <pre><code>curl http://localhost:8000\n</code></pre>"},{"location":"guides/performance-testing/#memory-issues","title":"Memory Issues","text":"<p>Reduce concurrent users:</p> <pre><code>context = {\n    \"users\": 10,  # Lower concurrent users\n}\n</code></pre>"},{"location":"guides/performance-testing/#next-steps","title":"Next Steps","text":"<ul> <li>Custom Behaviors Guide - Create your own performance behaviors</li> <li>Python API Guide - Advanced usage</li> <li>Configuration Guide - Configure performance testing</li> </ul>"},{"location":"guides/performance-testing/#resources","title":"Resources","text":"<ul> <li>Locust Documentation</li> <li>CodeOptiX Behaviors</li> <li>Performance Testing Best Practices</li> </ul>"},{"location":"guides/python-api/","title":"Python API Guide","text":"<p>Complete guide to using CodeOptiX in Python.</p>"},{"location":"guides/python-api/#installation","title":"Installation","text":"<pre><code>pip install codeoptix\n</code></pre>"},{"location":"guides/python-api/#quick-start","title":"Quick Start","text":"<pre><code>import os\nfrom codeoptix.adapters.factory import create_adapter\nfrom codeoptix.evaluation import EvaluationEngine\nfrom codeoptix.utils.llm import create_llm_client, LLMProvider\n\n# Create adapter\nadapter = create_adapter(\"codex\", {\n    \"llm_config\": {\n        \"provider\": \"openai\",\n        \"api_key\": os.getenv(\"OPENAI_API_KEY\"),\n    }\n})\n\n# Create evaluation engine\nllm_client = create_llm_client(LLMProvider.OPENAI)\nengine = EvaluationEngine(adapter, llm_client)\n\n# Evaluate\nresults = engine.evaluate_behaviors(\n    behavior_names=[\"insecure-code\"]\n)\n\nprint(f\"Score: {results['overall_score']:.2f}\")\n</code></pre>"},{"location":"guides/python-api/#agent-adapters","title":"Agent Adapters","text":""},{"location":"guides/python-api/#create-an-adapter","title":"Create an Adapter","text":"<pre><code>from codeoptix.adapters.factory import create_adapter\n\nadapter = create_adapter(\"codex\", {\n    \"llm_config\": {\n        \"provider\": \"openai\",\n        \"api_key\": os.getenv(\"OPENAI_API_KEY\"),\n        \"model\": \"gpt-5.2\"\n    },\n    \"prompt\": \"You are a helpful coding assistant.\"\n})\n</code></pre>"},{"location":"guides/python-api/#execute-tasks","title":"Execute Tasks","text":"<pre><code>output = adapter.execute(\n    \"Write a Python function to calculate fibonacci numbers\",\n    context={\"requirements\": [\"Use recursion\", \"Include docstring\"]}\n)\n\nprint(output.code)\nprint(output.tests)\n</code></pre>"},{"location":"guides/python-api/#update-prompt","title":"Update Prompt","text":"<pre><code>adapter.update_prompt(\"You are a security-focused coding assistant.\")\ncurrent_prompt = adapter.get_prompt()\n</code></pre>"},{"location":"guides/python-api/#evaluation-engine","title":"Evaluation Engine","text":""},{"location":"guides/python-api/#basic-evaluation","title":"Basic Evaluation","text":"<pre><code>from codeoptix.evaluation import EvaluationEngine\n\nengine = EvaluationEngine(adapter, llm_client)\n\nresults = engine.evaluate_behaviors(\n    behavior_names=[\"insecure-code\", \"vacuous-tests\"]\n)\n</code></pre>"},{"location":"guides/python-api/#with-configuration","title":"With Configuration","text":"<pre><code>config = {\n    \"scenario_generator\": {\n        \"num_scenarios\": 5,\n        \"use_bloom\": True\n    },\n    \"static_analysis\": {\n        \"bandit\": True\n    }\n}\n\nengine = EvaluationEngine(adapter, llm_client, config=config)\n</code></pre>"},{"location":"guides/python-api/#with-context","title":"With Context","text":"<pre><code>context = {\n    \"plan\": \"Create secure authentication API\",\n    \"requirements\": [\"JWT tokens\", \"Password hashing\"]\n}\n\nresults = engine.evaluate_behaviors(\n    behavior_names=[\"plan-drift\"],\n    context=context\n)\n</code></pre>"},{"location":"guides/python-api/#behavior-specifications","title":"Behavior Specifications","text":""},{"location":"guides/python-api/#create-behavior","title":"Create Behavior","text":"<pre><code>from codeoptix.behaviors import create_behavior\n\nbehavior = create_behavior(\"insecure-code\")\n</code></pre>"},{"location":"guides/python-api/#evaluate-output","title":"Evaluate Output","text":"<pre><code>from codeoptix.adapters.base import AgentOutput\n\noutput = AgentOutput(\n    code='def connect():\\n    password = \"secret\"\\n    return password',\n    tests=\"def test_connect():\\n    assert True\"\n)\n\nresult = behavior.evaluate(output)\n\nprint(f\"Passed: {result.passed}\")\nprint(f\"Score: {result.score}\")\nprint(f\"Evidence: {result.evidence}\")\n</code></pre>"},{"location":"guides/python-api/#with-configuration_1","title":"With Configuration","text":"<pre><code>behavior = create_behavior(\"insecure-code\", {\n    \"severity\": \"high\",\n    \"enabled\": True\n})\n</code></pre>"},{"location":"guides/python-api/#reflection-engine","title":"Reflection Engine","text":""},{"location":"guides/python-api/#generate-reflection","title":"Generate Reflection","text":"<pre><code>from codeoptix.reflection import ReflectionEngine\nfrom codeoptix.artifacts import ArtifactManager\n\nartifact_manager = ArtifactManager()\nreflection_engine = ReflectionEngine(artifact_manager)\n\nreflection = reflection_engine.reflect(\n    results=evaluation_results,\n    agent_name=\"codex\",\n    save=True\n)\n\nprint(reflection)\n</code></pre>"},{"location":"guides/python-api/#from-run-id","title":"From Run ID","text":"<pre><code>reflection = reflection_engine.reflect_from_run_id(\n    run_id=\"abc123\",\n    agent_name=\"codex\"\n)\n</code></pre>"},{"location":"guides/python-api/#evolution-engine","title":"Evolution Engine","text":""},{"location":"guides/python-api/#evolve-prompts","title":"Evolve Prompts","text":"<pre><code>from codeoptix.evolution import EvolutionEngine\n\nevolution_engine = EvolutionEngine(\n    adapter=adapter,\n    evaluation_engine=eval_engine,\n    llm_client=llm_client,\n    artifact_manager=artifact_manager\n)\n\nevolved = evolution_engine.evolve(\n    evaluation_results=results,\n    reflection=reflection_content,\n    behavior_names=[\"insecure-code\"]\n)\n\nprint(evolved[\"prompts\"][\"system_prompt\"])\n</code></pre>"},{"location":"guides/python-api/#with-configuration_2","title":"With Configuration","text":"<pre><code>config = {\n    \"max_iterations\": 5,\n    \"population_size\": 5,\n    \"proposer\": {\n        \"use_gepa\": True\n    }\n}\n\nevolution_engine = EvolutionEngine(\n    adapter, eval_engine, llm_client, artifact_manager, config=config\n)\n</code></pre>"},{"location":"guides/python-api/#artifact-management","title":"Artifact Management","text":""},{"location":"guides/python-api/#save-results","title":"Save Results","text":"<pre><code>from codeoptix.artifacts import ArtifactManager\n\nartifact_manager = ArtifactManager()\n\nresults_file = artifact_manager.save_results(evaluation_results)\nprint(f\"Saved to: {results_file}\")\n</code></pre>"},{"location":"guides/python-api/#load-results","title":"Load Results","text":"<pre><code>results = artifact_manager.load_results(run_id=\"abc123\")\n</code></pre>"},{"location":"guides/python-api/#list-runs","title":"List Runs","text":"<pre><code>runs = artifact_manager.list_runs()\n\nfor run in runs:\n    print(f\"Run ID: {run['run_id']}\")\n    print(f\"Score: {run['overall_score']:.2f}\")\n</code></pre>"},{"location":"guides/python-api/#acp-integration","title":"ACP Integration","text":""},{"location":"guides/python-api/#agent-registry","title":"Agent Registry","text":"<pre><code>from codeoptix.acp import ACPAgentRegistry\n\n# Create registry\nregistry = ACPAgentRegistry()\n\n# Register an agent\nregistry.register(\n    name=\"claude-code\",\n    command=[\"python\", \"claude_agent.py\"],\n    description=\"Claude Code via ACP\",\n)\n\n# Connect to agent\nconnection = await registry.connect(\"claude-code\")\nsession_id = registry.get_session_id(\"claude-code\")\n</code></pre>"},{"location":"guides/python-api/#quality-bridge","title":"Quality Bridge","text":"<pre><code>from codeoptix.acp import ACPQualityBridge\nfrom codeoptix.evaluation import EvaluationEngine\n\n# Create evaluation engine\nevaluation_engine = EvaluationEngine(adapter, llm_client)\n\n# Create quality bridge\nbridge = ACPQualityBridge(\n    agent_name=\"claude-code\",\n    evaluation_engine=evaluation_engine,\n    auto_eval=True,\n    behaviors=[\"insecure-code\", \"vacuous-tests\"],\n    registry=registry,\n)\n\n# Connect and use\nawait bridge.connect()\nresult = await bridge.prompt(\"Write secure code\")\n</code></pre>"},{"location":"guides/python-api/#multi-agent-judge","title":"Multi-Agent Judge","text":"<pre><code>from codeoptix.acp import ACPAgentRegistry, MultiAgentJudge\n\n# Create registry and register agents\nregistry = ACPAgentRegistry()\nregistry.register(name=\"claude-code\", command=[\"python\", \"claude.py\"])\nregistry.register(name=\"grok\", command=[\"python\", \"grok.py\"])\n\n# Create multi-agent judge\njudge = MultiAgentJudge(\n    registry=registry,\n    generate_agent=\"claude-code\",\n    judge_agent=\"grok\",\n    evaluation_engine=evaluation_engine,\n)\n\n# Generate and judge\nresult = await judge.generate_and_judge(\"Write a secure API endpoint\")\nprint(f\"Generated: {result['generated_code']}\")\nprint(f\"Judgment: {result['judgment']}\")\nprint(f\"Evaluation: {result['evaluation_results']}\")\n</code></pre>"},{"location":"guides/python-api/#agent-orchestration","title":"Agent Orchestration","text":"<pre><code>from codeoptix.acp import AgentOrchestrator\n\n# Create orchestrator\norchestrator = AgentOrchestrator(registry, evaluation_engine)\n\n# Intelligent agent selection\n# The orchestrator automatically:\n# 1. Checks for preferred_agent in context\n# 2. Infers task type from prompt (security, review, testing, etc.)\n# 3. Matches agents with relevant capabilities\n# 4. Falls back to first available agent if no match\nresult = await orchestrator.route_to_agent(\n    prompt=\"Write secure Python tests\",\n    context={\"language\": \"python\", \"preferred_agent\": \"claude-code\"},\n)\n\n# Execute multi-agent workflow\nworkflow = [\n    {\"agent\": \"claude-code\", \"prompt\": \"Generate code\"},\n    {\"agent\": \"grok\", \"prompt\": \"Review code\"},\n]\nresults = await orchestrator.execute_multi_agent_workflow(workflow)\n</code></pre>"},{"location":"guides/python-api/#code-extraction","title":"Code Extraction","text":"<p>CodeOptiX provides comprehensive code extraction from ACP messages:</p> <pre><code>from codeoptix.acp.code_extractor import (\n    extract_code_from_message,\n    extract_all_code,\n    extract_code_from_text\n)\n\n# Extract from single message\ncode_blocks = extract_code_from_message(acp_message)\n\n# Extract from multiple messages\ncode_blocks = extract_all_code(acp_messages)\n\n# Extract from text content\ncode_blocks = extract_code_from_text(\"```python\\ndef hello():\\n    pass\\n```\")\n\n# Each code block contains:\n# {\n#     \"language\": \"python\",\n#     \"content\": \"def hello(): ...\",\n#     \"type\": \"block\",  # or \"inline\", \"file_edit_new\", \"file_edit_old\"\n#     \"path\": \"file.py\"  # if from file edit\n# }\n</code></pre> <p>Supported extraction sources: - \u2705 <code>AgentMessageChunk</code> - Agent message content - \u2705 <code>ToolCallStart</code> - Tool call initiation - \u2705 <code>ToolCallProgress</code> - Tool call progress updates - \u2705 <code>TextContentBlock</code> - Text content with code - \u2705 Raw text strings - Direct text extraction</p>"},{"location":"guides/python-api/#codeoptix-as-acp-agent","title":"CodeOptiX as ACP Agent","text":"<p>CodeOptiX can act as an ACP agent for direct editor integration:</p> <pre><code>from codeoptix.acp import CodeOptiXAgent\nfrom codeoptix.evaluation import EvaluationEngine\nfrom codeoptix.utils.llm import LLMProvider, create_llm_client\nfrom acp import run_agent\n\n# Create evaluation engine and LLM client\nllm_client = create_llm_client(LLMProvider.OPENAI)\nevaluation_engine = EvaluationEngine(adapter, llm_client)\n\n# Create CodeOptiX agent with quality evaluation\nagent = CodeOptiXAgent(\n    evaluation_engine=evaluation_engine,\n    llm_client=llm_client,\n    behaviors=[\"insecure-code\", \"vacuous-tests\", \"plan-drift\"],\n)\n\n# Run as ACP agent (for editor integration)\n# CodeOptiX will automatically:\n# - Extract code from prompts\n# - Evaluate code quality\n# - Send formatted quality reports to editor\nawait run_agent(agent)\n</code></pre>"},{"location":"guides/python-api/#complete-example","title":"Complete Example","text":"<pre><code>import os\nfrom codeoptix.adapters.factory import create_adapter\nfrom codeoptix.evaluation import EvaluationEngine\nfrom codeoptix.reflection import ReflectionEngine\nfrom codeoptix.evolution import EvolutionEngine\nfrom codeoptix.artifacts import ArtifactManager\nfrom codeoptix.utils.llm import create_llm_client, LLMProvider\n\n# 1. Setup\nadapter = create_adapter(\"codex\", {\n    \"llm_config\": {\n        \"provider\": \"openai\",\n        \"api_key\": os.getenv(\"OPENAI_API_KEY\"),\n    }\n})\n\nllm_client = create_llm_client(LLMProvider.OPENAI)\nartifact_manager = ArtifactManager()\n\n# 2. Evaluate\neval_engine = EvaluationEngine(adapter, llm_client)\nresults = eval_engine.evaluate_behaviors(\n    behavior_names=[\"insecure-code\", \"vacuous-tests\"]\n)\n\nartifact_manager.save_results(results)\n\n# 3. Reflect\nreflection_engine = ReflectionEngine(artifact_manager)\nreflection = reflection_engine.reflect(results, save=True)\n\n# 4. Evolve\nevolution_engine = EvolutionEngine(\n    adapter, eval_engine, llm_client, artifact_manager\n)\n\nevolved = evolution_engine.evolve(\n    results, reflection, behavior_names=[\"insecure-code\"]\n)\n\n# 5. Results\nprint(f\"Initial Score: {results['overall_score']:.2f}\")\nprint(f\"Final Score: {evolved['metadata']['final_score']:.2f}\")\nprint(f\"Improvement: {evolved['metadata']['improvement']:.2f}\")\n</code></pre>"},{"location":"guides/python-api/#error-handling","title":"Error Handling","text":""},{"location":"guides/python-api/#retry-logic","title":"Retry Logic","text":"<pre><code>from codeoptix.utils.retry import retry_llm_call\n\n@retry_llm_call(max_attempts=3)\ndef call_api():\n    # Your API call\n    pass\n</code></pre>"},{"location":"guides/python-api/#error-messages","title":"Error Messages","text":"<pre><code>from codeoptix.utils.retry import handle_api_error\n\ntry:\n    result = llm_client.chat_completion(...)\nexcept Exception as e:\n    message = handle_api_error(e, context=\"LLM call\")\n    print(message)\n</code></pre>"},{"location":"guides/python-api/#next-steps","title":"Next Steps","text":"<ul> <li>CLI Usage - Command-line interface</li> <li>ACP Integration Guide - Complete ACP documentation</li> <li>Configuration Guide - Advanced configuration</li> <li>API Reference - Complete API reference</li> </ul>"},{"location":"guides/ui-testing/","title":"UI Testing Guide","text":"<p>Planned Feature</p> <p>UI testing is currently a planned feature, not yet implemented.</p> <p>This guide shows how you can implement UI testing yourself by creating custom behaviors. The dependencies (<code>selenium</code>, <code>playwright</code>) are available as optional dependencies, but you'll need to implement the evaluators yourself.</p>"},{"location":"guides/ui-testing/#installation","title":"Installation","text":"<p>Install UI testing dependencies:</p> <pre><code>pip install codeoptix[ui-testing]\n</code></pre> <p>This installs: - <code>selenium</code> - Web automation framework - <code>playwright</code> - Modern browser automation</p>"},{"location":"guides/ui-testing/#use-cases","title":"Use Cases","text":"<p>UI testing in CodeOptiX would be useful for:</p> <ul> <li>\u2705 Evaluating web applications generated by coding agents</li> <li>\u2705 Testing user interactions (clicks, forms, navigation)</li> <li>\u2705 Validating UI behavior matches requirements</li> <li>\u2705 Checking accessibility in real browsers</li> <li>\u2705 Performance testing of web pages</li> </ul>"},{"location":"guides/ui-testing/#implementation-guide","title":"Implementation Guide","text":"<p>Custom Implementation Required</p> <p>Since UI testing is not yet implemented in CodeOptiX, you'll need to create your own custom behavior and evaluator. Here's how:</p>"},{"location":"guides/ui-testing/#1-create-a-ui-test-behavior","title":"1. Create a UI Test Behavior","text":"<p>Create a custom behavior that uses UI testing:</p> <pre><code>from codeoptix.behaviors.base import BehaviorSpec\nfrom codeoptix.evaluation.evaluators import LLMEvaluator\n\nclass UIBehavior(BehaviorSpec):\n    \"\"\"Test UI functionality.\"\"\"\n\n    def get_name(self) -&gt; str:\n        return \"ui-functionality\"\n\n    def get_description(self) -&gt; str:\n        return \"Validates UI functionality and user interactions\"\n\n    def create_evaluator(self):\n        # For now, use LLMEvaluator or create your own\n        # TODO: Implement UITestEvaluator\n        return None  # You'll need to implement this\n</code></pre>"},{"location":"guides/ui-testing/#2-implement-ui-test-evaluator-example","title":"2. Implement UI Test Evaluator (Example)","text":"<p>Example Implementation</p> <p>This is an example of how you could implement UI testing. You'll need to integrate this into CodeOptiX's evaluation system yourself.</p> <pre><code># Example: Custom UI Test Evaluator\n# This is NOT part of CodeOptiX yet - you need to implement this yourself\n\nfrom playwright.sync_api import sync_playwright\nfrom typing import Dict, Any\n\nclass UITestEvaluator:\n    \"\"\"Example: Evaluates UI using Playwright.\"\"\"\n\n    def evaluate(self, code: str, context: Dict[str, Any] = None) -&gt; Dict[str, Any]:\n        \"\"\"Run UI tests on generated code.\"\"\"\n        url = context.get(\"url\", \"http://localhost:8000\")\n\n        with sync_playwright() as p:\n            browser = p.chromium.launch()\n            page = browser.new_page()\n\n            # Navigate to the application\n            page.goto(url)\n\n            # Run tests\n            issues = []\n\n            # Check if page loads\n            if page.title() == \"\":\n                issues.append(\"Page title is missing\")\n\n            # Check for key elements\n            if not page.query_selector(\"button\"):\n                issues.append(\"No buttons found on page\")\n\n            # Test form submission\n            try:\n                page.fill('input[name=\"email\"]', \"test@example.com\")\n                page.click('button[type=\"submit\"]')\n                page.wait_for_selector(\".success\", timeout=5000)\n            except Exception as e:\n                issues.append(f\"Form submission failed: {e}\")\n\n            browser.close()\n\n            # Calculate score\n            score = 1.0 if len(issues) == 0 else max(0.0, 1.0 - len(issues) * 0.2)\n\n            return {\n                \"passed\": len(issues) == 0,\n                \"score\": score,\n                \"evidence\": issues,\n            }\n</code></pre> <p>Integration Required</p> <p>To use this in CodeOptiX, you'll need to: 1. Integrate it with CodeOptiX's behavior system 2. Connect it to the evaluation engine 3. Handle the results format expected by CodeOptiX</p> <p>This is currently not implemented in the core CodeOptiX codebase.</p>"},{"location":"guides/ui-testing/#3-use-with-selenium-alternative","title":"3. Use with Selenium (Alternative)","text":"<pre><code>from selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\n\nclass SeleniumUITestEvaluator(BaseEvaluator):\n    \"\"\"Evaluates UI using Selenium.\"\"\"\n\n    def evaluate(self, code: str, context: Dict[str, Any] = None) -&gt; Dict[str, Any]:\n        \"\"\"Run UI tests using Selenium.\"\"\"\n        url = context.get(\"url\", \"http://localhost:8000\")\n\n        # Setup Selenium\n        options = webdriver.ChromeOptions()\n        options.add_argument(\"--headless\")\n        driver = webdriver.Chrome(options=options)\n\n        try:\n            driver.get(url)\n\n            issues = []\n\n            # Check page title\n            if not driver.title:\n                issues.append(\"Page title is missing\")\n\n            # Check for elements\n            try:\n                button = WebDriverWait(driver, 5).until(\n                    EC.presence_of_element_located((By.TAG_NAME, \"button\"))\n                )\n            except:\n                issues.append(\"No buttons found on page\")\n\n            # Test interactions\n            try:\n                email_input = driver.find_element(By.NAME, \"email\")\n                email_input.send_keys(\"test@example.com\")\n\n                submit_button = driver.find_element(By.CSS_SELECTOR, \"button[type='submit']\")\n                submit_button.click()\n\n                WebDriverWait(driver, 5).until(\n                    EC.presence_of_element_located((By.CLASS_NAME, \"success\"))\n                )\n            except Exception as e:\n                issues.append(f\"Form submission failed: {e}\")\n\n            score = 1.0 if len(issues) == 0 else max(0.0, 1.0 - len(issues) * 0.2)\n\n            return {\n                \"passed\": len(issues) == 0,\n                \"score\": score,\n                \"evidence\": issues,\n            }\n        finally:\n            driver.quit()\n</code></pre>"},{"location":"guides/ui-testing/#configuration","title":"Configuration","text":""},{"location":"guides/ui-testing/#playwright-setup","title":"Playwright Setup","text":"<p>Install Playwright browsers:</p> <pre><code>playwright install chromium\n</code></pre>"},{"location":"guides/ui-testing/#selenium-setup","title":"Selenium Setup","text":"<p>Install ChromeDriver:</p> <pre><code># macOS\nbrew install chromedriver\n\n# Linux\nsudo apt-get install chromium-chromedriver\n\n# Or download from https://chromedriver.chromium.org/\n</code></pre>"},{"location":"guides/ui-testing/#example-testing-a-login-form","title":"Example: Testing a Login Form","text":"<pre><code>from codeoptix.behaviors.base import BehaviorSpec\nfrom codeoptix.evaluation.evaluators import BaseEvaluator\nfrom playwright.sync_api import sync_playwright\n\nclass LoginFormBehavior(BehaviorSpec):\n    \"\"\"Tests login form functionality.\"\"\"\n\n    def get_name(self) -&gt; str:\n        return \"login-form\"\n\n    def get_description(self) -&gt; str:\n        return \"Validates login form works correctly\"\n\n    def create_evaluator(self) -&gt; BaseEvaluator:\n        return LoginFormEvaluator()\n\nclass LoginFormEvaluator(BaseEvaluator):\n    def evaluate(self, code: str, context: Dict[str, Any] = None) -&gt; Dict[str, Any]:\n        url = context.get(\"url\", \"http://localhost:8000/login\")\n\n        with sync_playwright() as p:\n            browser = p.chromium.launch()\n            page = browser.new_page()\n            page.goto(url)\n\n            issues = []\n\n            # Check form elements exist\n            if not page.query_selector('input[name=\"username\"]'):\n                issues.append(\"Username input missing\")\n\n            if not page.query_selector('input[name=\"password\"]'):\n                issues.append(\"Password input missing\")\n\n            if not page.query_selector('button[type=\"submit\"]'):\n                issues.append(\"Submit button missing\")\n\n            # Test form submission\n            try:\n                page.fill('input[name=\"username\"]', \"testuser\")\n                page.fill('input[name=\"password\"]', \"testpass\")\n                page.click('button[type=\"submit\"]')\n\n                # Wait for redirect or success message\n                page.wait_for_url(\"**/dashboard\", timeout=5000)\n            except Exception as e:\n                issues.append(f\"Login failed: {e}\")\n\n            browser.close()\n\n            score = 1.0 if len(issues) == 0 else max(0.0, 1.0 - len(issues) * 0.25)\n\n            return {\n                \"passed\": len(issues) == 0,\n                \"score\": score,\n                \"evidence\": issues,\n            }\n</code></pre>"},{"location":"guides/ui-testing/#running-ui-tests","title":"Running UI Tests","text":""},{"location":"guides/ui-testing/#with-codeoptix-cli","title":"With CodeOptiX CLI","text":"<pre><code># Set up your application URL\nexport APP_URL=\"http://localhost:8000\"\n\n# Run evaluation with UI behavior\ncodeoptix eval \\\n  --agent codex \\\n  --behaviors login-form \\\n  --context '{\"url\": \"http://localhost:8000\"}' \\\n  --llm-provider openai\n</code></pre>"},{"location":"guides/ui-testing/#with-python-api","title":"With Python API","text":"<pre><code>from codeoptix.adapters.factory import create_adapter\nfrom codeoptix.evaluation import EvaluationEngine\nfrom codeoptix.utils.llm import create_llm_client, LLMProvider\nimport os\n\n# Create adapter\nadapter = create_adapter(\"codex\", {\n    \"llm_config\": {\n        \"provider\": \"openai\",\n        \"api_key\": os.getenv(\"OPENAI_API_KEY\"),\n    }\n})\n\n# Create evaluation engine\nllm_client = create_llm_client(LLMProvider.OPENAI)\nengine = EvaluationEngine(adapter, llm_client)\n\n# Run UI test\nresults = engine.evaluate_behaviors(\n    behavior_names=[\"login-form\"],\n    context={\"url\": \"http://localhost:8000\"}\n)\n\nprint(f\"UI Test Score: {results['overall_score']:.2%}\")\n</code></pre>"},{"location":"guides/ui-testing/#best-practices","title":"Best Practices","text":""},{"location":"guides/ui-testing/#1-use-headless-mode","title":"1. Use Headless Mode","text":"<p>For CI/CD, always use headless mode:</p> <pre><code>browser = p.chromium.launch(headless=True)\n</code></pre>"},{"location":"guides/ui-testing/#2-set-timeouts","title":"2. Set Timeouts","text":"<p>Always set reasonable timeouts:</p> <pre><code>page.wait_for_selector(\".element\", timeout=5000)\n</code></pre>"},{"location":"guides/ui-testing/#3-clean-up-resources","title":"3. Clean Up Resources","text":"<p>Always close browsers:</p> <pre><code>try:\n    # Run tests\n    pass\nfinally:\n    browser.close()\n</code></pre>"},{"location":"guides/ui-testing/#4-use-context-for-configuration","title":"4. Use Context for Configuration","text":"<p>Pass configuration via context:</p> <pre><code>context = {\n    \"url\": \"http://localhost:8000\",\n    \"timeout\": 5000,\n    \"headless\": True,\n}\n</code></pre>"},{"location":"guides/ui-testing/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/ui-testing/#playwright-browser-not-found","title":"Playwright Browser Not Found","text":"<pre><code>playwright install chromium\n</code></pre>"},{"location":"guides/ui-testing/#selenium-chromedriver-not-found","title":"Selenium ChromeDriver Not Found","text":"<pre><code># Check ChromeDriver version matches Chrome\nchromedriver --version\ngoogle-chrome --version\n\n# Install matching version\nbrew install chromedriver  # macOS\n</code></pre>"},{"location":"guides/ui-testing/#tests-timeout","title":"Tests Timeout","text":"<p>Increase timeout in your evaluator:</p> <pre><code>page.wait_for_selector(\".element\", timeout=10000)\n</code></pre>"},{"location":"guides/ui-testing/#headless-mode-issues","title":"Headless Mode Issues","text":"<p>Some applications behave differently in headless mode. Test in non-headless first:</p> <pre><code>browser = p.chromium.launch(headless=False)\n</code></pre>"},{"location":"guides/ui-testing/#next-steps","title":"Next Steps","text":"<ul> <li>Custom Behaviors Guide - Create your own UI test behaviors</li> <li>Python API Guide - Advanced usage</li> <li>Configuration Guide - Configure UI testing</li> </ul>"},{"location":"guides/ui-testing/#resources","title":"Resources","text":"<ul> <li>Playwright Documentation</li> <li>Selenium Documentation</li> <li>CodeOptiX Behaviors</li> </ul>"}]}